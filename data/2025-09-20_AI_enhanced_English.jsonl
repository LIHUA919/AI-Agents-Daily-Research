{"id": "2509.14276", "categories": ["cs.MA", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.14276", "abs": "https://arxiv.org/abs/2509.14276", "authors": ["Yuxiang Mai", "Qiyue Yin", "Wancheng Ni", "Pei Xu", "Kaiqi Huang"], "title": "Constructive Conflict-Driven Multi-Agent Reinforcement Learning for Strategic Diversity", "comment": "Accepted by IJCAI 2025", "summary": "In recent years, diversity has emerged as a useful mechanism to enhance the\nefficiency of multi-agent reinforcement learning (MARL). However, existing\nmethods predominantly focus on designing policies based on individual agent\ncharacteristics, often neglecting the interplay and mutual influence among\nagents during policy formation. To address this gap, we propose Competitive\nDiversity through Constructive Conflict (CoDiCon), a novel approach that\nincorporates competitive incentives into cooperative scenarios to encourage\npolicy exchange and foster strategic diversity among agents. Drawing\ninspiration from sociological research, which highlights the benefits of\nmoderate competition and constructive conflict in group decision-making, we\ndesign an intrinsic reward mechanism using ranking features to introduce\ncompetitive motivations. A centralized intrinsic reward module generates and\ndistributes varying reward values to agents, ensuring an effective balance\nbetween competition and cooperation. By optimizing the parameterized\ncentralized reward module to maximize environmental rewards, we reformulate the\nconstrained bilevel optimization problem to align with the original task\nobjectives. We evaluate our algorithm against state-of-the-art methods in the\nSMAC and GRF environments. Experimental results demonstrate that CoDiCon\nachieves superior performance, with competitive intrinsic rewards effectively\npromoting diverse and adaptive strategies among cooperative agents.", "AI": {"tldr": "CoDiCon introduces competitive incentives in cooperative MARL through ranking-based intrinsic rewards to foster strategic diversity and improve performance.", "motivation": "Existing MARL diversity methods focus on individual agent characteristics but neglect agent interplay and mutual influence during policy formation.", "method": "Uses competitive intrinsic rewards with ranking features, centralized reward module for balanced competition-cooperation, and bilevel optimization to maximize environmental rewards.", "result": "Outperforms state-of-the-art methods in SMAC and GRF environments, with competitive rewards effectively promoting diverse and adaptive strategies.", "conclusion": "Incorporating constructive competition through ranking-based intrinsic rewards enhances cooperative MARL by fostering strategic diversity and improving overall performance."}}
{"id": "2509.14680", "categories": ["cs.MA", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.14680", "abs": "https://arxiv.org/abs/2509.14680", "authors": ["Tianyang Duan", "Zongyuan Zhang", "Songxiao Guo", "Dong Huang", "Yuanye Zhao", "Zheng Lin", "Zihan Fang", "Dianxin Luan", "Heming Cui", "Yong Cui"], "title": "LEED: A Highly Efficient and Scalable LLM-Empowered Expert Demonstrations Framework for Multi-Agent Reinforcement Learning", "comment": "5 pages, 4 figures", "summary": "Multi-agent reinforcement learning (MARL) holds substantial promise for\nintelligent decision-making in complex environments. However, it suffers from a\ncoordination and scalability bottleneck as the number of agents increases. To\naddress these issues, we propose the LLM-empowered expert demonstrations\nframework for multi-agent reinforcement learning (LEED). LEED consists of two\ncomponents: a demonstration generation (DG) module and a policy optimization\n(PO) module. Specifically, the DG module leverages large language models to\ngenerate instructions for interacting with the environment, thereby producing\nhigh-quality demonstrations. The PO module adopts a decentralized training\nparadigm, where each agent utilizes the generated demonstrations to construct\nan expert policy loss, which is then integrated with its own policy loss. This\nenables each agent to effectively personalize and optimize its local policy\nbased on both expert knowledge and individual experience. Experimental results\nshow that LEED achieves superior sample efficiency, time efficiency, and robust\nscalability compared to state-of-the-art baselines.", "AI": {"tldr": "LEED framework uses LLM-generated expert demonstrations to improve multi-agent reinforcement learning, achieving better efficiency and scalability than existing methods.", "motivation": "Multi-agent reinforcement learning faces coordination and scalability challenges as the number of agents increases, limiting its effectiveness in complex environments.", "method": "Two-component framework: 1) DG module uses large language models to generate high-quality expert demonstrations, 2) PO module uses decentralized training with expert policy loss integrated with individual policy loss for personalized optimization.", "result": "Experimental results show superior sample efficiency, time efficiency, and robust scalability compared to state-of-the-art baselines.", "conclusion": "LEED framework effectively addresses coordination and scalability issues in MARL by leveraging LLM-generated expert demonstrations, enabling more efficient and scalable multi-agent decision-making."}}
{"id": "2509.15103", "categories": ["cs.MA", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.15103", "abs": "https://arxiv.org/abs/2509.15103", "authors": ["Simin Li", "Zheng Yuwei", "Zihao Mao", "Linhao Wang", "Ruixiao Xu", "Chengdong Ma", "Xin Yu", "Yuqing Ma", "Qi Dou", "Xin Wang", "Jie Luo", "Bo An", "Yaodong Yang", "Weifeng Lv", "Xianglong Liu"], "title": "Vulnerable Agent Identification in Large-Scale Multi-Agent Reinforcement Learning", "comment": "submitted to NIPS 2025", "summary": "Partial agent failure becomes inevitable when systems scale up, making it\ncrucial to identify the subset of agents whose compromise would most severely\ndegrade overall performance. In this paper, we study this Vulnerable Agent\nIdentification (VAI) problem in large-scale multi-agent reinforcement learning\n(MARL). We frame VAI as a Hierarchical Adversarial Decentralized Mean Field\nControl (HAD-MFC), where the upper level involves an NP-hard combinatorial task\nof selecting the most vulnerable agents, and the lower level learns worst-case\nadversarial policies for these agents using mean-field MARL. The two problems\nare coupled together, making HAD-MFC difficult to solve. To solve this, we\nfirst decouple the hierarchical process by Fenchel-Rockafellar transform,\nresulting a regularized mean-field Bellman operator for upper level that\nenables independent learning at each level, thus reducing computational\ncomplexity. We then reformulate the upper-level combinatorial problem as a MDP\nwith dense rewards from our regularized mean-field Bellman operator, enabling\nus to sequentially identify the most vulnerable agents by greedy and RL\nalgorithms. This decomposition provably preserves the optimal solution of the\noriginal HAD-MFC. Experiments show our method effectively identifies more\nvulnerable agents in large-scale MARL and the rule-based system, fooling system\ninto worse failures, and learns a value function that reveals the vulnerability\nof each agent.", "AI": {"tldr": "This paper proposes a method to identify the most vulnerable agents in large-scale multi-agent systems by framing the problem as Hierarchical Adversarial Decentralized Mean Field Control (HAD-MFC) and solving it through decomposition and reformulation.", "motivation": "As systems scale up, partial agent failure becomes inevitable, making it crucial to identify which agents' compromise would most severely degrade overall system performance.", "method": "The authors frame the Vulnerable Agent Identification (VAI) problem as HAD-MFC, then decouple the hierarchical process using Fenchel-Rockafellar transform, resulting in a regularized mean-field Bellman operator. They reformulate the upper-level combinatorial problem as an MDP with dense rewards to sequentially identify vulnerable agents using greedy and RL algorithms.", "result": "Experiments show the method effectively identifies more vulnerable agents in large-scale MARL and rule-based systems, causes worse system failures, and learns a value function that reveals each agent's vulnerability.", "conclusion": "The proposed decomposition approach successfully solves the challenging HAD-MFC problem while preserving optimal solutions, enabling efficient identification of the most critical agents whose failure would maximally impact system performance."}}
{"id": "2509.14251", "categories": ["cs.AI", "cs.MA", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2509.14251", "abs": "https://arxiv.org/abs/2509.14251", "authors": ["Qihang Chen"], "title": "Unified Crew Planning and Replanning Optimization in Multi-Line Metro Systems Considering Workforce Heterogeneity", "comment": null, "summary": "Metro crew planning is a key component of smart city development as it\ndirectly impacts the operational efficiency and service reliability of public\ntransportation. With the rapid expansion of metro networks, effective\nmulti-line scheduling and emergency management have become essential for\nlarge-scale seamless operations. However, current research focuses primarily on\nindividual metro lines,with insufficient attention on cross-line coordination\nand rapid replanning during disruptions. Here, a unified optimization framework\nis presented for multi-line metro crew planning and replanning with\nheterogeneous workforce. Specifically, a hierarchical time-space network model\nis proposed to represent the unified crew action space, and computationally\nefficient constraints and formulations are derived for the crew's heterogeneous\nqualifications and preferences. Solution algorithms based on column generation\nand shortest path adjustment are further developed, utilizing the proposed\nnetwork model. Experiments with real data from Shanghai and Beijing Metro\ndemonstrate that the proposed methods outperform benchmark heuristics in both\ncost reduction and task completion,and achieve notable efficiency gains by\nincorporating cross-line operations, particularly for urgent tasks during\ndisruptions. This work highlights the role of global optimization and\ncross-line coordination in multi-line metro system operations, providing\ninsights into the efficient and reliable functioning of public transportation\nin smart cities.", "AI": {"tldr": "A unified optimization framework for multi-line metro crew planning and replanning with heterogeneous workforce, using hierarchical time-space network modeling and efficient algorithms that outperform benchmarks in cost reduction and task completion.", "motivation": "Metro crew planning is crucial for smart city development and operational efficiency, but current research focuses on individual lines with insufficient attention to cross-line coordination and rapid replanning during disruptions in rapidly expanding metro networks.", "method": "Proposed a hierarchical time-space network model to represent unified crew action space, with computationally efficient constraints for heterogeneous qualifications and preferences. Developed solution algorithms based on column generation and shortest path adjustment.", "result": "Experiments with real data from Shanghai and Beijing Metro show the methods outperform benchmark heuristics in cost reduction and task completion, achieving notable efficiency gains through cross-line operations, especially for urgent tasks during disruptions.", "conclusion": "This work demonstrates the importance of global optimization and cross-line coordination in multi-line metro system operations, providing insights for efficient and reliable public transportation in smart cities."}}
{"id": "2509.14274", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.14274", "abs": "https://arxiv.org/abs/2509.14274", "authors": ["Kazumi Kasaura", "Naoto Onda", "Yuta Oriike", "Masaya Taniguchi", "Akiyoshi Sannai", "Sho Sonoda"], "title": "Discovering New Theorems via LLMs with In-Context Proof Learning in Lean", "comment": "11 pages, 3 figures", "summary": "Large Language Models have demonstrated significant promise in formal theorem\nproving. However, previous works mainly focus on solving existing problems. In\nthis paper, we focus on the ability of LLMs to find novel theorems. We propose\nConjecturing-Proving Loop pipeline for automatically generating mathematical\nconjectures and proving them in Lean 4 format. A feature of our approach is\nthat we generate and prove further conjectures with context including\npreviously generated theorems and their proofs, which enables the generation of\nmore difficult proofs by in-context learning of proof strategies without\nchanging parameters of LLMs. We demonstrated that our framework rediscovered\ntheorems with verification, which were published in past mathematical papers\nand have not yet formalized. Moreover, at least one of these theorems could not\nbe proved by the LLM without in-context learning, even in natural language,\nwhich means that in-context learning was effective for neural theorem proving.\nThe source code is available at\nhttps://github.com/auto-res/ConjecturingProvingLoop.", "AI": {"tldr": "LLMs can automatically generate and prove novel mathematical theorems using a Conjecturing-Proving Loop that leverages in-context learning with previously generated theorems and proofs.", "motivation": "Previous works focused on solving existing problems, but this paper explores LLMs' ability to discover novel theorems automatically.", "method": "Proposed Conjecturing-Proving Loop pipeline that generates mathematical conjectures and proves them in Lean 4 format, using context from previously generated theorems and proofs for in-context learning.", "result": "The framework rediscovered theorems published in past mathematical papers that hadn't been formalized, and demonstrated that in-context learning was essential for proving at least one theorem that couldn't be proved without it.", "conclusion": "The approach enables generation of more difficult proofs through in-context learning of proof strategies without changing LLM parameters, showing effectiveness for neural theorem proving."}}
{"id": "2509.14778", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2509.14778", "abs": "https://arxiv.org/abs/2509.14778", "authors": ["Yuxiao Cheng", "Jinli Suo"], "title": "OpenLens AI: Fully Autonomous Research Agent for Health Infomatics", "comment": null, "summary": "Health informatics research is characterized by diverse data modalities,\nrapid knowledge expansion, and the need to integrate insights across biomedical\nscience, data analytics, and clinical practice. These characteristics make it\nparticularly well-suited for agent-based approaches that can automate knowledge\nexploration, manage complex workflows, and generate clinically meaningful\noutputs. Recent progress in large language model (LLM)-based agents has\ndemonstrated promising capabilities in literature synthesis, data analysis, and\neven end-to-end research execution. However, existing systems remain limited\nfor health informatics because they lack mechanisms to interpret medical\nvisualizations and often overlook domain-specific quality requirements. To\naddress these gaps, we introduce OpenLens AI, a fully automated framework\ntailored to health informatics. OpenLens AI integrates specialized agents for\nliterature review, data analysis, code generation, and manuscript preparation,\nenhanced by vision-language feedback for medical visualization and quality\ncontrol for reproducibility. The framework automates the entire research\npipeline, producing publication-ready LaTeX manuscripts with transparent and\ntraceable workflows, thereby offering a domain-adapted solution for advancing\nhealth informatics research.", "AI": {"tldr": "OpenLens AI is an automated framework for health informatics research that integrates specialized agents for literature review, data analysis, code generation, and manuscript preparation, enhanced by vision-language capabilities for medical visualizations and quality control.", "motivation": "Health informatics research involves diverse data modalities and requires integration across biomedical science, data analytics, and clinical practice. Existing LLM-based agent systems lack mechanisms to interpret medical visualizations and often overlook domain-specific quality requirements.", "method": "The framework integrates specialized agents for literature review, data analysis, code generation, and manuscript preparation, enhanced by vision-language feedback for medical visualization and quality control for reproducibility.", "result": "OpenLens AI automates the entire research pipeline and produces publication-ready LaTeX manuscripts with transparent and traceable workflows.", "conclusion": "The framework offers a domain-adapted solution for advancing health informatics research by addressing the specific gaps in existing automated research systems."}}
{"id": "2509.14384", "categories": ["cs.LG", "cs.NA", "math.NA"], "pdf": "https://arxiv.org/pdf/2509.14384", "abs": "https://arxiv.org/abs/2509.14384", "authors": ["Nishantak Panigrahi", "Mayank Patwal"], "title": "A Neural Network for the Identical Kuramoto Equation: Architectural Considerations and Performance Evaluation", "comment": "6 pages, 10 figures. Presented at IEEE International Conference on\n  Compute, Control, Network & Photonics (ICCCNP), 2025", "summary": "In this paper, we investigate the efficiency of Deep Neural Networks (DNNs)\nto approximate the solution of a nonlocal conservation law derived from the\nidentical-oscillator Kuramoto model, focusing on the evaluation of an\narchitectural choice and its impact on solution accuracy based on the energy\nnorm and computation time. Through systematic experimentation, we demonstrate\nthat network configuration parameters-specifically, activation function\nselection (tanh vs. sin vs. ReLU), network depth (4-8 hidden layers), width\n(64-256 neurons), and training methodology (collocation points, epoch\ncount)-significantly influence convergence characteristics. We observe that\ntanh activation yields stable convergence across configurations, whereas sine\nactivation can attain marginally lower errors and training times in isolated\ncases, but occasionally produce nonphysical artefacts. Our comparative analysis\nwith traditional numerical methods shows that optimally configured DNNs offer\ncompetitive accuracy with notably different computational trade-offs.\nFurthermore, we identify fundamental limitations of standard feed-forward\narchitectures when handling singular or piecewise-constant solutions, providing\nempirical evidence that such networks inherently oversmooth sharp features due\nto the natural function space limitations of standard activation functions.\nThis work contributes to the growing body of research on neural network-based\nscientific computing by providing practitioners with empirical guidelines for\nDNN implementation while illuminating fundamental theoretical constraints that\nmust be overcome to expand their applicability to more challenging physical\nsystems with discontinuities.", "AI": {"tldr": "DNNs for nonlocal conservation law approximation from Kuramoto model, showing tanh activation provides stable convergence while sine can achieve lower errors but may produce artifacts. DNNs offer competitive accuracy vs traditional methods but have limitations with singular/piecewise solutions due to oversmoothing.", "motivation": "Investigate efficiency of Deep Neural Networks to approximate solutions of nonlocal conservation laws derived from identical-oscillator Kuramoto model, focusing on architectural choices and their impact on accuracy and computation time.", "method": "Systematic experimentation with different network configurations: activation functions (tanh, sin, ReLU), network depth (4-8 hidden layers), width (64-256 neurons), and training methodology (collocation points, epoch count). Comparative analysis with traditional numerical methods.", "result": "Tanh activation yields stable convergence across configurations. Sine activation can achieve marginally lower errors and training times in some cases but occasionally produces nonphysical artifacts. Optimally configured DNNs offer competitive accuracy with different computational trade-offs compared to traditional methods.", "conclusion": "Standard feed-forward architectures have fundamental limitations when handling singular or piecewise-constant solutions, inherently oversmoothing sharp features due to function space limitations of standard activation functions. Provides empirical guidelines for DNN implementation while highlighting theoretical constraints for challenging physical systems with discontinuities."}}
{"id": "2509.14289", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.14289", "abs": "https://arxiv.org/abs/2509.14289", "authors": ["Lanxiao Huang", "Daksh Dave", "Ming Jin", "Tyler Cody", "Peter Beling"], "title": "From Capabilities to Performance: Evaluating Key Functional Properties of LLM Architectures in Penetration Testing", "comment": null, "summary": "Large language models (LLMs) are increasingly used to automate or augment\npenetration testing, but their effectiveness and reliability across attack\nphases remain unclear. We present a comprehensive evaluation of multiple\nLLM-based agents, from single-agent to modular designs, across realistic\npenetration testing scenarios, measuring empirical performance and recurring\nfailure patterns. We also isolate the impact of five core functional\ncapabilities via targeted augmentations: Global Context Memory (GCM),\nInter-Agent Messaging (IAM), Context-Conditioned Invocation (CCI), Adaptive\nPlanning (AP), and Real-Time Monitoring (RTM). These interventions support,\nrespectively: (i) context coherence and retention, (ii) inter-component\ncoordination and state management, (iii) tool use accuracy and selective\nexecution, (iv) multi-step strategic planning, error detection, and recovery,\nand (v) real-time dynamic responsiveness. Our results show that while some\narchitectures natively exhibit subsets of these properties, targeted\naugmentations substantially improve modular agent performance, especially in\ncomplex, multi-step, and real-time penetration testing tasks.", "AI": {"tldr": "Evaluation of LLM-based agents for penetration testing shows targeted functional augmentations significantly improve performance in complex multi-step attack scenarios.", "motivation": "To assess the effectiveness and reliability of LLM-based agents in penetration testing across different attack phases and identify key functional capabilities needed for success.", "method": "Comprehensive evaluation of multiple LLM agent architectures (single-agent to modular designs) across realistic penetration testing scenarios, with targeted testing of five core functional capabilities: Global Context Memory, Inter-Agent Messaging, Context-Conditioned Invocation, Adaptive Planning, and Real-Time Monitoring.", "result": "Targeted augmentations substantially improve modular agent performance, especially in complex, multi-step, and real-time penetration testing tasks, though some architectures natively exhibit subsets of these properties.", "conclusion": "Functional augmentations are crucial for enhancing LLM-based penetration testing agents, with specific capabilities like context management, coordination, and real-time responsiveness being particularly important for success in complex attack scenarios."}}
{"id": "2509.14956", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2509.14956", "abs": "https://arxiv.org/abs/2509.14956", "authors": ["Diego Gosmar", "Deborah A. Dahl"], "title": "Sentinel Agents for Secure and Trustworthy Agentic AI in Multi-Agent Systems", "comment": "25 pages, 12 figures", "summary": "This paper proposes a novel architectural framework aimed at enhancing\nsecurity and reliability in multi-agent systems (MAS). A central component of\nthis framework is a network of Sentinel Agents, functioning as a distributed\nsecurity layer that integrates techniques such as semantic analysis via large\nlanguage models (LLMs), behavioral analytics, retrieval-augmented verification,\nand cross-agent anomaly detection. Such agents can potentially oversee\ninter-agent communications, identify potential threats, enforce privacy and\naccess controls, and maintain comprehensive audit records. Complementary to the\nidea of Sentinel Agents is the use of a Coordinator Agent. The Coordinator\nAgent supervises policy implementation, and manages agent participation. In\naddition, the Coordinator also ingests alerts from Sentinel Agents. Based on\nthese alerts, it can adapt policies, isolate or quarantine misbehaving agents,\nand contain threats to maintain the integrity of the MAS ecosystem. This\ndual-layered security approach, combining the continuous monitoring of Sentinel\nAgents with the governance functions of Coordinator Agents, supports dynamic\nand adaptive defense mechanisms against a range of threats, including prompt\ninjection, collusive agent behavior, hallucinations generated by LLMs, privacy\nbreaches, and coordinated multi-agent attacks. In addition to the architectural\ndesign, we present a simulation study where 162 synthetic attacks of different\nfamilies (prompt injection, hallucination, and data exfiltration) were injected\ninto a multi-agent conversational environment. The Sentinel Agents successfully\ndetected the attack attempts, confirming the practical feasibility of the\nproposed monitoring approach. The framework also offers enhanced system\nobservability, supports regulatory compliance, and enables policy evolution\nover time.", "AI": {"tldr": "Novel dual-layered security framework for multi-agent systems using Sentinel Agents for continuous monitoring and Coordinator Agents for governance, successfully tested against 162 synthetic attacks.", "motivation": "To enhance security and reliability in multi-agent systems against emerging threats like prompt injection, LLM hallucinations, privacy breaches, and coordinated attacks.", "method": "Architectural framework with distributed Sentinel Agents using semantic analysis (LLMs), behavioral analytics, retrieval-augmented verification, and anomaly detection, plus Coordinator Agent for policy management and threat response.", "result": "Successfully detected 162 synthetic attacks (prompt injection, hallucination, data exfiltration) in simulation study, confirming practical feasibility of the monitoring approach.", "conclusion": "The dual-layered security approach provides dynamic, adaptive defense mechanisms while enhancing system observability, regulatory compliance, and enabling policy evolution over time."}}
{"id": "2509.14386", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.14386", "abs": "https://arxiv.org/abs/2509.14386", "authors": ["Arjun S. Nair", "Kristina P. Sinaga"], "title": "Disproving the Feasibility of Learned Confidence Calibration Under Binary Supervision: An Information-Theoretic Impossibility", "comment": "30 pages, 13 figures, 8 tables", "summary": "We prove a fundamental impossibility theorem: neural networks cannot\nsimultaneously learn well-calibrated confidence estimates with meaningful\ndiversity when trained using binary correct/incorrect supervision. Through\nrigorous mathematical analysis and comprehensive empirical evaluation spanning\nnegative reward training, symmetric loss functions, and post-hoc calibration\nmethods, we demonstrate this is an information-theoretic constraint, not a\nmethodological failure. Our experiments reveal universal failure patterns:\nnegative rewards produce extreme underconfidence (ECE greater than 0.8) while\ndestroying confidence diversity (std less than 0.05), symmetric losses fail to\nescape binary signal averaging, and post-hoc methods achieve calibration (ECE\nless than 0.02) only by compressing the confidence distribution. We formalize\nthis as an underspecified mapping problem where binary signals cannot\ndistinguish between different confidence levels for correct predictions: a 60\npercent confident correct answer receives identical supervision to a 90 percent\nconfident one. Crucially, our real-world validation shows 100 percent failure\nrate for all training methods across MNIST, Fashion-MNIST, and CIFAR-10, while\npost-hoc calibration's 33 percent success rate paradoxically confirms our\ntheorem by achieving calibration through transformation rather than learning.\nThis impossibility directly explains neural network hallucinations and\nestablishes why post-hoc calibration is mathematically necessary, not merely\nconvenient. We propose novel supervision paradigms using ensemble disagreement\nand adaptive multi-agent learning that could overcome these fundamental\nlimitations without requiring human confidence annotations.", "AI": {"tldr": "Neural networks cannot learn well-calibrated confidence estimates with meaningful diversity when trained using binary correct/incorrect supervision due to information-theoretic constraints.", "motivation": "To understand why neural networks struggle with confidence calibration and diversity, and to prove this is a fundamental limitation rather than a methodological issue.", "method": "Rigorous mathematical analysis and comprehensive empirical evaluation including negative reward training, symmetric loss functions, post-hoc calibration methods across MNIST, Fashion-MNIST, and CIFAR-10 datasets.", "result": "Universal failure patterns: negative rewards cause extreme underconfidence (ECE > 0.8) and destroy confidence diversity (std < 0.05), symmetric losses fail to escape binary signal averaging, post-hoc methods achieve calibration (ECE < 0.02) only by compressing confidence distribution. 100% failure rate for all training methods, 33% success rate for post-hoc calibration.", "conclusion": "Binary supervision creates an underspecified mapping problem where different confidence levels for correct predictions receive identical supervision. This explains neural network hallucinations and establishes why post-hoc calibration is mathematically necessary. Novel supervision paradigms using ensemble disagreement and adaptive multi-agent learning are proposed to overcome these limitations."}}
{"id": "2509.14382", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.14382", "abs": "https://arxiv.org/abs/2509.14382", "authors": ["Daniel R\u00f6der", "Akhil Juneja", "Roland Roller", "Sven Schmeier"], "title": "Detecting Pipeline Failures through Fine-Grained Analysis of Web Agents", "comment": null, "summary": "Web agents powered by large language models (LLMs) can autonomously perform\ncomplex, multistep tasks in dynamic web environments. However, current\nevaluations mostly focus on the overall success while overlooking intermediate\nerrors. This limits insight into failure modes and hinders systematic\nimprovement. This work analyzes existing benchmarks and highlights the lack of\nfine-grained diagnostic tools. To address this gap, we propose a modular\nevaluation framework that decomposes agent pipelines into interpretable stages\nfor detailed error analysis. Using the SeeAct framework and the Mind2Web\ndataset as a case study, we show how this approach reveals actionable\nweaknesses missed by standard metrics - paving the way for more robust and\ngeneralizable web agents.", "AI": {"tldr": "Proposes a modular evaluation framework for web agents that decomposes agent pipelines into interpretable stages to enable detailed error analysis, addressing the limitation of current evaluations that focus only on overall success.", "motivation": "Current evaluations of web agents powered by LLMs mostly focus on overall success while overlooking intermediate errors, which limits insight into failure modes and hinders systematic improvement.", "method": "Developed a modular evaluation framework that decomposes agent pipelines into interpretable stages for detailed error analysis, using the SeeAct framework and Mind2Web dataset as a case study.", "result": "The approach reveals actionable weaknesses missed by standard metrics, providing more granular diagnostic capabilities for web agent performance.", "conclusion": "The proposed framework paves the way for more robust and generalizable web agents by enabling fine-grained error analysis and systematic improvement."}}
{"id": "2509.14391", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.14391", "abs": "https://arxiv.org/abs/2509.14391", "authors": ["Ye Qiao", "Sitao Huang"], "title": "Q-ROAR: Outlier-Aware Rescaling for RoPE Position Interpolation in Quantized Long-Context LLMs", "comment": null, "summary": "Extending LLM context windows is crucial for long range tasks. RoPE-based\nposition interpolation (PI) methods like linear and frequency-aware scaling\nextend input lengths without retraining, while post-training quantization (PTQ)\nenables practical deployment. We show that combining PI with PTQ degrades\naccuracy due to coupled effects long context aliasing, dynamic range dilation,\naxis grid anisotropy, and outlier shifting that induce position-dependent logit\nnoise. We provide the first systematic analysis of PI plus PTQ and introduce\ntwo diagnostics: Interpolation Pressure (per-band phase scaling sensitivity)\nand Tail Inflation Ratios (outlier shift from short to long contexts). To\naddress this, we propose Q-ROAR, a RoPE-aware, weight-only stabilization that\ngroups RoPE dimensions into a few frequency bands and performs a small search\nover per-band scales for W_Q,W_K, with an optional symmetric variant to\npreserve logit scale. The diagnostics guided search uses a tiny long-context\ndev set and requires no fine-tuning, kernel, or architecture changes.\nEmpirically, Q-ROAR recovers up to 0.7% accuracy on standard tasks and reduces\nGovReport perplexity by more than 10%, while preserving short-context\nperformance and compatibility with existing inference stacks.", "AI": {"tldr": "Combining RoPE position interpolation with post-training quantization causes accuracy degradation due to position-dependent noise. Q-ROAR introduces RoPE-aware stabilization through frequency band grouping and per-band scaling search to recover accuracy.", "motivation": "Extending LLM context windows is crucial for long-range tasks, but combining position interpolation methods with post-training quantization leads to accuracy degradation due to coupled effects like long context aliasing and outlier shifting.", "method": "Proposes Q-ROAR - a RoPE-aware weight-only stabilization that groups RoPE dimensions into frequency bands and performs a small search over per-band scales for W_Q and W_K matrices, using diagnostics like Interpolation Pressure and Tail Inflation Ratios.", "result": "Q-ROAR recovers up to 0.7% accuracy on standard tasks and reduces GovReport perplexity by more than 10%, while preserving short-context performance and maintaining compatibility with existing inference stacks.", "conclusion": "The method effectively addresses the degradation issues when combining position interpolation with quantization, providing a no-fine-tuning solution that maintains performance across both short and long contexts."}}
{"id": "2509.14448", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.14448", "abs": "https://arxiv.org/abs/2509.14448", "authors": ["Rick Chen", "Joseph Ternasky", "Afriyie Samuel Kwesi", "Ben Griffin", "Aaron Ontoyin Yin", "Zakari Salifu", "Kelvin Amoaba", "Xianling Mu", "Fuat Alican", "Yigit Ihlamur"], "title": "VCBench: Benchmarking LLMs in Venture Capital", "comment": null, "summary": "Benchmarks such as SWE-bench and ARC-AGI demonstrate how shared datasets\naccelerate progress toward artificial general intelligence (AGI). We introduce\nVCBench, the first benchmark for predicting founder success in venture capital\n(VC), a domain where signals are sparse, outcomes are uncertain, and even top\ninvestors perform modestly. At inception, the market index achieves a precision\nof 1.9%. Y Combinator outperforms the index by a factor of 1.7x, while tier-1\nfirms are 2.9x better. VCBench provides 9,000 anonymized founder profiles,\nstandardized to preserve predictive features while resisting identity leakage,\nwith adversarial tests showing more than 90% reduction in re-identification\nrisk. We evaluate nine state-of-the-art large language models (LLMs).\nDeepSeek-V3 delivers over six times the baseline precision, GPT-4o achieves the\nhighest F0.5, and most models surpass human benchmarks. Designed as a public\nand evolving resource available at vcbench.com, VCBench establishes a\ncommunity-driven standard for reproducible and privacy-preserving evaluation of\nAGI in early-stage venture forecasting.", "AI": {"tldr": "VCBench is the first benchmark for predicting founder success in VC, featuring 9,000 anonymized founder profiles with strong privacy protection. LLMs like DeepSeek-V3 and GPT-4o significantly outperform human benchmarks and market indices.", "motivation": "Existing benchmarks like SWE-bench and ARC-AGI accelerate AGI progress, but there's no standardized benchmark for VC founder success prediction where signals are sparse and outcomes uncertain.", "method": "Created VCBench with 9,000 anonymized founder profiles standardized to preserve predictive features while resisting identity leakage. Evaluated nine state-of-the-art LLMs against human and market benchmarks.", "result": "DeepSeek-V3 delivered over 6x baseline precision, GPT-4o achieved highest F0.5 score, and most models surpassed human benchmarks. Market index precision was 1.9%, Y Combinator 1.7x better, tier-1 firms 2.9x better.", "conclusion": "VCBench establishes a community-driven standard for reproducible and privacy-preserving evaluation of AGI in venture forecasting, available as a public evolving resource at vcbench.com."}}
{"id": "2509.14427", "categories": ["cs.LG", "cs.IR"], "pdf": "https://arxiv.org/pdf/2509.14427", "abs": "https://arxiv.org/abs/2509.14427", "authors": ["Ilyass Moummad", "Kawtar Zaher", "Lukas Rauch", "Alexis Joly"], "title": "Hashing-Baseline: Rethinking Hashing in the Age of Pretrained Models", "comment": null, "summary": "Information retrieval with compact binary embeddings, also referred to as\nhashing, is crucial for scalable fast search applications, yet state-of-the-art\nhashing methods require expensive, scenario-specific training. In this work, we\nintroduce Hashing-Baseline, a strong training-free hashing method leveraging\npowerful pretrained encoders that produce rich pretrained embeddings. We\nrevisit classical, training-free hashing techniques: principal component\nanalysis, random orthogonal projection, and threshold binarization, to produce\na strong baseline for hashing. Our approach combines these techniques with\nfrozen embeddings from state-of-the-art vision and audio encoders to yield\ncompetitive retrieval performance without any additional learning or\nfine-tuning. To demonstrate the generality and effectiveness of this approach,\nwe evaluate it on standard image retrieval benchmarks as well as a newly\nintroduced benchmark for audio hashing.", "AI": {"tldr": "Hashing-Baseline is a training-free hashing method that combines classical techniques (PCA, random orthogonal projection, threshold binarization) with frozen pretrained encoders to produce competitive binary embeddings for fast search without additional training.", "motivation": "State-of-the-art hashing methods require expensive, scenario-specific training, which limits scalability and practical deployment for fast search applications.", "method": "Combines classical training-free hashing techniques (PCA, random orthogonal projection, threshold binarization) with frozen embeddings from state-of-the-art pretrained vision and audio encoders.", "result": "Achieves competitive retrieval performance on standard image retrieval benchmarks and a newly introduced audio hashing benchmark without any additional learning or fine-tuning.", "conclusion": "The approach demonstrates that strong hashing performance can be achieved without expensive training by leveraging powerful pretrained encoders and classical techniques, providing a practical and scalable solution for information retrieval."}}
{"id": "2509.14474", "categories": ["cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2509.14474", "abs": "https://arxiv.org/abs/2509.14474", "authors": ["Meltem Subasioglu", "Nevzat Subasioglu"], "title": "From Mimicry to True Intelligence (TI) -- A New Paradigm for Artificial General Intelligence", "comment": "27 pages, 1 figure", "summary": "The debate around Artificial General Intelligence (AGI) remains open due to\ntwo fundamentally different goals: replicating human-like performance versus\nreplicating human-like cognitive processes. We argue that current\nperformance-based definitions are inadequate because they provide no clear,\nmechanism-focused roadmap for research, and they fail to properly define the\nqualitative nature of genuine intelligence. Drawing inspiration from the human\nbrain, we propose a new paradigm that shifts the focus from external mimicry to\nthe development of foundational cognitive architectures. We define True\nIntelligence (TI) as a system characterized by six core components: embodied\nsensory fusion, core directives, dynamic schemata creation, a\nhighly-interconnected multi-expert architecture, an orchestration layer, and\nlastly, the unmeasurable quality of Interconnectedness, which we hypothesize\nresults in consciousness and a subjective experience. We propose a practical,\nfive-level taxonomy of AGI based on the number of the first five measurable\ncomponents a system exhibits. This framework provides a clear path forward with\ndevelopmental milestones that directly address the challenge of building\ngenuinely intelligent systems. We contend that once a system achieves Level-5\nAGI by implementing all five measurable components, the difference between it\nand TI remains as a purely philosophical debate. For practical purposes - and\ngiven theories indicate consciousness is an emergent byproduct of integrated,\nhigher-order cognition - we conclude that a fifth-level AGI is functionally and\npractically equivalent to TI. This work synthesizes diverse insights from\nanalytical psychology, schema theory, metacognition, modern brain architectures\nand latest works in AI to provide the first holistic, mechanism-based\ndefinition of AGI that offers a clear and actionable path for the research\ncommunity.", "AI": {"tldr": "The paper proposes a new paradigm for defining True Intelligence (TI) with 6 core components, creating a 5-level AGI taxonomy based on measurable components, arguing that Level-5 AGI is functionally equivalent to TI.", "motivation": "Current performance-based AGI definitions are inadequate as they lack mechanism-focused research roadmaps and fail to define the qualitative nature of genuine intelligence, necessitating a shift from external mimicry to cognitive architecture development.", "method": "Drawing from human brain inspiration, the authors define True Intelligence with six core components and propose a five-level taxonomy of AGI based on the number of measurable components (embodied sensory fusion, core directives, dynamic schemata creation, multi-expert architecture, orchestration layer) a system exhibits.", "result": "The framework provides clear developmental milestones and a practical path for building genuinely intelligent systems, with Level-5 AGI (implementing all five measurable components) considered functionally equivalent to True Intelligence.", "conclusion": "This work offers the first holistic, mechanism-based definition of AGI that synthesizes insights from psychology, schema theory, metacognition, brain architectures, and AI, providing an actionable research path where consciousness emerges as a byproduct of integrated higher-order cognition."}}
{"id": "2509.14444", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.14444", "abs": "https://arxiv.org/abs/2509.14444", "authors": ["Herlock", "Rahimi", "Dionysis Kalogerias"], "title": "FedAVOT: Exact Distribution Alignment in Federated Learning via Masked Optimal Transport", "comment": "5 pages, 1 figure, ICASSP", "summary": "Federated Learning (FL) allows distributed model training without sharing raw\ndata, but suffers when client participation is partial. In practice, the\ndistribution of available users (\\emph{availability distribution} $q$) rarely\naligns with the distribution defining the optimization objective\n(\\emph{importance distribution} $p$), leading to biased and unstable updates\nunder classical FedAvg. We propose \\textbf{Fereated AVerage with Optimal\nTransport (\\textbf{FedAVOT})}, which formulates aggregation as a masked optimal\ntransport problem aligning $q$ and $p$. Using Sinkhorn scaling,\n\\textbf{FedAVOT} computes transport-based aggregation weights with provable\nconvergence guarantees. \\textbf{FedAVOT} achieves a standard\n$\\mathcal{O}(1/\\sqrt{T})$ rate under a nonsmooth convex FL setting, independent\nof the number of participating users per round. Our experiments confirm\ndrastically improved performance compared to FedAvg across heterogeneous,\nfairness-sensitive, and low-availability regimes, even when only two clients\nparticipate per round.", "AI": {"tldr": "FedAVOT addresses FL bias from client availability mismatch by using optimal transport to align availability and importance distributions, achieving better convergence and performance than FedAvg.", "motivation": "Federated Learning suffers from biased updates when client participation distribution doesn't match the optimization objective distribution, leading to unstable training.", "method": "Proposes FedAVOT which formulates aggregation as a masked optimal transport problem using Sinkhorn scaling to compute transport-based aggregation weights.", "result": "Achieves O(1/\u221aT) convergence rate independent of participating users per round, with drastically improved performance across heterogeneous, fairness-sensitive, and low-availability scenarios.", "conclusion": "FedAVOT provides provable convergence guarantees and superior performance over FedAvg, even with as few as two clients per round."}}
{"id": "2509.14485", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.14485", "abs": "https://arxiv.org/abs/2509.14485", "authors": ["Marko Tesic", "Yue Zhao", "Joel Z. Leibo", "Rakshit S. Trivedi", "Jose Hernandez-Orallo"], "title": "Beyond the high score: Prosocial ability profiles of multi-agent populations", "comment": null, "summary": "The development and evaluation of social capabilities in AI agents require\ncomplex environments where competitive and cooperative behaviours naturally\nemerge. While game-theoretic properties can explain why certain teams or agent\npopulations outperform others, more abstract behaviours, such as convention\nfollowing, are harder to control in training and evaluation settings. The\nMelting Pot contest is a social AI evaluation suite designed to assess the\ncooperation capabilities of AI systems. In this paper, we apply a Bayesian\napproach known as Measurement Layouts to infer the capability profiles of\nmulti-agent systems in the Melting Pot contest. We show that these capability\nprofiles not only predict future performance within the Melting Pot suite but\nalso reveal the underlying prosocial abilities of agents. Our analysis\nindicates that while higher prosocial capabilities sometimes correlate with\nbetter performance, this is not a universal trend-some lower-scoring agents\nexhibit stronger cooperation abilities. Furthermore, we find that\ntop-performing contest submissions are more likely to achieve high scores in\nscenarios where prosocial capabilities are not required. These findings,\ntogether with reports that the contest winner used a hard-coded solution\ntailored to specific environments, suggest that at least one top-performing\nteam may have optimised for conditions where cooperation was not necessary,\npotentially exploiting limitations in the evaluation framework. We provide\nrecommendations for improving the annotation of cooperation demands and propose\nfuture research directions to account for biases introduced by different\ntesting environments. Our results demonstrate that Measurement Layouts offer\nboth strong predictive accuracy and actionable insights, contributing to a more\ntransparent and generalisable approach to evaluating AI systems in complex\nsocial settings.", "AI": {"tldr": "Bayesian Measurement Layouts analyze Melting Pot contest agents, revealing that top performers may exploit evaluation limitations rather than demonstrating true cooperation skills, with some lower-scoring agents showing stronger prosocial abilities.", "motivation": "To develop better methods for evaluating social AI capabilities, particularly cooperation, in complex multi-agent environments where current evaluation frameworks may be exploited by agents optimized for specific conditions rather than genuine cooperation.", "method": "Applied Bayesian Measurement Layouts to infer capability profiles of multi-agent systems in the Melting Pot contest, analyzing performance data to reveal underlying prosocial abilities and predict future performance.", "result": "Found that higher prosocial capabilities don't always correlate with better performance, top-performing submissions excel in scenarios where cooperation isn't required, and contest winner used hard-coded solutions tailored to specific environments rather than genuine cooperation.", "conclusion": "Measurement Layouts provide accurate predictions and actionable insights for AI evaluation, but current frameworks need improved cooperation demand annotation and bias accounting to prevent exploitation and ensure genuine social capability assessment."}}
{"id": "2509.14472", "categories": ["cs.LG", "astro-ph.IM", "astro-ph.SR"], "pdf": "https://arxiv.org/pdf/2509.14472", "abs": "https://arxiv.org/abs/2509.14472", "authors": ["Mahsa Khazaei", "Azim Ahmadzadeh", "Alexei Pevtsov", "Luca Bertello", "Alexander Pevtsov"], "title": "H-Alpha Anomalyzer: An Explainable Anomaly Detector for Solar H-Alpha Observations", "comment": null, "summary": "The plethora of space-borne and ground-based observatories has provided\nastrophysicists with an unprecedented volume of data, which can only be\nprocessed at scale using advanced computing algorithms. Consequently, ensuring\nthe quality of data fed into machine learning (ML) models is critical. The\nH$\\alpha$ observations from the GONG network represent one such data stream,\nproducing several observations per minute, 24/7, since 2010. In this study, we\nintroduce a lightweight (non-ML) anomaly-detection algorithm, called H-Alpha\nAnomalyzer, designed to identify anomalous observations based on user-defined\ncriteria. Unlike many black-box algorithms, our approach highlights exactly\nwhich regions triggered the anomaly flag and quantifies the corresponding\nanomaly likelihood. For our comparative analysis, we also created and released\na dataset of 2,000 observations, equally divided between anomalous and\nnon-anomalous cases. Our results demonstrate that the proposed model not only\noutperforms existing methods but also provides explainability, enabling\nqualitative evaluation by domain experts.", "AI": {"tldr": "Lightweight non-ML anomaly detection algorithm for H-alpha solar observations that outperforms existing methods while providing explainability and highlighting specific anomalous regions.", "motivation": "The increasing volume of astrophysical data requires quality control for ML models. GONG network's H-alpha observations produce continuous data since 2010 that needs reliable anomaly detection.", "method": "Developed H-Alpha Anomalyzer - a lightweight non-machine learning algorithm that identifies anomalies based on user-defined criteria, highlights specific anomalous regions, and quantifies anomaly likelihood.", "result": "Outperforms existing methods and provides explainability for domain expert evaluation. Created and released a dataset of 2,000 observations (50% anomalous, 50% normal) for comparative analysis.", "conclusion": "The proposed model effectively detects anomalies in solar H-alpha observations while offering transparency and interpretability that black-box ML algorithms lack."}}
{"id": "2509.14507", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.14507", "abs": "https://arxiv.org/abs/2509.14507", "authors": ["Jian Chen", "Zhenyan Chen", "Xuming Hu", "Peilin Zhou", "Yining Hua", "Han Fang", "Cissy Hing Yee Choy", "Xinmei Ke", "Jingfeng Luo", "Zixuan Yuan"], "title": "DeKeyNLU: Enhancing Natural Language to SQL Generation through Task Decomposition and Keyword Extraction", "comment": null, "summary": "Natural Language to SQL (NL2SQL) provides a new model-centric paradigm that\nsimplifies database access for non-technical users by converting natural\nlanguage queries into SQL commands. Recent advancements, particularly those\nintegrating Retrieval-Augmented Generation (RAG) and Chain-of-Thought (CoT)\nreasoning, have made significant strides in enhancing NL2SQL performance.\nHowever, challenges such as inaccurate task decomposition and keyword\nextraction by LLMs remain major bottlenecks, often leading to errors in SQL\ngeneration. While existing datasets aim to mitigate these issues by fine-tuning\nmodels, they struggle with over-fragmentation of tasks and lack of\ndomain-specific keyword annotations, limiting their effectiveness. To address\nthese limitations, we present DeKeyNLU, a novel dataset which contains 1,500\nmeticulously annotated QA pairs aimed at refining task decomposition and\nenhancing keyword extraction precision for the RAG pipeline. Fine-tuned with\nDeKeyNLU, we propose DeKeySQL, a RAG-based NL2SQL pipeline that employs three\ndistinct modules for user question understanding, entity retrieval, and\ngeneration to improve SQL generation accuracy. We benchmarked multiple model\nconfigurations within DeKeySQL RAG pipeline. Experimental results demonstrate\nthat fine-tuning with DeKeyNLU significantly improves SQL generation accuracy\non both BIRD (62.31% to 69.10%) and Spider (84.2% to 88.7%) dev datasets.", "AI": {"tldr": "DeKeyNLU dataset improves NL2SQL accuracy by enhancing task decomposition and keyword extraction in RAG pipelines, achieving significant performance gains on benchmark datasets.", "motivation": "Address limitations in current NL2SQL systems where inaccurate task decomposition and keyword extraction by LLMs lead to SQL generation errors, and existing datasets suffer from over-fragmentation and lack domain-specific annotations.", "method": "Created DeKeyNLU dataset with 1,500 annotated QA pairs, then developed DeKeySQL - a RAG-based pipeline with three modules for question understanding, entity retrieval, and SQL generation, fine-tuned with the new dataset.", "result": "Fine-tuning with DeKeyNLU significantly improved SQL generation accuracy: from 62.31% to 69.10% on BIRD dev dataset and from 84.2% to 88.7% on Spider dev dataset.", "conclusion": "The DeKeyNLU dataset effectively addresses key bottlenecks in NL2SQL systems, demonstrating that targeted annotation for task decomposition and keyword extraction can substantially improve RAG pipeline performance."}}
{"id": "2509.14488", "categories": ["cs.LG", "math.OC", "90C25, 68T05", "G.1.6; C.2.4; I.2.6; F.2.1"], "pdf": "https://arxiv.org/pdf/2509.14488", "abs": "https://arxiv.org/abs/2509.14488", "authors": ["Ying Lin", "Yao Kuang", "Ahmet Alacaoglu", "Michael P. Friedlander"], "title": "Decentralized Optimization with Topology-Independent Communication", "comment": "36 pages", "summary": "Distributed optimization requires nodes to coordinate, yet full\nsynchronization scales poorly. When $n$ nodes collaborate through $m$ pairwise\nregularizers, standard methods demand $\\mathcal{O}(m)$ communications per\niteration. This paper proposes randomized local coordination: each node\nindependently samples one regularizer uniformly and coordinates only with nodes\nsharing that term. This exploits partial separability, where each regularizer\n$G_j$ depends on a subset $S_j \\subseteq \\{1,\\ldots,n\\}$ of nodes. For\ngraph-guided regularizers where $|S_j|=2$, expected communication drops to\nexactly 2 messages per iteration. This method achieves\n$\\tilde{\\mathcal{O}}(\\varepsilon^{-2})$ iterations for convex objectives and\nunder strong convexity, $\\mathcal{O}(\\varepsilon^{-1})$ to an\n$\\varepsilon$-solution and $\\mathcal{O}(\\log(1/\\varepsilon))$ to a\nneighborhood. Replacing the proximal map of the sum $\\sum_j G_j$ with the\nproximal map of a single randomly selected regularizer $G_j$ preserves\nconvergence while eliminating global coordination. Experiments validate both\nconvergence rates and communication efficiency across synthetic and real-world\ndatasets.", "AI": {"tldr": "Randomized local coordination method reduces communication from O(m) to exactly 2 messages per iteration for graph-guided regularizers by sampling one regularizer per node and coordinating only with relevant nodes.", "motivation": "Full synchronization in distributed optimization scales poorly, with standard methods requiring O(m) communications per iteration when n nodes collaborate through m pairwise regularizers.", "method": "Each node independently samples one regularizer uniformly and coordinates only with nodes sharing that term, replacing the proximal map of the sum with the proximal map of a single randomly selected regularizer.", "result": "Achieves ~O(\u03b5\u207b\u00b2) iterations for convex objectives, O(\u03b5\u207b\u00b9) to \u03b5-solution under strong convexity, and O(log(1/\u03b5)) to a neighborhood. Communication drops to exactly 2 messages per iteration for graph-guided regularizers.", "conclusion": "Randomized local coordination preserves convergence while eliminating global coordination, validated by experiments showing both convergence rates and communication efficiency on synthetic and real-world datasets."}}
{"id": "2509.14546", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.14546", "abs": "https://arxiv.org/abs/2509.14546", "authors": ["Zhilun Zhou", "Jing Yi Wang", "Nicholas Sukiennik", "Chen Gao", "Fengli Xu", "Yong Li", "James Evans"], "title": "Rationality Check! Benchmarking the Rationality of Large Language Models", "comment": null, "summary": "Large language models (LLMs), a recent advance in deep learning and machine\nintelligence, have manifested astonishing capacities, now considered among the\nmost promising for artificial general intelligence. With human-like\ncapabilities, LLMs have been used to simulate humans and serve as AI assistants\nacross many applications. As a result, great concern has arisen about whether\nand under what circumstances LLMs think and behave like real human agents.\nRationality is among the most important concepts in assessing human behavior,\nboth in thinking (i.e., theoretical rationality) and in taking action (i.e.,\npractical rationality). In this work, we propose the first benchmark for\nevaluating the omnibus rationality of LLMs, covering a wide range of domains\nand LLMs. The benchmark includes an easy-to-use toolkit, extensive experimental\nresults, and analysis that illuminates where LLMs converge and diverge from\nidealized human rationality. We believe the benchmark can serve as a\nfoundational tool for both developers and users of LLMs.", "AI": {"tldr": "First benchmark for evaluating omnibus rationality of LLMs across theoretical and practical domains, with toolkit and analysis showing where LLMs converge/diverge from human rationality.", "motivation": "Concern about whether and under what circumstances LLMs think and behave like real human agents, given their human-like capabilities and increasing use as AI assistants.", "method": "Proposed benchmark covering wide range of domains and LLMs, including easy-to-use toolkit and extensive experimental evaluation.", "result": "Analysis illuminates where LLMs converge and diverge from idealized human rationality across different domains.", "conclusion": "Benchmark serves as foundational tool for both developers and users of LLMs to assess their rationality."}}
{"id": "2509.14519", "categories": ["cs.LG", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2509.14519", "abs": "https://arxiv.org/abs/2509.14519", "authors": ["Wadduwage Shanika Perera", "Haodi Jiang"], "title": "BEACON: Behavioral Malware Classification with Large Language Model Embeddings and Deep Learning", "comment": null, "summary": "Malware is becoming increasingly complex and widespread, making it essential\nto develop more effective and timely detection methods. Traditional static\nanalysis often fails to defend against modern threats that employ code\nobfuscation, polymorphism, and other evasion techniques. In contrast,\nbehavioral malware detection, which monitors runtime activities, provides a\nmore reliable and context-aware solution. In this work, we propose BEACON, a\nnovel deep learning framework that leverages large language models (LLMs) to\ngenerate dense, contextual embeddings from raw sandbox-generated behavior\nreports. These embeddings capture semantic and structural patterns of each\nsample and are processed by a one-dimensional convolutional neural network (1D\nCNN) for multi-class malware classification. Evaluated on the Avast-CTU Public\nCAPE Dataset, our framework consistently outperforms existing methods,\nhighlighting the effectiveness of LLM-based behavioral embeddings and the\noverall design of BEACON for robust malware classification.", "AI": {"tldr": "BEACON is a deep learning framework that uses large language models to generate behavioral embeddings from malware sandbox reports, achieving superior malware classification performance compared to existing methods.", "motivation": "Traditional static malware analysis fails against modern obfuscation and evasion techniques, creating a need for more reliable behavioral detection methods that monitor runtime activities.", "method": "Proposes BEACON framework that leverages LLMs to generate dense contextual embeddings from raw sandbox behavior reports, then uses 1D CNN for multi-class malware classification.", "result": "Evaluated on Avast-CTU Public CAPE Dataset, consistently outperforms existing methods, demonstrating effectiveness of LLM-based behavioral embeddings.", "conclusion": "The framework highlights the effectiveness of combining LLMs with behavioral analysis for robust malware classification against modern threats."}}
{"id": "2509.14547", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.14547", "abs": "https://arxiv.org/abs/2509.14547", "authors": ["Yi Lin", "Lujin Zhao", "Yijie Shi"], "title": "(P)rior(D)yna(F)low: A Priori Dynamic Workflow Construction via Multi-Agent Collaboration", "comment": null, "summary": "Recent studies have shown that carefully designed workflows coordinating\nlarge language models(LLMs) significantly enhance task-solving capabilities\ncompared to using a single model. While an increasing number of works focus on\nautonomous workflow construction, most existing approaches rely solely on\nhistorical experience, leading to limitations in efficiency and adaptability.\nWe argue that while historical experience is valuable, workflow construction\nshould also flexibly respond to the unique characteristics of each task. To\nthis end, we propose an a priori dynamic framework for automated workflow\nconstruction. Our framework first leverages Q-table learning to optimize the\ndecision space, guiding agent decisions and enabling effective use of\nhistorical experience. At the same time, agents evaluate the current task\nprogress and make a priori decisions regarding the next executing agent,\nallowing the system to proactively select the more suitable workflow structure\nfor each given task. Additionally, we incorporate mechanisms such as cold-start\ninitialization, early stopping, and pruning to further improve system\nefficiency. Experimental evaluations on four benchmark datasets demonstrate the\nfeasibility and effectiveness of our approach. Compared to state-of-the-art\nbaselines, our method achieves an average improvement of 4.05%, while reducing\nworkflow construction and inference costs to only 30.68%-48.31% of those\nrequired by existing methods.", "AI": {"tldr": "Proposes a dynamic framework for automated LLM workflow construction that combines Q-table learning with a priori decision-making to optimize task-specific workflows, improving performance while reducing costs.", "motivation": "Existing autonomous workflow construction methods rely too heavily on historical experience, lacking efficiency and adaptability to unique task characteristics.", "method": "Uses Q-table learning to optimize decision space and guide agent decisions, with a priori dynamic framework that evaluates task progress to proactively select optimal workflow structures. Includes cold-start initialization, early stopping, and pruning mechanisms.", "result": "Achieves 4.05% average improvement over state-of-the-art baselines while reducing workflow construction and inference costs to 30.68%-48.31% of existing methods.", "conclusion": "The proposed framework effectively combines historical experience with task-specific adaptability, demonstrating significant performance gains and cost efficiency in automated workflow construction."}}
{"id": "2509.14536", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.14536", "abs": "https://arxiv.org/abs/2509.14536", "authors": ["Muhammad Awais Ali", "Marlon Dumas", "Fredrik Milani"], "title": "Predicting Case Suffixes With Activity Start and End Times: A Sweep-Line Based Approach", "comment": null, "summary": "Predictive process monitoring techniques support the operational decision\nmaking by predicting future states of ongoing cases of a business process. A\nsubset of these techniques predict the remaining sequence of activities of an\nongoing case (case suffix prediction). Existing approaches for case suffix\nprediction generate sequences of activities with a single timestamp (e.g. the\nend timestamp). This output is insufficient for resource capacity planning,\nwhere we need to reason about the periods of time when resources will be busy\nperforming work. This paper introduces a technique for predicting case suffixes\nconsisting of activities with start and end timestamps. In other words, the\nproposed technique predicts both the waiting time and the processing time of\neach activity. Since the waiting time of an activity in a case depends on how\nbusy resources are in other cases, the technique adopts a sweep-line approach,\nwherein the suffixes of all ongoing cases in the process are predicted in\nlockstep, rather than predictions being made for each case in isolation. An\nevaluation on real-life and synthetic datasets compares the accuracy of\ndifferent instantiations of this approach, demonstrating the advantages of a\nmulti-model approach to case suffix prediction.", "AI": {"tldr": "Proposes a technique for predicting business process case suffixes with start and end timestamps to enable resource capacity planning, using a sweep-line approach that predicts all ongoing cases simultaneously rather than in isolation.", "motivation": "Existing case suffix prediction approaches only generate sequences with single timestamps, which is insufficient for resource capacity planning that requires reasoning about when resources will be busy performing work.", "method": "Introduces a sweep-line approach that predicts case suffixes consisting of activities with both start and end timestamps, predicting waiting and processing times for all ongoing cases in lockstep rather than in isolation.", "result": "Evaluation on real-life and synthetic datasets shows advantages of this multi-model approach for case suffix prediction accuracy.", "conclusion": "The proposed technique provides more comprehensive predictions suitable for resource capacity planning by capturing both waiting and processing times through simultaneous prediction of all ongoing cases."}}
{"id": "2509.14594", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.14594", "abs": "https://arxiv.org/abs/2509.14594", "authors": ["Yidan Sun", "Viktor Schlegel", "Srinivasan Nandakumar", "Iqra Zahid", "Yuping Wu", "Yulong Wu", "Hao Li", "Jie Zhang", "Warren Del-Pinto", "Goran Nenadic", "Siew Kei Lam", "Anil Anthony Bharath"], "title": "SynBench: A Benchmark for Differentially Private Text Generation", "comment": "15 pages", "summary": "Data-driven decision support in high-stakes domains like healthcare and\nfinance faces significant barriers to data sharing due to regulatory,\ninstitutional, and privacy concerns. While recent generative AI models, such as\nlarge language models, have shown impressive performance in open-domain tasks,\ntheir adoption in sensitive environments remains limited by unpredictable\nbehaviors and insufficient privacy-preserving datasets for benchmarking.\nExisting anonymization methods are often inadequate, especially for\nunstructured text, as redaction and masking can still allow re-identification.\nDifferential Privacy (DP) offers a principled alternative, enabling the\ngeneration of synthetic data with formal privacy assurances. In this work, we\naddress these challenges through three key contributions. First, we introduce a\ncomprehensive evaluation framework with standardized utility and fidelity\nmetrics, encompassing nine curated datasets that capture domain-specific\ncomplexities such as technical jargon, long-context dependencies, and\nspecialized document structures. Second, we conduct a large-scale empirical\nstudy benchmarking state-of-the-art DP text generation methods and LLMs of\nvarying sizes and different fine-tuning strategies, revealing that high-quality\ndomain-specific synthetic data generation under DP constraints remains an\nunsolved challenge, with performance degrading as domain complexity increases.\nThird, we develop a membership inference attack (MIA) methodology tailored for\nsynthetic text, providing first empirical evidence that the use of public\ndatasets - potentially present in pre-training corpora - can invalidate claimed\nprivacy guarantees. Our findings underscore the urgent need for rigorous\nprivacy auditing and highlight persistent gaps between open-domain and\nspecialist evaluations, informing responsible deployment of generative AI in\nprivacy-sensitive, high-stakes settings.", "AI": {"tldr": "This paper addresses privacy challenges in high-stakes domains by developing a comprehensive evaluation framework for differentially private text generation, benchmarking state-of-the-art methods, and revealing privacy risks from public data contamination.", "motivation": "Data sharing in healthcare and finance faces regulatory and privacy barriers, with existing anonymization methods inadequate for unstructured text. Differential privacy offers formal privacy assurances but lacks proper evaluation frameworks for domain-specific synthetic data generation.", "method": "Three key contributions: 1) Comprehensive evaluation framework with standardized utility/fidelity metrics across 9 curated domain-specific datasets, 2) Large-scale empirical study benchmarking DP text generation methods and LLMs of varying sizes, 3) Development of membership inference attack methodology for synthetic text.", "result": "High-quality domain-specific synthetic data generation under DP constraints remains an unsolved challenge, with performance degrading as domain complexity increases. Empirical evidence shows public datasets in pre-training corpora can invalidate claimed privacy guarantees.", "conclusion": "Urgent need for rigorous privacy auditing and persistent gaps between open-domain and specialist evaluations, informing responsible deployment of generative AI in privacy-sensitive, high-stakes settings."}}
{"id": "2509.14562", "categories": ["cs.LG", "math.OC"], "pdf": "https://arxiv.org/pdf/2509.14562", "abs": "https://arxiv.org/abs/2509.14562", "authors": ["Feihu Huang", "Yuning Luo", "Songcan Chen"], "title": "LiMuon: Light and Fast Muon Optimizer for Large Models", "comment": "28 pages", "summary": "Large models recently are widely applied in artificial intelligence, so\nefficient training of large models has received widespread attention. More\nrecently, a useful Muon optimizer is specifically designed for\nmatrix-structured parameters of large models. Although some works have begun to\nstudying Muon optimizer, the existing Muon and its variants still suffer from\nhigh sample complexity or high memory for large models. To fill this gap, we\npropose a light and fast Muon (LiMuon) optimizer for training large models,\nwhich builds on the momentum-based variance reduced technique and randomized\nSingular Value Decomposition (SVD). Our LiMuon optimizer has a lower memory\nthan the current Muon and its variants. Moreover, we prove that our LiMuon has\na lower sample complexity of $O(\\epsilon^{-3})$ for finding an\n$\\epsilon$-stationary solution of non-convex stochastic optimization under the\nsmooth condition. Recently, the existing convergence analysis of Muon optimizer\nmainly relies on the strict Lipschitz smooth assumption, while some artificial\nintelligence tasks such as training large language models (LLMs) do not satisfy\nthis condition. We also proved that our LiMuon optimizer has a sample\ncomplexity of $O(\\epsilon^{-3})$ under the generalized smooth condition.\nNumerical experimental results on training DistilGPT2 and ViT models verify\nefficiency of our LiMuon optimizer.", "AI": {"tldr": "LiMuon optimizer: a light and fast Muon variant using momentum-based variance reduction and randomized SVD for efficient large model training with lower memory and O(\u03b5\u207b\u00b3) sample complexity under both smooth and generalized smooth conditions.", "motivation": "Existing Muon optimizers suffer from high sample complexity or high memory requirements for large models, and current convergence analysis relies on strict Lipschitz smooth assumptions that don't hold for tasks like LLM training.", "method": "Proposes LiMuon optimizer based on momentum-based variance reduced technique and randomized Singular Value Decomposition (SVD) to reduce memory usage while maintaining efficiency.", "result": "LiMuon achieves lower memory than current Muon variants and O(\u03b5\u207b\u00b3) sample complexity for finding \u03b5-stationary solutions under both smooth and generalized smooth conditions. Experimental results on DistilGPT2 and ViT models verify efficiency.", "conclusion": "LiMuon provides an effective solution for efficient large model training with reduced memory requirements and proven convergence guarantees under more realistic smoothness conditions encountered in AI tasks like LLM training."}}
{"id": "2509.14647", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.14647", "abs": "https://arxiv.org/abs/2509.14647", "authors": ["NVJK Kartik", "Garvit Sapra", "Rishav Hada", "Nikhil Pareek"], "title": "AgentCompass: Towards Reliable Evaluation of Agentic Workflows in Production", "comment": null, "summary": "With the growing adoption of Large Language Models (LLMs) in automating\ncomplex, multi-agent workflows, organizations face mounting risks from errors,\nemergent behaviors, and systemic failures that current evaluation methods fail\nto capture. We present AgentCompass, the first evaluation framework designed\nspecifically for post-deployment monitoring and debugging of agentic workflows.\nAgentCompass models the reasoning process of expert debuggers through a\nstructured, multi-stage analytical pipeline: error identification and\ncategorization, thematic clustering, quantitative scoring, and strategic\nsummarization. The framework is further enhanced with a dual memory\nsystem-episodic and semantic-that enables continual learning across executions.\nThrough collaborations with design partners, we demonstrate the framework's\npractical utility on real-world deployments, before establishing its efficacy\nagainst the publicly available TRAIL benchmark. AgentCompass achieves\nstate-of-the-art results on key metrics, while uncovering critical issues\nmissed in human annotations, underscoring its role as a robust,\ndeveloper-centric tool for reliable monitoring and improvement of agentic\nsystems in production.", "AI": {"tldr": "AgentCompass is the first evaluation framework for post-deployment monitoring and debugging of LLM-based multi-agent workflows, featuring a structured analytical pipeline and dual memory system that achieves state-of-the-art results.", "motivation": "Current evaluation methods fail to capture risks from errors, emergent behaviors, and systemic failures in LLM-based multi-agent workflows, creating a need for robust post-deployment monitoring tools.", "method": "A structured multi-stage analytical pipeline modeling expert debuggers: error identification/categorization, thematic clustering, quantitative scoring, and strategic summarization, enhanced with dual episodic/semantic memory for continual learning.", "result": "Achieves state-of-the-art results on TRAIL benchmark, uncovers critical issues missed in human annotations, and demonstrates practical utility through real-world deployments with design partners.", "conclusion": "AgentCompass serves as a robust, developer-centric tool for reliable monitoring and improvement of agentic systems in production environments."}}
{"id": "2509.14563", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.14563", "abs": "https://arxiv.org/abs/2509.14563", "authors": ["Shiyuan Luo", "Runlong Yu", "Chonghao Qiu", "Rahul Ghosh", "Robert Ladwig", "Paul C. Hanson", "Yiqun Xie", "Xiaowei Jia"], "title": "Learning to Retrieve for Environmental Knowledge Discovery: An Augmentation-Adaptive Self-Supervised Learning Framework", "comment": null, "summary": "The discovery of environmental knowledge depends on labeled task-specific\ndata, but is often constrained by the high cost of data collection. Existing\nmachine learning approaches usually struggle to generalize in data-sparse or\natypical conditions. To this end, we propose an Augmentation-Adaptive\nSelf-Supervised Learning (A$^2$SL) framework, which retrieves relevant\nobservational samples to enhance modeling of the target ecosystem.\nSpecifically, we introduce a multi-level pairwise learning loss to train a\nscenario encoder that captures varying degrees of similarity among scenarios.\nThese learned similarities drive a retrieval mechanism that supplements a\ntarget scenario with relevant data from different locations or time periods.\nFurthermore, to better handle variable scenarios, particularly under atypical\nor extreme conditions where traditional models struggle, we design an\naugmentation-adaptive mechanism that selectively enhances these scenarios\nthrough targeted data augmentation. Using freshwater ecosystems as a case\nstudy, we evaluate A$^2$SL in modeling water temperature and dissolved oxygen\ndynamics in real-world lakes. Experimental results show that A$^2$SL\nsignificantly improves predictive accuracy and enhances robustness in\ndata-scarce and atypical scenarios. Although this study focuses on freshwater\necosystems, the A$^2$SL framework offers a broadly applicable solution in\nvarious scientific domains.", "AI": {"tldr": "A$^2$SL framework improves environmental modeling by retrieving relevant observational data and using adaptive augmentation for data-scarce and atypical conditions, demonstrated with freshwater ecosystem case study.", "motivation": "High cost of environmental data collection and poor generalization of existing ML approaches in data-sparse or atypical conditions.", "method": "Augmentation-Adaptive Self-Supervised Learning with multi-level pairwise learning loss for scenario similarity, retrieval mechanism, and selective data augmentation for extreme conditions.", "result": "Significantly improves predictive accuracy and robustness for water temperature and dissolved oxygen modeling in real-world lakes under data-scarce scenarios.", "conclusion": "A$^2$SL provides a broadly applicable solution for various scientific domains beyond freshwater ecosystems, addressing data scarcity and atypical condition challenges."}}
{"id": "2509.14662", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.14662", "abs": "https://arxiv.org/abs/2509.14662", "authors": ["Ming Li", "Nan Zhang", "Chenrui Fan", "Hong Jiao", "Yanbin Fu", "Sydney Peters", "Qingshu Xu", "Robert Lissitz", "Tianyi Zhou"], "title": "Understanding the Thinking Process of Reasoning Models: A Perspective from Schoenfeld's Episode Theory", "comment": "EMNLP2025 main, Camera-ready", "summary": "While Large Reasoning Models (LRMs) generate extensive chain-of-thought\nreasoning, we lack a principled framework for understanding how these thoughts\nare structured. In this paper, we introduce a novel approach by applying\nSchoenfeld's Episode Theory, a classic cognitive framework for human\nmathematical problem-solving, to analyze the reasoning traces of LRMs. We\nannotated thousands of sentences and paragraphs from model-generated solutions\nto math problems using seven cognitive labels (e.g., Plan, Implement, Verify).\nThe result is the first publicly available benchmark for the fine-grained\nanalysis of machine reasoning, including a large annotated corpus and detailed\nannotation guidebooks. Our preliminary analysis reveals distinct patterns in\nLRM reasoning, such as the transition dynamics between cognitive states. This\nframework provides a theoretically grounded methodology for interpreting LRM\ncognition and enables future work on more controllable and transparent\nreasoning systems.", "AI": {"tldr": "Applying Schoenfeld's Episode Theory to analyze Large Reasoning Models' thought structures through cognitive labeling of math problem solutions.", "motivation": "Lack of principled framework to understand how Large Reasoning Models structure their chain-of-thought reasoning, despite generating extensive reasoning traces.", "method": "Annotated thousands of sentences/paragraphs from model-generated math solutions using seven cognitive labels (Plan, Implement, Verify, etc.) based on Schoenfeld's Episode Theory.", "result": "Created first publicly available benchmark for fine-grained analysis of machine reasoning, including large annotated corpus and annotation guides. Preliminary analysis reveals distinct patterns in LRM reasoning and transition dynamics between cognitive states.", "conclusion": "Provides theoretically grounded methodology for interpreting LRM cognition and enables future work on more controllable and transparent reasoning systems."}}
{"id": "2509.14568", "categories": ["cs.LG", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2509.14568", "abs": "https://arxiv.org/abs/2509.14568", "authors": ["Hai Siong Tan", "Kuancheng Wang", "Rafe McBeth"], "title": "Evidential Physics-Informed Neural Networks for Scientific Discovery", "comment": "15 pages, 4 figures", "summary": "We present the fundamental theory and implementation guidelines underlying\nEvidential Physics-Informed Neural Network (E-PINN) -- a novel class of\nuncertainty-aware PINN. It leverages the marginal distribution loss function of\nevidential deep learning for estimating uncertainty of outputs, and infers\nunknown parameters of the PDE via a learned posterior distribution. Validating\nour model on two illustrative case studies -- the 1D Poisson equation with a\nGaussian source and the 2D Fisher-KPP equation, we found that E-PINN generated\nempirical coverage probabilities that were calibrated significantly better than\nBayesian PINN and Deep Ensemble methods. To demonstrate real-world\napplicability, we also present a brief case study on applying E-PINN to analyze\nclinical glucose-insulin datasets that have featured in medical research on\ndiabetes pathophysiology.", "AI": {"tldr": "E-PINN is a novel uncertainty-aware Physics-Informed Neural Network that uses evidential deep learning for uncertainty estimation and parameter inference, outperforming Bayesian PINN and Deep Ensemble methods in calibration.", "motivation": "To develop a more reliable uncertainty-aware PINN framework that can better quantify uncertainty in PDE solutions and parameter estimation, addressing limitations of existing methods like Bayesian PINN and Deep Ensembles.", "method": "Leverages marginal distribution loss function from evidential deep learning to estimate output uncertainty and infers unknown PDE parameters through learned posterior distributions. Validated on 1D Poisson equation and 2D Fisher-KPP equation.", "result": "E-PINN generated significantly better calibrated empirical coverage probabilities compared to Bayesian PINN and Deep Ensemble methods. Demonstrated real-world applicability on clinical glucose-insulin datasets for diabetes research.", "conclusion": "E-PINN provides a superior uncertainty quantification framework for physics-informed neural networks with better calibration performance and practical applicability to real-world scientific problems."}}
{"id": "2509.14693", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.14693", "abs": "https://arxiv.org/abs/2509.14693", "authors": ["Song Xu", "Yilun Liu", "Minggui He", "Mingchen Dai", "Ziang Chen", "Chunguang Zhao", "Jingzhou Du", "Shimin Tao", "Weibin Meng", "Shenglin Zhang", "Yongqian Sun", "Boxing Chen", "Daimeng Wei"], "title": "RationAnomaly: Log Anomaly Detection with Rationality via Chain-of-Thought and Reinforcement Learning", "comment": "5 pages, 3 figures", "summary": "Logs constitute a form of evidence signaling the operational status of\nsoftware systems. Automated log anomaly detection is crucial for ensuring the\nreliability of modern software systems. However, existing approaches face\nsignificant limitations: traditional deep learning models lack interpretability\nand generalization, while methods leveraging Large Language Models are often\nhindered by unreliability and factual inaccuracies. To address these issues, we\npropose RationAnomaly, a novel framework that enhances log anomaly detection by\nsynergizing Chain-of-Thought (CoT) fine-tuning with reinforcement learning. Our\napproach first instills expert-like reasoning patterns using CoT-guided\nsupervised fine-tuning, grounded in a high-quality dataset corrected through a\nrigorous expert-driven process. Subsequently, a reinforcement learning phase\nwith a multi-faceted reward function optimizes for accuracy and logical\nconsistency, effectively mitigating hallucinations. Experimentally,\nRationAnomaly outperforms state-of-the-art baselines, achieving superior\nF1-scores on key benchmarks while providing transparent, step-by-step\nanalytical outputs. We have released the corresponding resources, including\ncode and datasets.", "AI": {"tldr": "RationAnomaly is a novel framework that combines Chain-of-Thought fine-tuning with reinforcement learning to improve log anomaly detection, addressing interpretability and reliability issues in existing methods.", "motivation": "Existing log anomaly detection approaches face limitations: traditional deep learning models lack interpretability and generalization, while LLM-based methods suffer from unreliability and factual inaccuracies.", "method": "The framework uses CoT-guided supervised fine-tuning with expert-corrected data to instill reasoning patterns, followed by reinforcement learning with a multi-faceted reward function to optimize accuracy and logical consistency while mitigating hallucinations.", "result": "RationAnomaly outperforms state-of-the-art baselines, achieving superior F1-scores on key benchmarks while providing transparent, step-by-step analytical outputs.", "conclusion": "The proposed framework successfully addresses the limitations of existing methods by combining CoT fine-tuning with reinforcement learning, resulting in improved accuracy, reliability, and interpretability for log anomaly detection."}}
{"id": "2509.14577", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.14577", "abs": "https://arxiv.org/abs/2509.14577", "authors": ["Yang Xu", "Junpeng Li", "Changchun Hua", "Yana Yang"], "title": "Structure-Preserving Margin Distribution Learning for High-Order Tensor Data with Low-Rank Decomposition", "comment": null, "summary": "The Large Margin Distribution Machine (LMDM) is a recent advancement in\nclassifier design that optimizes not just the minimum margin (as in SVM) but\nthe entire margin distribution, thereby improving generalization. However,\nexisting LMDM formulations are limited to vectorized inputs and struggle with\nhigh-dimensional tensor data due to the need for flattening, which destroys the\ndata's inherent multi-mode structure and increases computational burden. In\nthis paper, we propose a Structure-Preserving Margin Distribution Learning for\nHigh-Order Tensor Data with Low-Rank Decomposition (SPMD-LRT) that operates\ndirectly on tensor representations without vectorization. The SPMD-LRT\npreserves multi-dimensional spatial structure by incorporating first-order and\nsecond-order tensor statistics (margin mean and variance) into the objective,\nand it leverages low-rank tensor decomposition techniques including rank-1(CP),\nhigher-rank CP, and Tucker decomposition to parameterize the weight tensor. An\nalternating optimization (double-gradient descent) algorithm is developed to\nefficiently solve the SPMD-LRT, iteratively updating factor matrices and core\ntensor. This approach enables SPMD-LRT to maintain the structural information\nof high-order data while optimizing margin distribution for improved\nclassification. Extensive experiments on diverse datasets (including MNIST,\nimages and fMRI neuroimaging) demonstrate that SPMD-LRT achieves superior\nclassification accuracy compared to conventional SVM, vector-based LMDM, and\nprior tensor-based SVM extensions (Support Tensor Machines and Support Tucker\nMachines). Notably, SPMD-LRT with Tucker decomposition attains the highest\naccuracy, highlighting the benefit of structure preservation. These results\nconfirm the effectiveness and robustness of SPMD-LRT in handling\nhigh-dimensional tensor data for classification.", "AI": {"tldr": "SPMD-LRT is a tensor-based classifier that preserves data structure while optimizing margin distribution, outperforming traditional methods on high-dimensional tensor data.", "motivation": "Existing LMDM methods require flattening tensor data, destroying structural information and increasing computational burden for high-dimensional data.", "method": "Proposes SPMD-LRT that operates directly on tensor representations, incorporates first/second-order margin statistics, and uses low-rank tensor decomposition (CP/Tucker) with alternating optimization.", "result": "Superior classification accuracy on MNIST, images, and fMRI data compared to SVM, vector-based LMDM, and tensor SVM extensions. Tucker decomposition achieved highest accuracy.", "conclusion": "SPMD-LRT effectively handles high-dimensional tensor data while preserving structural information and optimizing margin distribution for improved classification performance."}}
{"id": "2509.14704", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.14704", "abs": "https://arxiv.org/abs/2509.14704", "authors": ["Masaharu Mizumoto", "Dat Nguyen", "Zhiheng Han", "Jiyuan Fang", "Heyuan Guan", "Xingfu Li", "Naoya Shiraishi", "Xuyang Tian", "Yo Nakawake", "Le Minh Nguyen"], "title": "The NazoNazo Benchmark: A Cost-Effective and Extensible Test of Insight-Based Reasoning in LLMs", "comment": null, "summary": "Benchmark saturation and contamination undermine confidence in LLM\nevaluation. We present Nazonazo, a cost-effective and extensible benchmark\nbuilt from Japanese children's riddles to test insight-based reasoning. Items\nare short (mostly one sentence), require no specialized domain knowledge, and\ncan be generated at scale, enabling rapid refresh of blind sets when leakage is\nsuspected. We evaluate 38 frontier models and 126 adults on 120 riddles. No\nmodel except for GPT-5 is comparable to human performance, which achieves a\n52.9% mean accuracy. Model comparison on extended 201 items shows that\nreasoning models significantly outperform non-reasoning peers, while model size\nshows no reliable association with accuracy. Beyond aggregate accuracy, an\ninformal candidate-tracking analysis of thought logs reveals many cases of\nverification failure: models often produce the correct solution among\nintermediate candidates yet fail to select it as the final answer, which we\nillustrate with representative examples observed in multiple models. Nazonazo\nthus offers a cost-effective, scalable, and easily renewable benchmark format\nthat addresses the current evaluation crisis while also suggesting a recurrent\nmeta-cognitive weakness, providing clear targets for future control and\ncalibration methods.", "AI": {"tldr": "Nazonazo is a Japanese riddle-based benchmark for testing insight-based reasoning in LLMs, showing most models underperform humans except GPT-5, with reasoning models outperforming non-reasoning ones regardless of size.", "motivation": "Address benchmark saturation and contamination issues in LLM evaluation by creating a cost-effective, extensible benchmark that tests insight-based reasoning.", "method": "Built benchmark from Japanese children's riddles - short items requiring no specialized knowledge, scalable generation. Evaluated 38 frontier models and 126 humans on 120 riddles, with extended analysis on 201 items including thought log analysis.", "result": "No model except GPT-5 comparable to human performance (52.9% mean accuracy). Reasoning models significantly outperform non-reasoning peers, model size shows no reliable association with accuracy. Many cases of verification failure observed where models produce correct solution but fail to select it.", "conclusion": "Nazonazo provides cost-effective, scalable, renewable benchmark format addressing evaluation crisis while revealing meta-cognitive weaknesses in models, offering clear targets for future control and calibration methods."}}
{"id": "2509.14585", "categories": ["cs.LG", "math.OC"], "pdf": "https://arxiv.org/pdf/2509.14585", "abs": "https://arxiv.org/abs/2509.14585", "authors": ["Minh Vu", "Konstantinos Slavakis"], "title": "Online reinforcement learning via sparse Gaussian mixture model Q-functions", "comment": null, "summary": "This paper introduces a structured and interpretable online policy-iteration\nframework for reinforcement learning (RL), built around the novel class of\nsparse Gaussian mixture model Q-functions (S-GMM-QFs). Extending earlier work\nthat trained GMM-QFs offline, the proposed framework develops an online scheme\nthat leverages streaming data to encourage exploration. Model complexity is\nregulated through sparsification by Hadamard overparametrization, which\nmitigates overfitting while preserving expressiveness. The parameter space of\nS-GMM-QFs is naturally endowed with a Riemannian manifold structure, allowing\nfor principled parameter updates via online gradient descent on a smooth\nobjective. Numerical tests show that S-GMM-QFs match the performance of dense\ndeep RL (DeepRL) methods on standard benchmarks while using significantly fewer\nparameters, and maintain strong performance even in low-parameter-count regimes\nwhere sparsified DeepRL methods fail to generalize.", "AI": {"tldr": "Online RL framework using sparse Gaussian mixture model Q-functions that achieves performance comparable to dense DeepRL methods with fewer parameters and better generalization in low-parameter regimes.", "motivation": "To develop an interpretable and structured online policy-iteration framework that extends previous offline GMM-QF work to online settings, enabling better exploration and mitigating overfitting while maintaining expressiveness.", "method": "Proposes sparse Gaussian mixture model Q-functions (S-GMM-QFs) with sparsification via Hadamard overparametrization, uses Riemannian manifold structure for principled parameter updates through online gradient descent on smooth objectives.", "result": "S-GMM-QFs match performance of dense DeepRL methods on standard benchmarks while using significantly fewer parameters, and maintain strong performance in low-parameter-count regimes where sparsified DeepRL methods fail.", "conclusion": "The framework provides an effective online RL approach that combines interpretability, parameter efficiency, and strong generalization capabilities through structured sparse modeling and Riemannian optimization."}}
{"id": "2509.14750", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.14750", "abs": "https://arxiv.org/abs/2509.14750", "authors": ["Letian Zhang", "Guanghao Meng", "Xudong Ren", "Yiming Wang", "Shu-Tao Xia"], "title": "Enhancing Retrieval Augmentation via Adversarial Collaboration", "comment": null, "summary": "Retrieval-augmented Generation (RAG) is a prevalent approach for\ndomain-specific LLMs, yet it is often plagued by \"Retrieval Hallucinations\"--a\nphenomenon where fine-tuned models fail to recognize and act upon poor-quality\nretrieved documents, thus undermining performance. To address this, we propose\nthe Adversarial Collaboration RAG (AC-RAG) framework. AC-RAG employs two\nheterogeneous agents: a generalist Detector that identifies knowledge gaps, and\na domain-specialized Resolver that provides precise solutions. Guided by a\nmoderator, these agents engage in an adversarial collaboration, where the\nDetector's persistent questioning challenges the Resolver's expertise. This\ndynamic process allows for iterative problem dissection and refined knowledge\nretrieval. Extensive experiments show that AC-RAG significantly improves\nretrieval accuracy and outperforms state-of-the-art RAG methods across various\nvertical domains.", "AI": {"tldr": "AC-RAG framework uses adversarial collaboration between two agents (Detector and Resolver) to address retrieval hallucinations in RAG systems, significantly improving performance across domains.", "motivation": "Address retrieval hallucinations in RAG systems where models fail to recognize and act upon poor-quality retrieved documents, which undermines performance.", "method": "Propose AC-RAG framework with two heterogeneous agents: a generalist Detector that identifies knowledge gaps, and a domain-specialized Resolver that provides solutions. They engage in adversarial collaboration guided by a moderator, with iterative questioning and problem dissection.", "result": "Extensive experiments show AC-RAG significantly improves retrieval accuracy and outperforms state-of-the-art RAG methods across various vertical domains.", "conclusion": "The adversarial collaboration approach effectively addresses retrieval hallucinations and enhances RAG system performance through iterative problem dissection and refined knowledge retrieval."}}
{"id": "2509.14600", "categories": ["cs.LG", "physics.bio-ph", "I.2.1"], "pdf": "https://arxiv.org/pdf/2509.14600", "abs": "https://arxiv.org/abs/2509.14600", "authors": ["Alexander Aghili", "Andy Bruce", "Daniel Sabo", "Razvan Marinescu"], "title": "TICA-Based Free Energy Matching for Machine-Learned Molecular Dynamics", "comment": "Proceedings of the ICML 2025 Workshop on Multi-modal Foundation\n  Models and Large Language Models for Life Sciences, Vancouver, Canada. 2025.\n  Copyright 2025 by the author(s). 4 Pages 5 Figures", "summary": "Molecular dynamics (MD) simulations provide atomistic insight into\nbiomolecular systems but are often limited by high computational costs required\nto access long timescales. Coarse-grained machine learning models offer a\npromising avenue for accelerating sampling, yet conventional force matching\napproaches often fail to capture the full thermodynamic landscape as fitting a\nmodel on the gradient may not fit the absolute differences between low-energy\nconformational states. In this work, we incorporate a complementary energy\nmatching term into the loss function. We evaluate our framework on the\nChignolin protein using the CGSchNet model, systematically varying the weight\nof the energy loss term. While energy matching did not yield statistically\nsignificant improvements in accuracy, it revealed distinct tendencies in how\nmodels generalize the free energy surface. Our results suggest future\nopportunities to enhance coarse-grained modeling through improved energy\nestimation techniques and multi-modal loss formulations.", "AI": {"tldr": "Adding energy matching to coarse-grained machine learning models for molecular dynamics shows potential for improving free energy surface generalization but no significant accuracy gains.", "motivation": "Molecular dynamics simulations are computationally expensive for long timescales, and conventional force matching approaches often fail to capture the full thermodynamic landscape of biomolecular systems.", "method": "Incorporated a complementary energy matching term into the loss function of CGSchNet model, systematically varying the weight of the energy loss term, and evaluated on Chignolin protein.", "result": "Energy matching did not yield statistically significant improvements in accuracy but revealed distinct tendencies in how models generalize the free energy surface.", "conclusion": "Future opportunities exist to enhance coarse-grained modeling through improved energy estimation techniques and multi-modal loss formulations."}}
{"id": "2509.14603", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.14603", "abs": "https://arxiv.org/abs/2509.14603", "authors": ["Xingchen Wang", "Feijie Wu", "Chenglin Miao", "Tianchun Li", "Haoyu Hu", "Qiming Cao", "Jing Gao", "Lu Su"], "title": "Towards Privacy-Preserving and Heterogeneity-aware Split Federated Learning via Probabilistic Masking", "comment": null, "summary": "Split Federated Learning (SFL) has emerged as an efficient alternative to\ntraditional Federated Learning (FL) by reducing client-side computation through\nmodel partitioning. However, exchanging of intermediate activations and model\nupdates introduces significant privacy risks, especially from data\nreconstruction attacks that recover original inputs from intermediate\nrepresentations. Existing defenses using noise injection often degrade model\nperformance. To overcome these challenges, we present PM-SFL, a scalable and\nprivacy-preserving SFL framework that incorporates Probabilistic Mask training\nto add structured randomness without relying on explicit noise. This mitigates\ndata reconstruction risks while maintaining model utility. To address data\nheterogeneity, PM-SFL employs personalized mask learning that tailors submodel\nstructures to each client's local data. For system heterogeneity, we introduce\na layer-wise knowledge compensation mechanism, enabling clients with varying\nresources to participate effectively under adaptive model splitting.\nTheoretical analysis confirms its privacy protection, and experiments on image\nand wireless sensing tasks demonstrate that PM-SFL consistently improves\naccuracy, communication efficiency, and robustness to privacy attacks, with\nparticularly strong performance under data and system heterogeneity.", "AI": {"tldr": "PM-SFL is a privacy-preserving Split Federated Learning framework that uses probabilistic mask training instead of noise injection to protect against data reconstruction attacks while maintaining model performance, with additional features for handling data and system heterogeneity.", "motivation": "Split Federated Learning reduces client computation but introduces privacy risks from intermediate data exchange. Existing noise-based defenses degrade model performance, creating a need for better privacy-preserving solutions.", "method": "Uses probabilistic mask training to add structured randomness without explicit noise, personalized mask learning for data heterogeneity, and layer-wise knowledge compensation for system heterogeneity with adaptive model splitting.", "result": "Theoretical privacy protection confirmed, with experiments showing improved accuracy, communication efficiency, and robustness to privacy attacks, especially under data and system heterogeneity conditions.", "conclusion": "PM-SFL provides an effective privacy-preserving SFL framework that maintains model utility while addressing both privacy risks and heterogeneity challenges through innovative mask training and compensation mechanisms."}}
{"id": "2509.14942", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.14942", "abs": "https://arxiv.org/abs/2509.14942", "authors": ["Minh-Khoi Pham", "Tai Tan Mai", "Martin Crane", "Rob Brennan", "Marie E. Ward", "Una Geary", "Declan Byrne", "Brian O Connell", "Colm Bergin", "Donncha Creagh", "Nick McDonald", "Marija Bezbradica"], "title": "Explainable AI for Infection Prevention and Control: Modeling CPE Acquisition and Patient Outcomes in an Irish Hospital with Transformers", "comment": "Accepted to BMC Medical Informatics and Decision Making on September\n  18th 2025", "summary": "Carbapenemase-Producing Enterobacteriace poses a critical concern for\ninfection prevention and control in hospitals. However, predictive modeling of\npreviously highlighted CPE-associated risks such as readmission, mortality, and\nextended length of stay (LOS) remains underexplored, particularly with modern\ndeep learning approaches. This study introduces an eXplainable AI modeling\nframework to investigate CPE impact on patient outcomes from Electronic Medical\nRecords data of an Irish hospital. We analyzed an inpatient dataset from an\nIrish acute hospital, incorporating diagnostic codes, ward transitions, patient\ndemographics, infection-related variables and contact network features. Several\nTransformer-based architectures were benchmarked alongside traditional machine\nlearning models. Clinical outcomes were predicted, and XAI techniques were\napplied to interpret model decisions. Our framework successfully demonstrated\nthe utility of Transformer-based models, with TabTransformer consistently\noutperforming baselines across multiple clinical prediction tasks, especially\nfor CPE acquisition (AUROC and sensitivity). We found infection-related\nfeatures, including historical hospital exposure, admission context, and\nnetwork centrality measures, to be highly influential in predicting patient\noutcomes and CPE acquisition risk. Explainability analyses revealed that\nfeatures like \"Area of Residence\", \"Admission Ward\" and prior admissions are\nkey risk factors. Network variables like \"Ward PageRank\" also ranked highly,\nreflecting the potential value of structural exposure information. This study\npresents a robust and explainable AI framework for analyzing complex EMR data\nto identify key risk factors and predict CPE-related outcomes. Our findings\nunderscore the superior performance of the Transformer models and highlight the\nimportance of diverse clinical and network features.", "AI": {"tldr": "Transformer-based AI framework predicts CPE infection risks and patient outcomes from EMR data, outperforming traditional models with explainable insights on key risk factors.", "motivation": "Carbapenemase-Producing Enterobacteriace (CPE) poses critical infection control challenges in hospitals, but predictive modeling of CPE-associated risks like readmission, mortality, and extended length of stay remains underexplored with modern deep learning approaches.", "method": "Developed an eXplainable AI framework using Transformer-based architectures benchmarked against traditional ML models. Analyzed inpatient EMR data including diagnostic codes, ward transitions, demographics, infection variables, and contact network features. Applied XAI techniques to interpret model decisions.", "result": "TabTransformer consistently outperformed baselines across multiple clinical prediction tasks, especially for CPE acquisition (AUROC and sensitivity). Infection-related features, historical hospital exposure, admission context, and network centrality measures were highly influential. Key risk factors identified included Area of Residence, Admission Ward, prior admissions, and Ward PageRank.", "conclusion": "The study presents a robust and explainable AI framework for analyzing complex EMR data to identify key risk factors and predict CPE-related outcomes, demonstrating superior performance of Transformer models and highlighting the importance of diverse clinical and network features."}}
{"id": "2509.14617", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.14617", "abs": "https://arxiv.org/abs/2509.14617", "authors": ["Jianglan Wei", "Zhenyu Zhang", "Pengcheng Wang", "Mingjie Zeng", "Zhigang Zeng"], "title": "HD3C: Efficient Medical Data Classification for Embedded Devices", "comment": null, "summary": "Energy-efficient medical data classification is essential for modern disease\nscreening, particularly in home and field healthcare where embedded devices are\nprevalent. While deep learning models achieve state-of-the-art accuracy, their\nsubstantial energy consumption and reliance on GPUs limit deployment on such\nplatforms. We present Hyperdimensional Computing with Class-Wise Clustering\n(HD3C), a lightweight classification framework designed for low-power\nenvironments. HD3C encodes data into high-dimensional hypervectors, aggregates\nthem into multiple cluster-specific prototypes, and performs classification\nthrough similarity search in hyperspace. We evaluate HD3C across three medical\nclassification tasks; on heart sound classification, HD3C is $350\\times$ more\nenergy-efficient than Bayesian ResNet with less than 1% accuracy difference.\nMoreover, HD3C demonstrates exceptional robustness to noise, limited training\ndata, and hardware error, supported by both theoretical analysis and empirical\nresults, highlighting its potential for reliable deployment in real-world\nsettings. Code is available at https://github.com/jianglanwei/HD3C.", "AI": {"tldr": "HD3C is an energy-efficient hyperdimensional computing framework for medical data classification that achieves 350x better energy efficiency than Bayesian ResNet with minimal accuracy loss, while providing robustness to noise and limited data.", "motivation": "Energy-efficient medical data classification is needed for home and field healthcare applications where embedded devices have limited power resources, but current deep learning models consume too much energy and require GPUs.", "method": "HD3C encodes data into high-dimensional hypervectors, aggregates them into multiple cluster-specific prototypes, and performs classification through similarity search in hyperspace.", "result": "HD3C achieves 350x better energy efficiency than Bayesian ResNet on heart sound classification with less than 1% accuracy difference, and demonstrates exceptional robustness to noise, limited training data, and hardware errors.", "conclusion": "HD3C provides a lightweight, energy-efficient classification framework suitable for reliable deployment in real-world low-power medical applications, with both theoretical and empirical support for its robustness."}}
{"id": "2509.14633", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.14633", "abs": "https://arxiv.org/abs/2509.14633", "authors": ["Jiaxing Miao", "Liang Hu", "Qi Zhang", "Lai Zhong Yuan", "Usman Naseem"], "title": "CUFG: Curriculum Unlearning Guided by the Forgetting Gradient", "comment": "under review (early)", "summary": "As privacy and security take center stage in AI, machine unlearning, the\nability to erase specific knowledge from models, has garnered increasing\nattention. However, existing methods overly prioritize efficiency and\naggressive forgetting, which introduces notable limitations. In particular,\nradical interventions like gradient ascent, influence functions, and random\nlabel noise can destabilize model weights, leading to collapse and reduced\nreliability. To address this, we propose CUFG (Curriculum Unlearning via\nForgetting Gradients), a novel framework that enhances the stability of\napproximate unlearning through innovations in both forgetting mechanisms and\ndata scheduling strategies. Specifically, CUFG integrates a new gradient\ncorrector guided by forgetting gradients for fine-tuning-based unlearning and a\ncurriculum unlearning paradigm that progressively forgets from easy to hard.\nThese innovations narrow the gap with the gold-standard Retrain method by\nenabling more stable and progressive unlearning, thereby improving both\neffectiveness and reliability. Furthermore, we believe that the concept of\ncurriculum unlearning has substantial research potential and offers\nforward-looking insights for the development of the MU field. Extensive\nexperiments across various forgetting scenarios validate the rationale and\neffectiveness of our approach and CUFG. Codes are available at\nhttps://anonymous.4open.science/r/CUFG-6375.", "AI": {"tldr": "CUFG is a novel curriculum-based unlearning framework that uses forgetting gradients and progressive data scheduling to enable more stable and reliable machine unlearning compared to aggressive existing methods.", "motivation": "Existing machine unlearning methods prioritize efficiency and aggressive forgetting too much, causing model instability, weight collapse, and reduced reliability through radical interventions like gradient ascent and random label noise.", "method": "Proposes CUFG framework with two innovations: 1) gradient corrector guided by forgetting gradients for fine-tuning-based unlearning, and 2) curriculum unlearning paradigm that progressively forgets from easy to hard data.", "result": "Extensive experiments across various forgetting scenarios validate the approach, showing CUFG narrows the gap with gold-standard Retrain method and improves both effectiveness and reliability.", "conclusion": "CUFG provides more stable and progressive unlearning, and the curriculum unlearning concept offers substantial research potential and forward-looking insights for machine unlearning field development."}}
{"id": "2509.14963", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.14963", "abs": "https://arxiv.org/abs/2509.14963", "authors": ["Filip Naudot", "Andreas Br\u00e4nnstr\u00f6m", "Vicen\u00e7 Torra", "Timotheus Kampik"], "title": "Set Contribution Functions for Quantitative Bipolar Argumentation and their Principles", "comment": null, "summary": "We present functions that quantify the contribution of a set of arguments in\nquantitative bipolar argumentation graphs to (the final strength of) an\nargument of interest, a so-called topic. Our set contribution functions are\ngeneralizations of existing functions that quantify the contribution of a\nsingle contributing argument to a topic. Accordingly, we generalize existing\ncontribution function principles for set contribution functions and provide a\ncorresponding principle-based analysis. We introduce new principles specific to\nset-based functions that focus on properties pertaining to the interaction of\narguments within a set. Finally, we sketch how the principles play out across\ndifferent set contribution functions given a recommendation system application\nscenario.", "AI": {"tldr": "Generalization of single-argument contribution functions to set-based functions in quantitative bipolar argumentation graphs, with new principles for set interactions and application to recommendation systems.", "motivation": "To extend existing quantitative bipolar argumentation frameworks by developing functions that measure the collective contribution of argument sets (rather than individual arguments) to determine the final strength of a topic argument.", "method": "Generalize existing single-argument contribution functions to set-based functions, establish new principles specific to set interactions, and perform principle-based analysis across different set contribution functions.", "result": "Developed generalized set contribution functions that quantify how argument sets collectively influence topic strength, with new principles addressing set-specific properties and interactions between arguments within sets.", "conclusion": "The proposed set contribution functions provide a more comprehensive framework for analyzing collective argument influence in bipolar argumentation, with practical applications demonstrated in recommendation systems."}}
{"id": "2509.14640", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.14640", "abs": "https://arxiv.org/abs/2509.14640", "authors": ["Habib Irani", "Vangelis Metsis"], "title": "DyWPE: Signal-Aware Dynamic Wavelet Positional Encoding for Time Series Transformers", "comment": null, "summary": "Existing positional encoding methods in transformers are fundamentally\nsignal-agnostic, deriving positional information solely from sequence indices\nwhile ignoring the underlying signal characteristics. This limitation is\nparticularly problematic for time series analysis, where signals exhibit\ncomplex, non-stationary dynamics across multiple temporal scales. We introduce\nDynamic Wavelet Positional Encoding (DyWPE), a novel signal-aware framework\nthat generates positional embeddings directly from input time series using the\nDiscrete Wavelet Transform (DWT). Comprehensive experiments in ten diverse time\nseries datasets demonstrate that DyWPE consistently outperforms eight existing\nstate-of-the-art positional encoding methods, achieving average relative\nimprovements of 9.1\\% compared to baseline sinusoidal absolute position\nencoding in biomedical signals, while maintaining competitive computational\nefficiency.", "AI": {"tldr": "DyWPE is a signal-aware positional encoding method that uses Discrete Wavelet Transform to generate embeddings directly from time series data, outperforming traditional signal-agnostic methods.", "motivation": "Existing positional encoding methods ignore signal characteristics and are problematic for time series analysis with complex, non-stationary dynamics across multiple temporal scales.", "method": "Dynamic Wavelet Positional Encoding (DyWPE) framework that generates positional embeddings directly from input time series using Discrete Wavelet Transform (DWT).", "result": "Outperformed 8 state-of-the-art positional encoding methods across 10 diverse time series datasets, achieving 9.1% average improvement over baseline sinusoidal encoding in biomedical signals while maintaining computational efficiency.", "conclusion": "Signal-aware positional encoding using wavelet transforms significantly improves performance in time series analysis compared to traditional signal-agnostic approaches."}}
{"id": "2509.14998", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.14998", "abs": "https://arxiv.org/abs/2509.14998", "authors": ["Xiao Wu", "Ting-Zhu Huang", "Liang-Jian Deng", "Yanyuan Qiao", "Imran Razzak", "Yutong Xie"], "title": "A Knowledge-driven Adaptive Collaboration of LLMs for Enhancing Medical Decision-making", "comment": "The paper has been accepted to the EMNLP 2025 Main Conference", "summary": "Medical decision-making often involves integrating knowledge from multiple\nclinical specialties, typically achieved through multidisciplinary teams.\nInspired by this collaborative process, recent work has leveraged large\nlanguage models (LLMs) in multi-agent collaboration frameworks to emulate\nexpert teamwork. While these approaches improve reasoning through agent\ninteraction, they are limited by static, pre-assigned roles, which hinder\nadaptability and dynamic knowledge integration. To address these limitations,\nwe propose KAMAC, a Knowledge-driven Adaptive Multi-Agent Collaboration\nframework that enables LLM agents to dynamically form and expand expert teams\nbased on the evolving diagnostic context. KAMAC begins with one or more expert\nagents and then conducts a knowledge-driven discussion to identify and fill\nknowledge gaps by recruiting additional specialists as needed. This supports\nflexible, scalable collaboration in complex clinical scenarios, with decisions\nfinalized through reviewing updated agent comments. Experiments on two\nreal-world medical benchmarks demonstrate that KAMAC significantly outperforms\nboth single-agent and advanced multi-agent methods, particularly in complex\nclinical scenarios (i.e., cancer prognosis) requiring dynamic, cross-specialty\nexpertise. Our code is publicly available at:\nhttps://github.com/XiaoXiao-Woo/KAMAC.", "AI": {"tldr": "KAMAC is a dynamic multi-agent framework that enables LLM agents to form and expand expert teams based on diagnostic context, outperforming existing methods in complex medical scenarios.", "motivation": "Current multi-agent collaboration frameworks use static, pre-assigned roles that limit adaptability and dynamic knowledge integration needed for complex medical decision-making.", "method": "KAMAC starts with expert agents and conducts knowledge-driven discussions to identify gaps, then recruits additional specialists as needed, with decisions finalized through reviewing updated agent comments.", "result": "Experiments on real-world medical benchmarks show KAMAC significantly outperforms both single-agent and advanced multi-agent methods, especially in complex clinical scenarios like cancer prognosis.", "conclusion": "The framework enables flexible, scalable collaboration for complex clinical scenarios requiring dynamic, cross-specialty expertise integration."}}
{"id": "2509.14642", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.14642", "abs": "https://arxiv.org/abs/2509.14642", "authors": ["Yuemin Wu", "Zhongze Wu", "Xiu Su", "Feng Yang", "Hongyan Xu", "Xi Lin", "Wenti Huang", "Shan You", "Chang Xu"], "title": "DeCoP: Enhancing Self-Supervised Time Series Representation with Dependency Controlled Pre-training", "comment": null, "summary": "Modeling dynamic temporal dependencies is a critical challenge in time series\npre-training, which evolve due to distribution shifts and multi-scale patterns.\nThis temporal variability severely impairs the generalization of pre-trained\nmodels to downstream tasks. Existing frameworks fail to capture the complex\ninteractions of short- and long-term dependencies, making them susceptible to\nspurious correlations that degrade generalization. To address these\nlimitations, we propose DeCoP, a Dependency Controlled Pre-training framework\nthat explicitly models dynamic, multi-scale dependencies by simulating evolving\ninter-patch dependencies. At the input level, DeCoP introduces Instance-wise\nPatch Normalization (IPN) to mitigate distributional shifts while preserving\nthe unique characteristics of each patch, creating a robust foundation for\nrepresentation learning. At the latent level, a hierarchical Dependency\nControlled Learning (DCL) strategy explicitly models inter-patch dependencies\nacross multiple temporal scales, with an Instance-level Contrastive Module\n(ICM) enhances global generalization by learning instance-discriminative\nrepresentations from time-invariant positive pairs. DeCoP achieves\nstate-of-the-art results on ten datasets with lower computing resources,\nimproving MSE by 3% on ETTh1 over PatchTST using only 37% of the FLOPs.", "AI": {"tldr": "DeCoP is a novel time series pre-training framework that addresses dynamic temporal dependencies and distribution shifts through instance-wise patch normalization and hierarchical dependency controlled learning, achieving state-of-the-art results with improved efficiency.", "motivation": "Existing time series pre-training models struggle with dynamic temporal dependencies caused by distribution shifts and multi-scale patterns, leading to poor generalization and susceptibility to spurious correlations in downstream tasks.", "method": "DeCoP uses Instance-wise Patch Normalization (IPN) to handle distributional shifts while preserving patch characteristics, and a hierarchical Dependency Controlled Learning (DCL) strategy with Instance-level Contrastive Module (ICM) to model multi-scale inter-patch dependencies and learn instance-discriminative representations.", "result": "DeCoP achieves state-of-the-art performance on ten datasets with 3% MSE improvement on ETTh1 over PatchTST while using only 37% of the FLOPs, demonstrating superior efficiency and effectiveness.", "conclusion": "The proposed framework successfully addresses temporal variability challenges in time series pre-training through explicit modeling of dynamic multi-scale dependencies, offering improved generalization and computational efficiency."}}
{"id": "2509.15035", "categories": ["cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2509.15035", "abs": "https://arxiv.org/abs/2509.15035", "authors": ["Gabriela C. Zapata", "Bill Cope", "Mary Kalantzis", "Duane Searsmith"], "title": "Calibrated Generative AI as Meta-Reviewer: A Systemic Functional Linguistics Discourse Analysis of Reviews of Peer Reviews", "comment": "39 pages, 3 tables", "summary": "This study investigates the use of generative AI to support formative\nassessment through machine generated reviews of peer reviews in graduate online\ncourses in a public university in the United States. Drawing on Systemic\nFunctional Linguistics and Appraisal Theory, we analyzed 120 metareviews to\nexplore how generative AI feedback constructs meaning across ideational,\ninterpersonal, and textual dimensions. The findings suggest that generative AI\ncan approximate key rhetorical and relational features of effective human\nfeedback, offering directive clarity while also maintaining a supportive\nstance. The reviews analyzed demonstrated a balance of praise and constructive\ncritique, alignment with rubric expectations, and structured staging that\nforegrounded student agency. By modeling these qualities, AI metafeedback has\nthe potential to scaffold feedback literacy and enhance leaner engagement with\npeer review.", "AI": {"tldr": "Study shows generative AI can provide effective formative assessment feedback in online graduate courses by approximating human feedback qualities like directive clarity, supportive stance, and balanced critique.", "motivation": "To investigate how generative AI can support formative assessment through machine-generated reviews of peer reviews in online graduate education.", "method": "Analyzed 120 metareviews using Systemic Functional Linguistics and Appraisal Theory to examine how AI feedback constructs meaning across ideational, interpersonal, and textual dimensions.", "result": "Generative AI feedback demonstrated key features of effective human feedback - balanced praise and constructive critique, alignment with rubric expectations, structured staging that foregrounded student agency, and maintained supportive stance with directive clarity.", "conclusion": "AI metafeedback has potential to scaffold feedback literacy and enhance learner engagement with peer review by modeling effective feedback qualities."}}
{"id": "2509.14678", "categories": ["cs.LG", "physics.data-an"], "pdf": "https://arxiv.org/pdf/2509.14678", "abs": "https://arxiv.org/abs/2509.14678", "authors": ["Hyungjoon Soh", "Junghyo Jo"], "title": "Stochastic Clock Attention for Aligning Continuous and Ordered Sequences", "comment": "8 pages, 3 figures", "summary": "We formulate an attention mechanism for continuous and ordered sequences that\nexplicitly functions as an alignment model, which serves as the core of many\nsequence-to-sequence tasks. Standard scaled dot-product attention relies on\npositional encodings and masks but does not enforce continuity or monotonicity,\nwhich are crucial for frame-synchronous targets. We propose learned nonnegative\n\\emph{clocks} to source and target and model attention as the meeting\nprobability of these clocks; a path-integral derivation yields a closed-form,\nGaussian-like scoring rule with an intrinsic bias toward causal, smooth,\nnear-diagonal alignments, without external positional regularizers. The\nframework supports two complementary regimes: normalized clocks for parallel\ndecoding when a global length is available, and unnormalized clocks for\nautoregressive decoding -- both nearly-parameter-free, drop-in replacements. In\na Transformer text-to-speech testbed, this construction produces more stable\nalignments and improved robustness to global time-scaling while matching or\nimproving accuracy over scaled dot-product baselines. We hypothesize\napplicability to other continuous targets, including video and temporal signal\nmodeling.", "AI": {"tldr": "A novel attention mechanism using learned nonnegative clocks for continuous sequences that enforces alignment continuity and monotonicity without external positional regularizers, improving stability and robustness in sequence-to-sequence tasks.", "motivation": "Standard scaled dot-product attention lacks enforcement of continuity and monotonicity, which are crucial for frame-synchronous targets in sequence-to-sequence tasks.", "method": "Proposed learned nonnegative clocks for source and target sequences, modeling attention as meeting probability of these clocks. Uses path-integral derivation to create Gaussian-like scoring with intrinsic bias toward causal, smooth, near-diagonal alignments.", "result": "In Transformer text-to-speech testbed, produces more stable alignments and improved robustness to global time-scaling while matching or improving accuracy over scaled dot-product baselines.", "conclusion": "The clock-based attention framework provides effective drop-in replacements for both parallel and autoregressive decoding, with potential applicability to other continuous targets like video and temporal signal modeling."}}
{"id": "2509.15084", "categories": ["cs.AI", "cs.CY", "cs.HC"], "pdf": "https://arxiv.org/pdf/2509.15084", "abs": "https://arxiv.org/abs/2509.15084", "authors": ["Doreen Jirak", "Pieter Maes", "Armeen Saroukanoff", "Dirk van Rooy"], "title": "From Sea to System: Exploring User-Centered Explainable AI for Maritime Decision Support", "comment": "Paper accepted at Human Learning and Decision-Making Workshop\n  @ECML-PKDD Conference 2025, Porto, Portugal", "summary": "As autonomous technologies increasingly shape maritime operations,\nunderstanding why an AI system makes a decision becomes as crucial as what it\ndecides. In complex and dynamic maritime environments, trust in AI depends not\nonly on performance but also on transparency and interpretability. This paper\nhighlights the importance of Explainable AI (XAI) as a foundation for effective\nhuman-machine teaming in the maritime domain, where informed oversight and\nshared understanding are essential. To support the user-centered integration of\nXAI, we propose a domain-specific survey designed to capture maritime\nprofessionals' perceptions of trust, usability, and explainability. Our aim is\nto foster awareness and guide the development of user-centric XAI systems\ntailored to the needs of seafarers and maritime teams.", "AI": {"tldr": "Survey on maritime professionals' perceptions of XAI for trust and usability in autonomous maritime operations.", "motivation": "As AI systems become integral to maritime operations, trust depends on transparency and interpretability, not just performance. Effective human-machine teaming requires explainable AI to enable informed oversight and shared understanding in complex maritime environments.", "method": "Propose a domain-specific survey designed to capture maritime professionals' perceptions of trust, usability, and explainability of AI systems.", "result": "The paper presents a survey framework to assess XAI needs in maritime domain, but does not report specific survey results or findings.", "conclusion": "The survey aims to foster awareness and guide development of user-centric XAI systems tailored to seafarers' needs, supporting effective integration of explainable AI in maritime operations."}}
{"id": "2509.14718", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.14718", "abs": "https://arxiv.org/abs/2509.14718", "authors": ["Zihao Feng", "Xiaoxue Wang", "Bowen Wu", "Hailong Cao", "Tiejun Zhao", "Qun Yu", "Baoxun Wang"], "title": "ToolSample: Dual Dynamic Sampling Methods with Curriculum Learning for RL-based Tool Learning", "comment": null, "summary": "While reinforcement learning (RL) is increasingly used for LLM-based tool\nlearning, its efficiency is often hampered by an overabundance of simple\nsamples that provide diminishing learning value as training progresses.\nExisting dynamic sampling techniques are ill-suited for the multi-task\nstructure and fine-grained reward mechanisms inherent to tool learning. This\npaper introduces Dynamic Sampling with Curriculum Learning (DSCL), a framework\nspecifically designed to address this challenge by targeting the unique\ncharacteristics of tool learning: its multiple interdependent sub-tasks and\nmulti-valued reward functions. DSCL features two core components: Reward-Based\nDynamic Sampling, which uses multi-dimensional reward statistics (mean and\nvariance) to prioritize valuable data, and Task-Based Dynamic Curriculum\nLearning, which adaptively focuses training on less-mastered sub-tasks. Through\nextensive experiments, we demonstrate that DSCL significantly improves training\nefficiency and model performance over strong baselines, achieving a 3.29\\%\nimprovement on the BFCLv3 benchmark. Our method provides a tailored solution\nthat effectively leverages the complex reward signals and sub-task dynamics\nwithin tool learning to achieve superior results.", "AI": {"tldr": "DSCL framework improves RL efficiency for LLM tool learning by dynamically sampling valuable data and adapting curriculum to focus on challenging sub-tasks, achieving 3.29% improvement on BFCLv3 benchmark.", "motivation": "Existing dynamic sampling techniques are inadequate for tool learning's multi-task structure and fine-grained reward mechanisms, leading to inefficient training with diminishing returns from simple samples.", "method": "Two core components: 1) Reward-Based Dynamic Sampling using multi-dimensional reward statistics (mean/variance) to prioritize valuable data, and 2) Task-Based Dynamic Curriculum Learning that adaptively focuses training on less-mastered sub-tasks.", "result": "Significantly improves training efficiency and model performance over strong baselines, achieving 3.29% improvement on the BFCLv3 benchmark.", "conclusion": "DSCL provides a tailored solution that effectively leverages complex reward signals and sub-task dynamics within tool learning to achieve superior results."}}
{"id": "2509.15172", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2509.15172", "abs": "https://arxiv.org/abs/2509.15172", "authors": ["Ankur Samanta", "Akshayaa Magesh", "Youliang Yu", "Runzhe Wu", "Ayush Jain", "Daniel Jiang", "Boris Vidolov", "Paul Sajda", "Yonathan Efroni", "Kaveh Hassani"], "title": "Internalizing Self-Consistency in Language Models: Multi-Agent Consensus Alignment", "comment": null, "summary": "Language Models (LMs) are inconsistent reasoners, often generating\ncontradictory responses to identical prompts. While inference-time methods can\nmitigate these inconsistencies, they fail to address the core problem: LMs\nstruggle to reliably select reasoning pathways leading to consistent outcomes\nunder exploratory sampling. To address this, we formalize self-consistency as\nan intrinsic property of well-aligned reasoning models and introduce\nMulti-Agent Consensus Alignment (MACA), a reinforcement learning framework that\npost-trains models to favor reasoning trajectories aligned with their internal\nconsensus using majority/minority outcomes from multi-agent debate. These\ntrajectories emerge from deliberative exchanges where agents ground reasoning\nin peer arguments, not just aggregation of independent attempts, creating\nricher consensus signals than single-round majority voting. MACA enables agents\nto teach themselves to be more decisive and concise, and better leverage peer\ninsights in multi-agent settings without external supervision, driving\nsubstantial improvements across self-consistency (+27.6% on GSM8K),\nsingle-agent reasoning (+23.7% on MATH), sampling-based inference (+22.4%\nPass@20 on MATH), and multi-agent ensemble decision-making (+42.7% on MathQA).\nThese findings, coupled with strong generalization to unseen benchmarks (+16.3%\non GPQA, +11.6% on CommonsenseQA), demonstrate robust self-alignment that more\nreliably unlocks latent reasoning potential of language models.", "AI": {"tldr": "MACA is a reinforcement learning framework that uses multi-agent debate to train language models to be more self-consistent reasoners by aligning with internal consensus from deliberative exchanges.", "motivation": "Language models are inconsistent reasoners that generate contradictory responses to identical prompts, and current inference-time methods don't address the core problem of unreliable reasoning pathway selection.", "method": "Multi-Agent Consensus Alignment (MACA) - a reinforcement learning framework that post-trains models to favor reasoning trajectories aligned with internal consensus using majority/minority outcomes from multi-agent debate with deliberative exchanges.", "result": "Substantial improvements across multiple benchmarks: +27.6% on GSM8K, +23.7% on MATH, +22.4% Pass@20 on MATH, +42.7% on MathQA, with strong generalization to unseen benchmarks (+16.3% on GPQA, +11.6% on CommonsenseQA).", "conclusion": "MACA enables robust self-alignment that more reliably unlocks the latent reasoning potential of language models through multi-agent consensus learning without external supervision."}}
{"id": "2509.14722", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.14722", "abs": "https://arxiv.org/abs/2509.14722", "authors": ["Yeyu Yan", "Shuai Zheng", "Wenjun Hui", "Xiangkai Zhu", "Dong Chen", "Zhenfeng Zhu", "Yao Zhao", "Kunlun He"], "title": "Towards Pre-trained Graph Condensation via Optimal Transport", "comment": null, "summary": "Graph condensation (GC) aims to distill the original graph into a small-scale\ngraph, mitigating redundancy and accelerating GNN training. However,\nconventional GC approaches heavily rely on rigid GNNs and task-specific\nsupervision. Such a dependency severely restricts their reusability and\ngeneralization across various tasks and architectures. In this work, we revisit\nthe goal of ideal GC from the perspective of GNN optimization consistency, and\nthen a generalized GC optimization objective is derived, by which those\ntraditional GC methods can be viewed nicely as special cases of this\noptimization paradigm. Based on this, Pre-trained Graph Condensation (PreGC)\nvia optimal transport is proposed to transcend the limitations of task- and\narchitecture-dependent GC methods. Specifically, a hybrid-interval graph\ndiffusion augmentation is presented to suppress the weak generalization ability\nof the condensed graph on particular architectures by enhancing the uncertainty\nof node states. Meanwhile, the matching between optimal graph transport plan\nand representation transport plan is tactfully established to maintain semantic\nconsistencies across source graph and condensed graph spaces, thereby freeing\ngraph condensation from task dependencies. To further facilitate the adaptation\nof condensed graphs to various downstream tasks, a traceable semantic\nharmonizer from source nodes to condensed nodes is proposed to bridge semantic\nassociations through the optimized representation transport plan in\npre-training. Extensive experiments verify the superiority and versatility of\nPreGC, demonstrating its task-independent nature and seamless compatibility\nwith arbitrary GNNs.", "AI": {"tldr": "PreGC is a novel graph condensation method that uses optimal transport to create task-agnostic condensed graphs that work with any GNN architecture, overcoming limitations of traditional task-specific approaches.", "motivation": "Traditional graph condensation methods are heavily dependent on specific GNN architectures and task-specific supervision, which limits their reusability and generalization across different tasks and models.", "method": "Proposes Pre-trained Graph Condensation (PreGC) using optimal transport with hybrid-interval graph diffusion augmentation to enhance generalization, and establishes matching between optimal graph transport plan and representation transport plan to maintain semantic consistency.", "result": "Extensive experiments show PreGC achieves superior performance and versatility, demonstrating task-independent nature and seamless compatibility with arbitrary GNN architectures.", "conclusion": "PreGC successfully transcends the limitations of traditional task- and architecture-dependent graph condensation methods, providing a generalized solution that maintains semantic consistency while being adaptable to various downstream tasks."}}
{"id": "2509.15217", "categories": ["cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.15217", "abs": "https://arxiv.org/abs/2509.15217", "authors": ["Yue Xin", "Wenyuan Wang", "Rui Pan", "Ruida Wang", "Howard Meng", "Renjie Pi", "Shizhe Diao", "Tong Zhang"], "title": "Generalizable Geometric Image Caption Synthesis", "comment": null, "summary": "Multimodal large language models have various practical applications that\ndemand strong reasoning abilities. Despite recent advancements, these models\nstill struggle to solve complex geometric problems. A key challenge stems from\nthe lack of high-quality image-text pair datasets for understanding geometric\nimages. Furthermore, most template-based data synthesis pipelines typically\nfail to generalize to questions beyond their predefined templates. In this\npaper, we bridge this gap by introducing a complementary process of\nReinforcement Learning with Verifiable Rewards (RLVR) into the data generation\npipeline. By adopting RLVR to refine captions for geometric images synthesized\nfrom 50 basic geometric relations and using reward signals derived from\nmathematical problem-solving tasks, our pipeline successfully captures the key\nfeatures of geometry problem-solving. This enables better task generalization\nand yields non-trivial improvements. Furthermore, even in out-of-distribution\nscenarios, the generated dataset enhances the general reasoning capabilities of\nmultimodal large language models, yielding accuracy improvements of\n$2.8\\%\\text{-}4.8\\%$ in statistics, arithmetic, algebraic, and numerical tasks\nwith non-geometric input images of MathVista and MathVerse, along with\n$2.4\\%\\text{-}3.9\\%$ improvements in Art, Design, Tech, and Engineering tasks\nin MMMU.", "AI": {"tldr": "RLVR method enhances multimodal LLMs' geometric reasoning by generating high-quality training data with verifiable rewards, improving performance on both geometric and non-geometric tasks.", "motivation": "Multimodal LLMs struggle with complex geometric problems due to lack of high-quality image-text datasets and limited generalization of template-based data synthesis methods.", "method": "Introduces Reinforcement Learning with Verifiable Rewards (RLVR) to refine captions for geometric images synthesized from 50 basic relations, using reward signals from math problem-solving tasks.", "result": "Achieves 2.8%-4.8% accuracy improvements on MathVista/MathVerse non-geometric tasks and 2.4%-3.9% improvements on MMMU Art/Design/Tech/Engineering tasks, demonstrating better generalization.", "conclusion": "RLVR pipeline effectively captures geometry problem-solving features, enhances multimodal LLM reasoning capabilities, and improves performance across diverse tasks including out-of-distribution scenarios."}}
{"id": "2509.14723", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.14723", "abs": "https://arxiv.org/abs/2509.14723", "authors": ["Sosuke Hosokawa", "Toshiharu Kawakami", "Satoshi Kodera", "Masamichi Ito", "Norihiko Takeda"], "title": "Transcoder-based Circuit Analysis for Interpretable Single-Cell Foundation Models", "comment": null, "summary": "Single-cell foundation models (scFMs) have demonstrated state-of-the-art\nperformance on various tasks, such as cell-type annotation and perturbation\nresponse prediction, by learning gene regulatory networks from large-scale\ntranscriptome data. However, a significant challenge remains: the\ndecision-making processes of these models are less interpretable compared to\ntraditional methods like differential gene expression analysis. Recently,\ntranscoders have emerged as a promising approach for extracting interpretable\ndecision circuits from large language models (LLMs). In this work, we train a\ntranscoder on the cell2sentence (C2S) model, a state-of-the-art scFM. By\nleveraging the trained transcoder, we extract internal decision-making circuits\nfrom the C2S model. We demonstrate that the discovered circuits correspond to\nreal-world biological mechanisms, confirming the potential of transcoders to\nuncover biologically plausible pathways within complex single-cell models.", "AI": {"tldr": "Training a transcoder on cell2sentence model to extract interpretable decision circuits from single-cell foundation models, revealing biologically meaningful pathways.", "motivation": "Single-cell foundation models lack interpretability compared to traditional methods, making their decision processes opaque despite superior performance.", "method": "Train a transcoder on the cell2sentence (C2S) model to extract internal decision-making circuits from this state-of-the-art single-cell foundation model.", "result": "The extracted circuits correspond to real-world biological mechanisms, demonstrating transcoders can uncover biologically plausible pathways within complex models.", "conclusion": "Transcoders show promising potential for making single-cell foundation models more interpretable by revealing their internal biological decision pathways."}}
{"id": "2509.14724", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.14724", "abs": "https://arxiv.org/abs/2509.14724", "authors": ["Zhiyuan Xue", "Ben Yang", "Xuetao Zhang", "Fei Wang", "Zhiping Lin"], "title": "One-step Multi-view Clustering With Adaptive Low-rank Anchor-graph Learning", "comment": "13 pages, 7 figures, journal article. Accepted by IEEE Transactions\n  on Multimedia, not yet published online", "summary": "In light of their capability to capture structural information while reducing\ncomputing complexity, anchor graph-based multi-view clustering (AGMC) methods\nhave attracted considerable attention in large-scale clustering problems.\nNevertheless, existing AGMC methods still face the following two issues: 1)\nThey directly embedded diverse anchor graphs into a consensus anchor graph\n(CAG), and hence ignore redundant information and numerous noises contained in\nthese anchor graphs, leading to a decrease in clustering effectiveness; 2) They\ndrop effectiveness and efficiency due to independent post-processing to acquire\nclustering indicators. To overcome the aforementioned issues, we deliver a\nnovel one-step multi-view clustering method with adaptive low-rank anchor-graph\nlearning (OMCAL). To construct a high-quality CAG, OMCAL provides a nuclear\nnorm-based adaptive CAG learning model against information redundancy and noise\ninterference. Then, to boost clustering effectiveness and efficiency\nsubstantially, we incorporate category indicator acquisition and CAG learning\ninto a unified framework. Numerous studies conducted on ordinary and\nlarge-scale datasets indicate that OMCAL outperforms existing state-of-the-art\nmethods in terms of clustering effectiveness and efficiency.", "AI": {"tldr": "Proposes OMCAL, a one-step multi-view clustering method with adaptive low-rank anchor-graph learning to address redundancy and noise issues in existing anchor graph-based methods while improving efficiency.", "motivation": "Existing anchor graph-based multi-view clustering methods suffer from redundant information and noise in consensus anchor graphs, and require independent post-processing that reduces both effectiveness and efficiency.", "method": "Develops a nuclear norm-based adaptive consensus anchor graph learning model to handle information redundancy and noise, and integrates category indicator acquisition with anchor graph learning into a unified one-step framework.", "result": "Extensive experiments on ordinary and large-scale datasets show that OMCAL outperforms state-of-the-art methods in both clustering effectiveness and efficiency.", "conclusion": "OMCAL successfully addresses the limitations of existing AGMC methods by providing a unified framework that handles redundancy and noise while improving clustering performance and computational efficiency."}}
{"id": "2509.14775", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.14775", "abs": "https://arxiv.org/abs/2509.14775", "authors": ["Shuangshuang He", "Yuanting Zhang", "Hongli Liang", "Qingye Meng", "Xingyuan Yuan"], "title": "FlowCast-ODE: Continuous Hourly Weather Forecasting with Dynamic Flow Matching and ODE Integration", "comment": null, "summary": "Accurate hourly weather forecasting is critical for numerous applications.\nRecent deep learning models have demonstrated strong capability on 6-hour\nintervals, yet achieving accurate and stable hourly predictions remains a\ncritical challenge. This is primarily due to the rapid accumulation of errors\nin autoregressive rollouts and temporal discontinuities within the ERA5 data's\n12-hour assimilation cycle. To address these issues, we propose FlowCast-ODE, a\nframework that models atmospheric state evolution as a continuous flow.\nFlowCast-ODE learns the conditional flow path directly from the previous state,\nan approach that aligns more naturally with physical dynamic systems and\nenables efficient computation. A coarse-to-fine strategy is introduced to train\nthe model on 6-hour data using dynamic flow matching and then refined on hourly\ndata that incorporates an Ordinary Differential Equation (ODE) solver to\nachieve temporally coherent forecasts. In addition, a lightweight low-rank\nAdaLN-Zero modulation mechanism is proposed and reduces model size by 15%\nwithout compromising accuracy. Experiments demonstrate that FlowCast-ODE\noutperforms strong baselines, yielding lower root mean square error (RMSE) and\nbetter energy conservation, which reduces blurring and preserves more\nfine-scale spatial details. It also shows comparable performance to the\nstate-of-the-art model in forecasting extreme events like typhoons.\nFurthermore, the model alleviates temporal discontinuities associated with\nassimilation cycle transitions.", "AI": {"tldr": "FlowCast-ODE is a continuous flow modeling framework for accurate hourly weather forecasting that addresses error accumulation and temporal discontinuities in ERA5 data through dynamic flow matching and ODE solvers.", "motivation": "Accurate hourly weather forecasting is critical but challenging due to error accumulation in autoregressive rollouts and temporal discontinuities in ERA5's 12-hour assimilation cycle.", "method": "Models atmospheric state evolution as continuous flow using conditional flow paths, employs coarse-to-fine training with dynamic flow matching on 6-hour data, refines with ODE solver on hourly data, and uses lightweight low-rank AdaLN-Zero modulation to reduce model size.", "result": "Outperforms baselines with lower RMSE, better energy conservation, reduced blurring, preserved fine-scale spatial details, comparable performance on extreme events like typhoons, and alleviates temporal discontinuities.", "conclusion": "FlowCast-ODE provides an effective framework for stable and accurate hourly weather forecasting by modeling atmospheric dynamics as continuous flows with improved computational efficiency and reduced model size."}}
{"id": "2509.14786", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.14786", "abs": "https://arxiv.org/abs/2509.14786", "authors": ["Konwoo Kim", "Suhas Kotha", "Percy Liang", "Tatsunori Hashimoto"], "title": "Pre-training under infinite compute", "comment": null, "summary": "Since compute grows much faster than web text available for language model\npre-training, we ask how one should approach pre-training under fixed data and\nno compute constraints. We first show that existing data-constrained approaches\nof increasing epoch count and parameter count eventually overfit, and we\nsignificantly improve upon such recipes by properly tuning regularization,\nfinding that the optimal weight decay is $30\\times$ larger than standard\npractice. Since our regularized recipe monotonically decreases loss following a\nsimple power law in parameter count, we estimate its best possible performance\nvia the asymptote of its scaling law rather than the performance at a fixed\ncompute budget. We then identify that ensembling independently trained models\nachieves a significantly lower loss asymptote than the regularized recipe. Our\nbest intervention combining epoching, regularization, parameter scaling, and\nensemble scaling achieves an asymptote at 200M tokens using $5.17\\times$ less\ndata than our baseline, and our data scaling laws predict that this improvement\npersists at higher token budgets. We find that our data efficiency gains can be\nrealized at much smaller parameter counts as we can distill an ensemble into a\nstudent model that is 8$\\times$ smaller and retains $83\\%$ of the ensembling\nbenefit. Finally, our interventions designed for validation loss generalize to\ndownstream benchmarks, achieving a $9\\%$ improvement for pre-training evals and\na $17.5\\times$ data efficiency improvement over continued pre-training on math\nmid-training data. Our results show that simple algorithmic improvements can\nenable significantly more data-efficient pre-training in a compute-rich future.", "AI": {"tldr": "This paper shows that with proper regularization and ensembling techniques, language models can achieve significantly better data efficiency during pre-training, achieving up to 17.5x data efficiency improvements over baseline approaches.", "motivation": "As compute resources grow faster than available web text for language model pre-training, the research addresses how to optimize pre-training under fixed data constraints with unlimited compute resources.", "method": "The authors developed a regularized training recipe with much larger weight decay (30x standard), parameter scaling, ensemble scaling, and knowledge distillation techniques to improve data efficiency while preventing overfitting.", "result": "The approach achieved 5.17x less data usage at 200M tokens, 83% retention of ensemble benefits in distilled models 8x smaller, and 17.5x data efficiency improvement on math tasks compared to continued pre-training.", "conclusion": "Simple algorithmic improvements like proper regularization and ensembling can enable significantly more data-efficient pre-training, making better use of limited data resources in compute-rich environments."}}
{"id": "2509.14788", "categories": ["cs.LG", "cs.AI", "q-bio.BM"], "pdf": "https://arxiv.org/pdf/2509.14788", "abs": "https://arxiv.org/abs/2509.14788", "authors": ["Jing Lan", "Hexiao Ding", "Hongzhao Chen", "Yufeng Jiang", "Nga-Chun Ng", "Gwing Kei Yip", "Gerald W. Y. Cheng", "Yunlin Mao", "Jing Cai", "Liang-ting Lin", "Jung Sun Yoo"], "title": "Structure-Aware Contrastive Learning with Fine-Grained Binding Representations for Drug Discovery", "comment": null, "summary": "Accurate identification of drug-target interactions (DTI) remains a central\nchallenge in computational pharmacology, where sequence-based methods offer\nscalability. This work introduces a sequence-based drug-target interaction\nframework that integrates structural priors into protein representations while\nmaintaining high-throughput screening capability. Evaluated across multiple\nbenchmarks, the model achieves state-of-the-art performance on Human and\nBioSNAP datasets and remains competitive on BindingDB. In virtual screening\ntasks, it surpasses prior methods on LIT-PCBA, yielding substantial gains in\nAUROC and BEDROC. Ablation studies confirm the critical role of learned\naggregation, bilinear attention, and contrastive alignment in enhancing\npredictive robustness. Embedding visualizations reveal improved spatial\ncorrespondence with known binding pockets and highlight interpretable attention\npatterns over ligand-residue contacts. These results validate the framework's\nutility for scalable and structure-aware DTI prediction.", "AI": {"tldr": "A sequence-based drug-target interaction framework that integrates structural priors while maintaining high-throughput screening capability, achieving state-of-the-art performance on multiple benchmarks.", "motivation": "Accurate identification of drug-target interactions (DTI) remains a central challenge in computational pharmacology, where sequence-based methods offer scalability but often lack structural awareness.", "method": "Sequence-based DTI framework that integrates structural priors into protein representations, using learned aggregation, bilinear attention, and contrastive alignment techniques.", "result": "Achieves state-of-the-art performance on Human and BioSNAP datasets, remains competitive on BindingDB, and surpasses prior methods in virtual screening tasks on LIT-PCBA with substantial gains in AUROC and BEDROC metrics.", "conclusion": "The framework demonstrates utility for scalable and structure-aware DTI prediction, with embedding visualizations confirming improved spatial correspondence with known binding pockets and interpretable attention patterns."}}
{"id": "2509.14801", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.14801", "abs": "https://arxiv.org/abs/2509.14801", "authors": ["Julian F. Schumann", "Anna M\u00e9sz\u00e1ros", "Jens Kober", "Arkady Zgonnikov"], "title": "STEP: Structured Training and Evaluation Platform for benchmarking trajectory prediction models", "comment": null, "summary": "While trajectory prediction plays a critical role in enabling safe and\neffective path-planning in automated vehicles, standardized practices for\nevaluating such models remain underdeveloped. Recent efforts have aimed to\nunify dataset formats and model interfaces for easier comparisons, yet existing\nframeworks often fall short in supporting heterogeneous traffic scenarios,\njoint prediction models, or user documentation. In this work, we introduce STEP\n-- a new benchmarking framework that addresses these limitations by providing a\nunified interface for multiple datasets, enforcing consistent training and\nevaluation conditions, and supporting a wide range of prediction models. We\ndemonstrate the capabilities of STEP in a number of experiments which reveal 1)\nthe limitations of widely-used testing procedures, 2) the importance of joint\nmodeling of agents for better predictions of interactions, and 3) the\nvulnerability of current state-of-the-art models against both distribution\nshifts and targeted attacks by adversarial agents. With STEP, we aim to shift\nthe focus from the ``leaderboard'' approach to deeper insights about model\nbehavior and generalization in complex multi-agent settings.", "AI": {"tldr": "STEP is a new benchmarking framework for trajectory prediction models that addresses limitations of existing frameworks by providing unified dataset interfaces, consistent evaluation conditions, and support for diverse prediction models including joint modeling approaches.", "motivation": "Standardized evaluation practices for trajectory prediction models are underdeveloped, with existing frameworks lacking support for heterogeneous traffic scenarios, joint prediction models, and proper user documentation, making fair comparisons difficult.", "method": "The authors introduce STEP framework that provides a unified interface for multiple datasets, enforces consistent training and evaluation conditions, and supports a wide range of prediction models including joint modeling approaches.", "result": "Experiments reveal limitations of current testing procedures, demonstrate the importance of joint agent modeling for interaction prediction, and show vulnerability of state-of-the-art models to distribution shifts and adversarial attacks.", "conclusion": "STEP aims to shift focus from leaderboard rankings to deeper insights about model behavior and generalization in complex multi-agent settings, providing a more comprehensive evaluation framework for trajectory prediction research."}}
{"id": "2509.14821", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.14821", "abs": "https://arxiv.org/abs/2509.14821", "authors": ["Andrea Cavallo", "Samuel Rey", "Antonio G. Marques", "Elvin Isufi"], "title": "Precision Neural Networks: Joint Graph And Relational Learning", "comment": null, "summary": "CoVariance Neural Networks (VNNs) perform convolutions on the graph\ndetermined by the covariance matrix of the data, which enables expressive and\nstable covariance-based learning. However, covariance matrices are typically\ndense, fail to encode conditional independence, and are often precomputed in a\ntask-agnostic way, which may hinder performance. To overcome these limitations,\nwe study Precision Neural Networks (PNNs), i.e., VNNs on the precision matrix\n-- the inverse covariance. The precision matrix naturally encodes statistical\nindependence, often exhibits sparsity, and preserves the covariance spectral\nstructure. To make precision estimation task-aware, we formulate an\noptimization problem that jointly learns the network parameters and the\nprecision matrix, and solve it via alternating optimization, by sequentially\nupdating the network weights and the precision estimate. We theoretically bound\nthe distance between the estimated and true precision matrices at each\niteration, and demonstrate the effectiveness of joint estimation compared to\ntwo-step approaches on synthetic and real-world data.", "AI": {"tldr": "Precision Neural Networks (PNNs) extend VNNs by using precision matrices instead of covariance matrices, enabling joint learning of network parameters and precision estimation for better task-aware performance.", "motivation": "Covariance matrices are dense, don't encode conditional independence, and are often precomputed in a task-agnostic way, which limits performance of VNNs.", "method": "Formulate joint optimization problem for network parameters and precision matrix, solve via alternating optimization with sequential updates of weights and precision estimates.", "result": "Theoretical bounds on precision matrix estimation error and demonstrated effectiveness over two-step approaches on synthetic and real-world data.", "conclusion": "PNNs provide improved performance by leveraging precision matrices' sparsity, independence encoding, and joint task-aware estimation."}}
{"id": "2509.14832", "categories": ["cs.LG", "cs.AI", "cs.SY", "eess.SY", "I.2.6; I.5.1"], "pdf": "https://arxiv.org/pdf/2509.14832", "abs": "https://arxiv.org/abs/2509.14832", "authors": ["Stelios Zarifis", "Ioannis Kordonis", "Petros Maragos"], "title": "Diffusion-Based Scenario Tree Generation for Multivariate Time Series Prediction and Multistage Stochastic Optimization", "comment": "5 pages, 2 figures, 2 tables, and 1 algorithm. This version is\n  submitted to the 51st IEEE International Conference on Acoustics, Speech, and\n  Signal Processing (ICASSP 2026), to be held in Barcelona, Spain, on May 4-8,\n  2026", "summary": "Stochastic forecasting is critical for efficient decision-making in uncertain\nsystems, such as energy markets and finance, where estimating the full\ndistribution of future scenarios is essential. We propose Diffusion Scenario\nTree (DST), a general framework for constructing scenario trees for\nmultivariate prediction tasks using diffusion-based probabilistic forecasting\nmodels. DST recursively samples future trajectories and organizes them into a\ntree via clustering, ensuring non-anticipativity (decisions depending only on\nobserved history) at each stage. We evaluate the framework on the optimization\ntask of energy arbitrage in New York State's day-ahead electricity market.\nExperimental results show that our approach consistently outperforms the same\noptimization algorithms that use scenario trees from more conventional models\nand Model-Free Reinforcement Learning baselines. Furthermore, using DST for\nstochastic optimization yields more efficient decision policies, achieving\nhigher performance by better handling uncertainty than deterministic and\nstochastic MPC variants using the same diffusion-based forecaster.", "AI": {"tldr": "DST is a diffusion-based framework for creating scenario trees that enables better stochastic optimization in energy markets by handling uncertainty more effectively than conventional methods.", "motivation": "Stochastic forecasting is essential for decision-making in uncertain systems like energy markets, where full distribution estimation of future scenarios is needed for optimal performance.", "method": "Proposes Diffusion Scenario Tree (DST) framework that uses diffusion models to recursively sample future trajectories and organizes them into trees via clustering while maintaining non-anticipativity.", "result": "DST consistently outperforms conventional scenario tree models and Model-Free Reinforcement Learning baselines in energy arbitrage optimization tasks in New York's electricity market.", "conclusion": "Using DST for stochastic optimization yields more efficient decision policies with higher performance by better handling uncertainty compared to deterministic and stochastic MPC variants using the same diffusion forecaster."}}
{"id": "2509.14863", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.14863", "abs": "https://arxiv.org/abs/2509.14863", "authors": ["Zhengwei Wang", "Gang Wu"], "title": "Exploring the Global-to-Local Attention Scheme in Graph Transformers: An Empirical Study", "comment": null, "summary": "Graph Transformers (GTs) show considerable potential in graph representation\nlearning. The architecture of GTs typically integrates Graph Neural Networks\n(GNNs) with global attention mechanisms either in parallel or as a precursor to\nattention mechanisms, yielding a local-and-global or local-to-global attention\nscheme. However, as the global attention mechanism primarily captures\nlong-range dependencies between nodes, these integration schemes may suffer\nfrom information loss, where the local neighborhood information learned by GNN\ncould be diluted by the attention mechanism. Therefore, we propose G2LFormer,\nfeaturing a novel global-to-local attention scheme where the shallow network\nlayers use attention mechanisms to capture global information, while the deeper\nlayers employ GNN modules to learn local structural information, thereby\npreventing nodes from ignoring their immediate neighbors. An effective\ncross-layer information fusion strategy is introduced to allow local layers to\nretain beneficial information from global layers and alleviate information\nloss, with acceptable trade-offs in scalability. To validate the feasibility of\nthe global-to-local attention scheme, we compare G2LFormer with\nstate-of-the-art linear GTs and GNNs on node-level and graph-level tasks. The\nresults indicate that G2LFormer exhibits excellent performance while keeping\nlinear complexity.", "AI": {"tldr": "G2LFormer is a Graph Transformer that reverses traditional attention schemes by using global attention in shallow layers and local GNN modules in deeper layers, preventing local information loss while maintaining linear complexity.", "motivation": "Existing Graph Transformers integrate GNNs with global attention in ways that may dilute local neighborhood information learned by GNNs, causing information loss.", "method": "Proposes global-to-local attention scheme: shallow layers use attention for global information, deeper layers use GNNs for local structure, with cross-layer fusion to retain beneficial global information.", "result": "G2LFormer shows excellent performance on node-level and graph-level tasks while maintaining linear complexity, outperforming state-of-the-art linear GTs and GNNs.", "conclusion": "The global-to-local attention scheme effectively prevents nodes from ignoring immediate neighbors and alleviates information loss with acceptable scalability trade-offs."}}
{"id": "2509.14848", "categories": ["cs.LG", "eess.SP"], "pdf": "https://arxiv.org/pdf/2509.14848", "abs": "https://arxiv.org/abs/2509.14848", "authors": ["Houssem Sifaou", "Osvaldo Simeone"], "title": "Multi-Fidelity Hybrid Reinforcement Learning via Information Gain Maximization", "comment": null, "summary": "Optimizing a reinforcement learning (RL) policy typically requires extensive\ninteractions with a high-fidelity simulator of the environment, which are often\ncostly or impractical. Offline RL addresses this problem by allowing training\nfrom pre-collected data, but its effectiveness is strongly constrained by the\nsize and quality of the dataset. Hybrid offline-online RL leverages both\noffline data and interactions with a single simulator of the environment. In\nmany real-world scenarios, however, multiple simulators with varying levels of\nfidelity and computational cost are available. In this work, we study\nmulti-fidelity hybrid RL for policy optimization under a fixed cost budget. We\nintroduce multi-fidelity hybrid RL via information gain maximization\n(MF-HRL-IGM), a hybrid offline-online RL algorithm that implements fidelity\nselection based on information gain maximization through a bootstrapping\napproach. Theoretical analysis establishes the no-regret property of\nMF-HRL-IGM, while empirical evaluations demonstrate its superior performance\ncompared to existing benchmarks.", "AI": {"tldr": "MF-HRL-IGM is a multi-fidelity hybrid RL algorithm that optimizes policy training by selecting simulation fidelity levels based on information gain maximization, achieving no-regret performance under fixed cost constraints.", "motivation": "Traditional RL requires expensive high-fidelity simulator interactions, while offline RL is limited by dataset quality. Real-world scenarios often have multiple simulators with varying fidelity and cost, but existing methods don't effectively leverage this multi-fidelity setup.", "method": "Proposes MF-HRL-IGM algorithm that uses information gain maximization through bootstrapping to select appropriate fidelity levels for hybrid offline-online RL training under fixed cost budget.", "result": "Theoretical analysis proves the algorithm has no-regret property. Empirical evaluations show superior performance compared to existing benchmarks.", "conclusion": "MF-HRL-IGM effectively leverages multiple simulators with varying fidelity levels to optimize RL policy training while respecting cost constraints, outperforming traditional approaches."}}
{"id": "2509.14868", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.14868", "abs": "https://arxiv.org/abs/2509.14868", "authors": ["Qianyang Li", "Xingjun Zhang", "Shaoxun Wang", "Jia Wei"], "title": "DPANet: Dual Pyramid Attention Network for Multivariate Time Series Forecasting", "comment": null, "summary": "We conducted rigorous ablation studies to validate DPANet's key components\n(Table \\ref{tab:ablation-study}). The full model consistently outperforms all\nvariants. To test our dual-domain hypothesis, we designed two specialized\nversions: a Temporal-Only model (fusing two identical temporal pyramids) and a\nFrequency-Only model (fusing two spectral pyramids). Both variants\nunderperformed significantly, confirming that the fusion of heterogeneous\ntemporal and frequency information is critical. Furthermore, replacing the\ncross-attention mechanism with a simpler method (w/o Cross-Fusion) caused the\nmost severe performance degradation. This result underscores that our\ninteractive fusion block is the most essential component.", "AI": {"tldr": "Ablation studies confirm DPANet's dual-domain architecture and cross-attention fusion are essential for optimal performance", "motivation": "To validate the importance of combining temporal and frequency domains through interactive fusion in DPANet architecture", "method": "Conducted ablation studies comparing full DPANet against Temporal-Only and Frequency-Only variants, and tested without cross-attention fusion mechanism", "result": "Full model consistently outperformed all variants; both single-domain versions underperformed significantly; removing cross-attention caused the most severe performance degradation", "conclusion": "Fusion of heterogeneous temporal and frequency information is critical, with cross-attention mechanism being the most essential component"}}
{"id": "2509.15024", "categories": ["cs.LG", "cs.AI", "cs.NI"], "pdf": "https://arxiv.org/pdf/2509.15024", "abs": "https://arxiv.org/abs/2509.15024", "authors": ["Xuanting Xie", "Bingheng Li", "Erlin Pan", "Rui Hou", "Wenyu Chen", "Zhao Kang"], "title": "Attention Beyond Neighborhoods: Reviving Transformer for Graph Clustering", "comment": "9 pages, 5 figures", "summary": "Attention mechanisms have become a cornerstone in modern neural networks,\ndriving breakthroughs across diverse domains. However, their application to\ngraph structured data, where capturing topological connections is essential,\nremains underexplored and underperforming compared to Graph Neural Networks\n(GNNs), particularly in the graph clustering task. GNN tends to overemphasize\nneighborhood aggregation, leading to a homogenization of node representations.\nConversely, Transformer tends to over globalize, highlighting distant nodes at\nthe expense of meaningful local patterns. This dichotomy raises a key question:\nIs attention inherently redundant for unsupervised graph learning? To address\nthis, we conduct a comprehensive empirical analysis, uncovering the\ncomplementary weaknesses of GNN and Transformer in graph clustering. Motivated\nby these insights, we propose the Attentive Graph Clustering Network (AGCN) a\nnovel architecture that reinterprets the notion that graph is attention. AGCN\ndirectly embeds the attention mechanism into the graph structure, enabling\neffective global information extraction while maintaining sensitivity to local\ntopological cues. Our framework incorporates theoretical analysis to contrast\nAGCN behavior with GNN and Transformer and introduces two innovations: (1) a KV\ncache mechanism to improve computational efficiency, and (2) a pairwise margin\ncontrastive loss to boost the discriminative capacity of the attention space.\nExtensive experimental results demonstrate that AGCN outperforms\nstate-of-the-art methods.", "AI": {"tldr": "AGCN is a novel graph clustering network that integrates attention mechanisms directly into graph structure to overcome limitations of both GNNs (over-localizing) and Transformers (over-globalizing), achieving state-of-the-art performance.", "motivation": "Current attention mechanisms underperform on graph data compared to GNNs. GNNs overemphasize neighborhood aggregation causing homogenization, while Transformers over-globalize and miss local patterns. The paper questions if attention is redundant for unsupervised graph learning.", "method": "Proposes Attentive Graph Clustering Network (AGCN) that embeds attention directly into graph structure. Includes KV cache for computational efficiency and pairwise margin contrastive loss to enhance attention space discriminability. Theoretical analysis contrasts AGCN with GNN and Transformer behaviors.", "result": "Extensive experiments show AGCN outperforms state-of-the-art methods in graph clustering tasks.", "conclusion": "Attention is not redundant for graph learning when properly integrated. AGCN successfully combines the strengths of both GNNs and Transformers while mitigating their weaknesses, demonstrating effective global information extraction with local topological sensitivity."}}
{"id": "2509.15032", "categories": ["cs.LG", "cs.AI", "cs.NI"], "pdf": "https://arxiv.org/pdf/2509.15032", "abs": "https://arxiv.org/abs/2509.15032", "authors": ["Tianyang Duan", "Zongyuan Zhang", "Songxiao Guo", "Yuanye Zhao", "Zheng Lin", "Zihan Fang", "Yi Liu", "Dianxin Luan", "Dong Huang", "Heming Cui", "Yong Cui"], "title": "Sample Efficient Experience Replay in Non-stationary Environments", "comment": "5 pages, 3 figures", "summary": "Reinforcement learning (RL) in non-stationary environments is challenging, as\nchanging dynamics and rewards quickly make past experiences outdated.\nTraditional experience replay (ER) methods, especially those using TD-error\nprioritization, struggle to distinguish between changes caused by the agent's\npolicy and those from the environment, resulting in inefficient learning under\ndynamic conditions. To address this challenge, we propose the Discrepancy of\nEnvironment Dynamics (DoE), a metric that isolates the effects of environment\nshifts on value functions. Building on this, we introduce Discrepancy of\nEnvironment Prioritized Experience Replay (DEER), an adaptive ER framework that\nprioritizes transitions based on both policy updates and environmental changes.\nDEER uses a binary classifier to detect environment changes and applies\ndistinct prioritization strategies before and after each shift, enabling more\nsample-efficient learning. Experiments on four non-stationary benchmarks\ndemonstrate that DEER further improves the performance of off-policy algorithms\nby 11.54 percent compared to the best-performing state-of-the-art ER methods.", "AI": {"tldr": "DEER is a novel experience replay method that uses environmental discrepancy detection and adaptive prioritization to improve reinforcement learning in non-stationary environments, achieving 11.54% performance improvement over state-of-the-art methods.", "motivation": "Traditional experience replay methods struggle in non-stationary environments because they cannot distinguish between policy-induced changes and actual environmental shifts, leading to inefficient learning when dynamics and rewards change over time.", "method": "Proposes Discrepancy of Environment Dynamics (DoE) metric to isolate environmental shift effects, and DEER framework that uses a binary classifier to detect environment changes and applies different prioritization strategies before/after each shift.", "result": "Experiments on four non-stationary benchmarks show DEER improves performance of off-policy algorithms by 11.54% compared to the best state-of-the-art experience replay methods.", "conclusion": "DEER successfully addresses the challenge of non-stationary environments by adaptively prioritizing experiences based on detected environmental changes, enabling more sample-efficient reinforcement learning."}}
{"id": "2509.14887", "categories": ["cs.LG", "eess.SP"], "pdf": "https://arxiv.org/pdf/2509.14887", "abs": "https://arxiv.org/abs/2509.14887", "authors": ["Hoang-Son Nguyen", "Hoi-To Wai"], "title": "Learning Graph from Smooth Signals under Partial Observation: A Robustness Analysis", "comment": "7 pages, 3 figures", "summary": "Learning the graph underlying a networked system from nodal signals is\ncrucial to downstream tasks in graph signal processing and machine learning.\nThe presence of hidden nodes whose signals are not observable might corrupt the\nestimated graph. While existing works proposed various robustifications of\nvanilla graph learning objectives by explicitly accounting for the presence of\nthese hidden nodes, a robustness analysis of \"naive\", hidden-node agnostic\napproaches is still underexplored. This work demonstrates that vanilla graph\ntopology learning methods are implicitly robust to partial observations of\nlow-pass filtered graph signals. We achieve this theoretical result through\nextending the restricted isometry property (RIP) to the Dirichlet energy\nfunction used in graph learning objectives. We show that smoothness-based graph\nlearning formulation (e.g., the GL-SigRep method) on partial observations can\nrecover the ground truth graph topology corresponding to the observed nodes.\nSynthetic and real data experiments corroborate our findings.", "AI": {"tldr": "Vanilla graph topology learning methods are implicitly robust to partial observations of low-pass filtered graph signals, recovering ground truth graph topology even with hidden nodes.", "motivation": "Existing graph learning methods are vulnerable to corruption from hidden nodes whose signals are unobservable, but robustness analysis of naive approaches is underexplored.", "method": "Extend the restricted isometry property (RIP) to Dirichlet energy function used in graph learning objectives, showing smoothness-based formulations like GL-SigRep can recover ground truth topology from partial observations.", "result": "Theoretical analysis demonstrates implicit robustness, and synthetic/real data experiments corroborate that vanilla methods can recover correct graph topology for observed nodes despite hidden nodes.", "conclusion": "Vanilla graph learning approaches possess inherent robustness to partial observations of low-pass filtered signals, providing theoretical foundation for their effectiveness even with unobserved hidden nodes."}}
{"id": "2509.15040", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.15040", "abs": "https://arxiv.org/abs/2509.15040", "authors": ["Juwon Kim", "Hyunwook Lee", "Hyotaek Jeon", "Seungmin Jin", "Sungahn Ko"], "title": "From Patterns to Predictions: A Shapelet-Based Framework for Directional Forecasting in Noisy Financial Markets", "comment": "10 pages, 7 figures, accepted at ACM CIKM 2025 conference", "summary": "Directional forecasting in financial markets requires both accuracy and\ninterpretability. Before the advent of deep learning, interpretable approaches\nbased on human-defined patterns were prevalent, but their structural vagueness\nand scale ambiguity hindered generalization. In contrast, deep learning models\ncan effectively capture complex dynamics, yet often offer limited transparency.\nTo bridge this gap, we propose a two-stage framework that integrates\nunsupervised pattern extracion with interpretable forecasting. (i) SIMPC\nsegments and clusters multivariate time series, extracting recurrent patterns\nthat are invariant to amplitude scaling and temporal distortion, even under\nvarying window sizes. (ii) JISC-Net is a shapelet-based classifier that uses\nthe initial part of extracted patterns as input and forecasts subsequent\npartial sequences for short-term directional movement. Experiments on Bitcoin\nand three S&P 500 equities demonstrate that our method ranks first or second in\n11 out of 12 metric--dataset combinations, consistently outperforming\nbaselines. Unlike conventional deep learning models that output buy-or-sell\nsignals without interpretable justification, our approach enables transparent\ndecision-making by revealing the underlying pattern structures that drive\npredictive outcomes.", "AI": {"tldr": "A two-stage framework combining unsupervised pattern extraction with interpretable forecasting for financial directional prediction, achieving top performance while maintaining transparency.", "motivation": "Bridge the gap between accurate but opaque deep learning models and interpretable but limited traditional pattern-based approaches in financial forecasting.", "method": "Two-stage approach: (1) SIMPC for segmenting/clustering multivariate time series to extract amplitude and time-invariant patterns, (2) JISC-Net shapelet-based classifier using pattern beginnings to forecast short-term directional movements.", "result": "Ranked first or second in 11 out of 12 metric-dataset combinations on Bitcoin and S&P 500 equities, consistently outperforming baselines.", "conclusion": "The framework provides both high accuracy and interpretability by revealing underlying pattern structures that drive predictions, enabling transparent decision-making in financial markets."}}
{"id": "2509.14894", "categories": ["cs.LG", "hep-ex"], "pdf": "https://arxiv.org/pdf/2509.14894", "abs": "https://arxiv.org/abs/2509.14894", "authors": ["Guillermo Hijano Mendizabal", "Davide Lancierini", "Alex Marshall", "Andrea Mauri", "Patrick Haworth Owen", "Mitesh Patel", "Konstantinos Petridis", "Shah Rukh Qasim", "Nicola Serra", "William Sutcliffe", "Hanae Tilquin"], "title": "Leveraging Reinforcement Learning, Genetic Algorithms and Transformers for background determination in particle physics", "comment": "32 pages, 12 figures", "summary": "Experimental studies of beauty hadron decays face significant challenges due\nto a wide range of backgrounds arising from the numerous possible decay\nchannels with similar final states. For a particular signal decay, the process\nfor ascertaining the most relevant background processes necessitates a detailed\nanalysis of final state particles, potential misidentifications, and kinematic\noverlaps, which, due to computational limitations, is restricted to the\nsimulation of only the most relevant backgrounds. Moreover, this process\ntypically relies on the physicist's intuition and expertise, as no systematic\nmethod exists.\n  This paper has two primary goals. First, from a particle physics perspective,\nwe present a novel approach that utilises Reinforcement Learning (RL) to\novercome the aforementioned challenges by systematically determining the\ncritical backgrounds affecting beauty hadron decay measurements. While beauty\nhadron physics serves as the case study in this work, the proposed strategy is\nbroadly adaptable to other types of particle physics measurements. Second, from\na Machine Learning perspective, we introduce a novel algorithm which exploits\nthe synergy between RL and Genetic Algorithms (GAs) for environments with\nhighly sparse rewards and a large trajectory space. This strategy leverages GAs\nto efficiently explore the trajectory space and identify successful\ntrajectories, which are used to guide the RL agent's training. Our method also\nincorporates a transformer architecture for the RL agent to handle token\nsequences representing decays.", "AI": {"tldr": "A novel RL-GA hybrid approach to systematically identify critical background processes in beauty hadron decay measurements, addressing sparse rewards and large search spaces.", "motivation": "Overcoming challenges in beauty hadron decay analysis where traditional methods rely on physicist intuition and computational limitations restrict background simulation to only the most relevant processes.", "method": "Combines Reinforcement Learning with Genetic Algorithms, using GAs to explore large trajectory spaces and identify successful paths, then guides RL agent training with transformer architecture to handle token sequences representing decays.", "result": "Developed a systematic method to determine critical background processes that was previously unavailable, with broad applicability beyond beauty hadron physics to other particle physics measurements.", "conclusion": "The RL-GA hybrid approach successfully addresses sparse reward environments and large search spaces, providing a systematic solution for background identification in particle physics that reduces reliance on expert intuition."}}
{"id": "2509.15042", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.15042", "abs": "https://arxiv.org/abs/2509.15042", "authors": ["Thomas Ackermann", "Moritz Spang", "Hamza A. A. Gardi"], "title": "Reinforcement Learning Agent for a 2D Shooter Game", "comment": null, "summary": "Reinforcement learning agents in complex game environments often suffer from\nsparse rewards, training instability, and poor sample efficiency. This paper\npresents a hybrid training approach that combines offline imitation learning\nwith online reinforcement learning for a 2D shooter game agent. We implement a\nmulti-head neural network with separate outputs for behavioral cloning and\nQ-learning, unified by shared feature extraction layers with attention\nmechanisms. Initial experiments using pure deep Q-Networks exhibited\nsignificant instability, with agents frequently reverting to poor policies\ndespite occasional good performance. To address this, we developed a hybrid\nmethodology that begins with behavioral cloning on demonstration data from\nrule-based agents, then transitions to reinforcement learning. Our hybrid\napproach achieves consistently above 70% win rate against rule-based opponents,\nsubstantially outperforming pure reinforcement learning methods which showed\nhigh variance and frequent performance degradation. The multi-head architecture\nenables effective knowledge transfer between learning modes while maintaining\ntraining stability. Results demonstrate that combining demonstration-based\ninitialization with reinforcement learning optimization provides a robust\nsolution for developing game AI agents in complex multi-agent environments\nwhere pure exploration proves insufficient.", "AI": {"tldr": "Hybrid training combining offline imitation learning with online RL for 2D shooter game agents, using multi-head neural network with attention mechanisms to address sparse rewards and training instability.", "motivation": "Reinforcement learning agents in complex game environments suffer from sparse rewards, training instability, and poor sample efficiency, making pure exploration insufficient.", "method": "Multi-head neural network with separate outputs for behavioral cloning and Q-learning, unified by shared feature extraction layers with attention mechanisms. Starts with behavioral cloning on demonstration data, then transitions to reinforcement learning.", "result": "Achieves consistently above 70% win rate against rule-based opponents, substantially outperforming pure RL methods which showed high variance and frequent performance degradation.", "conclusion": "Combining demonstration-based initialization with reinforcement learning optimization provides a robust solution for developing game AI agents in complex multi-agent environments."}}
{"id": "2509.14904", "categories": ["cs.LG", "cs.CG"], "pdf": "https://arxiv.org/pdf/2509.14904", "abs": "https://arxiv.org/abs/2509.14904", "authors": ["Keanu Sisouk", "Eloi Tanguy", "Julie Delon", "Julien Tierny"], "title": "Robust Barycenters of Persistence Diagrams", "comment": null, "summary": "This short paper presents a general approach for computing robust Wasserstein\nbarycenters of persistence diagrams. The classical method consists in computing\nassignment arithmetic means after finding the optimal transport plans between\nthe barycenter and the persistence diagrams. However, this procedure only works\nfor the transportation cost related to the $q$-Wasserstein distance $W_q$ when\n$q=2$. We adapt an alternative fixed-point method to compute a barycenter\ndiagram for generic transportation costs ($q > 1$), in particular those robust\nto outliers, $q \\in (1,2)$. We show the utility of our work in two\napplications: \\emph{(i)} the clustering of persistence diagrams on their metric\nspace and \\emph{(ii)} the dictionary encoding of persistence diagrams. In both\nscenarios, we demonstrate the added robustness to outliers provided by our\ngeneralized framework. Our Python implementation is available at this address:\nhttps://github.com/Keanu-Sisouk/RobustBarycenter .", "AI": {"tldr": "General approach for computing robust Wasserstein barycenters of persistence diagrams using fixed-point method for q>1 transportation costs, particularly robust q\u2208(1,2) cases.", "motivation": "Classical methods only work for q=2 Wasserstein distance, limiting robustness to outliers. Need for generalized framework that handles q>1 transportation costs for outlier-resistant barycenter computation.", "method": "Adapted fixed-point method to compute barycenter diagrams for generic transportation costs (q>1), particularly focusing on robust q\u2208(1,2) cases that are resistant to outliers.", "result": "Successfully implemented robust barycenter computation for persistence diagrams. Demonstrated utility in clustering persistence diagrams and dictionary encoding, showing improved robustness to outliers compared to classical q=2 methods.", "conclusion": "The generalized framework provides robust Wasserstein barycenter computation for persistence diagrams with q>1 transportation costs, offering significant outlier resistance particularly for q\u2208(1,2), with practical applications in clustering and dictionary encoding tasks."}}
{"id": "2509.15044", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.15044", "abs": "https://arxiv.org/abs/2509.15044", "authors": ["Iva Popova", "Hamza A. A. Gardi"], "title": "Credit Card Fraud Detection", "comment": null, "summary": "Credit card fraud remains a significant challenge due to class imbalance and\nfraudsters mimicking legitimate behavior. This study evaluates five machine\nlearning models - Logistic Regression, Random Forest, XGBoost, K-Nearest\nNeighbors (KNN), and Multi-Layer Perceptron (MLP) on a real-world dataset using\nundersampling, SMOTE, and a hybrid approach. Our models are evaluated on the\noriginal imbalanced test set to better reflect real-world performance. Results\nshow that the hybrid method achieves the best balance between recall and\nprecision, especially improving MLP and KNN performance.", "AI": {"tldr": "Evaluation of 5 ML models for credit card fraud detection using undersampling, SMOTE, and hybrid approaches on imbalanced data, with hybrid method achieving best recall-precision balance.", "motivation": "Credit card fraud detection faces challenges due to class imbalance and fraudsters mimicking legitimate behavior, requiring effective ML approaches.", "method": "Tested Logistic Regression, Random Forest, XGBoost, KNN, and MLP on real-world dataset using undersampling, SMOTE, and hybrid approach, evaluated on original imbalanced test set.", "result": "Hybrid method achieved the best balance between recall and precision, particularly improving performance of MLP and KNN models.", "conclusion": "Hybrid sampling approach shows superior performance in handling class imbalance for credit card fraud detection compared to individual sampling methods."}}
{"id": "2509.14925", "categories": ["cs.LG", "cs.NI"], "pdf": "https://arxiv.org/pdf/2509.14925", "abs": "https://arxiv.org/abs/2509.14925", "authors": ["Konrad Nowosadko", "Franco Ruggeri", "Ahmad Terra"], "title": "Self-Explaining Reinforcement Learning for Mobile Network Resource Allocation", "comment": null, "summary": "Reinforcement Learning (RL) methods that incorporate deep neural networks\n(DNN), though powerful, often lack transparency. Their black-box characteristic\nhinders interpretability and reduces trustworthiness, particularly in critical\ndomains. To address this challenge in RL tasks, we propose a solution based on\nSelf-Explaining Neural Networks (SENNs) along with explanation extraction\nmethods to enhance interpretability while maintaining predictive accuracy. Our\napproach targets low-dimensionality problems to generate robust local and\nglobal explanations of the model's behaviour. We evaluate the proposed method\non the resource allocation problem in mobile networks, demonstrating that SENNs\ncan constitute interpretable solutions with competitive performance. This work\nhighlights the potential of SENNs to improve transparency and trust in\nAI-driven decision-making for low-dimensional tasks. Our approach strong\nperformance on par with the existing state-of-the-art methods, while providing\nrobust explanations.", "AI": {"tldr": "Proposes using Self-Explaining Neural Networks (SENNs) to make deep reinforcement learning more interpretable while maintaining competitive performance on low-dimensional tasks like mobile network resource allocation.", "motivation": "Deep reinforcement learning methods lack transparency and interpretability, which reduces trustworthiness in critical domains. The black-box nature of DNNs hinders understanding of model behavior.", "method": "Uses Self-Explaining Neural Networks (SENNs) with explanation extraction methods to enhance interpretability while preserving predictive accuracy. Focuses on low-dimensionality problems to generate robust local and global explanations.", "result": "The approach demonstrates competitive performance on par with state-of-the-art methods while providing robust explanations. Successfully evaluated on mobile network resource allocation problems.", "conclusion": "SENNs show potential to improve transparency and trust in AI-driven decision-making for low-dimensional tasks, offering interpretable solutions without sacrificing performance."}}
{"id": "2509.15057", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.15057", "abs": "https://arxiv.org/abs/2509.15057", "authors": ["Quincy Hershey", "Randy Paffenroth"], "title": "Balancing Sparse RNNs with Hyperparameterization Benefiting Meta-Learning", "comment": null, "summary": "This paper develops alternative hyperparameters for specifying sparse\nRecurrent Neural Networks (RNNs). These hyperparameters allow for varying\nsparsity within the trainable weight matrices of the model while improving\noverall performance. This architecture enables the definition of a novel\nmetric, hidden proportion, which seeks to balance the distribution of unknowns\nwithin the model and provides significant explanatory power of model\nperformance. Together, the use of the varied sparsity RNN architecture combined\nwith the hidden proportion metric generates significant performance gains while\nimproving performance expectations on an a priori basis. This combined approach\nprovides a path forward towards generalized meta-learning applications and\nmodel optimization based on intrinsic characteristics of the data set,\nincluding input and output dimensions.", "AI": {"tldr": "Novel hyperparameters for sparse RNNs with varying sparsity levels, introducing hidden proportion metric that balances unknowns and predicts performance, enabling meta-learning applications.", "motivation": "To develop improved sparse RNN architectures with better performance predictability and explanatory power for model optimization based on dataset characteristics.", "method": "Developed alternative hyperparameters for specifying sparse RNNs with varying sparsity in weight matrices, and created a hidden proportion metric to balance unknown distribution.", "result": "Achieved significant performance gains with improved a priori performance expectations through the combined sparse RNN architecture and hidden proportion metric.", "conclusion": "The approach provides a path toward generalized meta-learning applications and model optimization based on intrinsic dataset characteristics like input/output dimensions."}}
{"id": "2509.14933", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.14933", "abs": "https://arxiv.org/abs/2509.14933", "authors": ["Xiangfei Qiu", "Yuhan Zhu", "Zhengyu Li", "Hanyin Cheng", "Xingjian Wu", "Chenjuan Guo", "Bin Yang", "Jilin Hu"], "title": "DAG: A Dual Causal Network for Time Series Forecasting with Exogenous Variables", "comment": null, "summary": "Time series forecasting is crucial in various fields such as economics,\ntraffic, and AIOps. However, in real-world applications, focusing solely on the\nendogenous variables (i.e., target variables), is often insufficient to ensure\naccurate predictions. Considering exogenous variables (i.e., covariates)\nprovides additional predictive information, thereby improving forecasting\naccuracy. However, existing methods for time series forecasting with exogenous\nvariables (TSF-X) have the following shortcomings: 1) they do not leverage\nfuture exogenous variables, 2) they fail to account for the causal\nrelationships between endogenous and exogenous variables. As a result, their\nperformance is suboptimal. In this study, to better leverage exogenous\nvariables, especially future exogenous variable, we propose a general framework\nDAG, which utilizes dual causal network along both the temporal and channel\ndimensions for time series forecasting with exogenous variables. Specifically,\nwe first introduce the Temporal Causal Module, which includes a causal\ndiscovery module to capture how historical exogenous variables affect future\nexogenous variables. Following this, we construct a causal injection module\nthat incorporates the discovered causal relationships into the process of\nforecasting future endogenous variables based on historical endogenous\nvariables. Next, we propose the Channel Causal Module, which follows a similar\ndesign principle. It features a causal discovery module models how historical\nexogenous variables influence historical endogenous variables, and a causal\ninjection module incorporates the discovered relationships to enhance the\nprediction of future endogenous variables based on future exogenous variables.", "AI": {"tldr": "Proposes DAG framework with dual causal networks (temporal and channel dimensions) to better leverage exogenous variables, especially future ones, for improved time series forecasting accuracy.", "motivation": "Existing methods for time series forecasting with exogenous variables (TSF-X) fail to leverage future exogenous variables and ignore causal relationships between endogenous and exogenous variables, leading to suboptimal performance.", "method": "Dual causal network framework with: 1) Temporal Causal Module - causal discovery of historical exogenous variables affecting future exogenous variables, and causal injection for forecasting; 2) Channel Causal Module - causal discovery of historical exogenous variables influencing historical endogenous variables, and causal injection using future exogenous variables.", "result": "The proposed DAG framework aims to improve forecasting accuracy by properly incorporating causal relationships and future exogenous variables.", "conclusion": "The DAG framework provides a comprehensive approach to leverage exogenous variables through causal modeling in both temporal and channel dimensions, addressing limitations of existing TSF-X methods."}}
{"id": "2509.15058", "categories": ["cs.LG", "cs.AI", "cs.CV", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.15058", "abs": "https://arxiv.org/abs/2509.15058", "authors": ["Federico Alvetreti", "Jary Pomponi", "Paolo Di Lorenzo", "Simone Scardapane"], "title": "Communication Efficient Split Learning of ViTs with Attention-based Double Compression", "comment": null, "summary": "This paper proposes a novel communication-efficient Split Learning (SL)\nframework, named Attention-based Double Compression (ADC), which reduces the\ncommunication overhead required for transmitting intermediate Vision\nTransformers activations during the SL training process. ADC incorporates two\nparallel compression strategies. The first one merges samples' activations that\nare similar, based on the average attention score calculated in the last client\nlayer; this strategy is class-agnostic, meaning that it can also merge samples\nhaving different classes, without losing generalization ability nor decreasing\nfinal results. The second strategy follows the first and discards the least\nmeaningful tokens, further reducing the communication cost. Combining these\nstrategies not only allows for sending less during the forward pass, but also\nthe gradients are naturally compressed, allowing the whole model to be trained\nwithout additional tuning or approximations of the gradients. Simulation\nresults demonstrate that Attention-based Double Compression outperforms\nstate-of-the-art SL frameworks by significantly reducing communication\noverheads while maintaining high accuracy.", "AI": {"tldr": "ADC framework reduces communication overhead in Split Learning for Vision Transformers by using two compression strategies: merging similar activations based on attention scores and discarding least meaningful tokens.", "motivation": "To address the high communication overhead required for transmitting intermediate Vision Transformers activations during Split Learning training process.", "method": "Proposes Attention-based Double Compression (ADC) with two parallel strategies: 1) class-agnostic merging of similar activations based on average attention scores, and 2) discarding least meaningful tokens to further reduce communication cost.", "result": "ADC outperforms state-of-the-art SL frameworks by significantly reducing communication overheads while maintaining high accuracy.", "conclusion": "The combined compression strategies enable sending less data during forward pass and naturally compress gradients, allowing whole model training without additional tuning or gradient approximations."}}
{"id": "2509.14936", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.14936", "abs": "https://arxiv.org/abs/2509.14936", "authors": ["Rohan Veit", "Michael Lones"], "title": "A Comparative Analysis of Transformer Models in Social Bot Detection", "comment": "To appear in proceedings of UKCI 2025", "summary": "Social media has become a key medium of communication in today's society.\nThis realisation has led to many parties employing artificial users (or bots)\nto mislead others into believing untruths or acting in a beneficial manner to\nsuch parties. Sophisticated text generation tools, such as large language\nmodels, have further exacerbated this issue. This paper aims to compare the\neffectiveness of bot detection models based on encoder and decoder\ntransformers. Pipelines are developed to evaluate the performance of these\nclassifiers, revealing that encoder-based classifiers demonstrate greater\naccuracy and robustness. However, decoder-based models showed greater\nadaptability through task-specific alignment, suggesting more potential for\ngeneralisation across different use cases in addition to superior observa.\nThese findings contribute to the ongoing effort to prevent digital environments\nbeing manipulated while protecting the integrity of online discussion.", "AI": {"tldr": "Comparison of encoder vs decoder transformer models for bot detection, showing encoders are more accurate but decoders have better generalization potential.", "motivation": "Social media bots using advanced text generation tools like LLMs are misleading users and manipulating online discussions, requiring effective detection methods.", "method": "Developed evaluation pipelines to test performance of encoder-based and decoder-based transformer classifiers for bot detection.", "result": "Encoder-based classifiers demonstrated greater accuracy and robustness, while decoder-based models showed better adaptability through task-specific alignment.", "conclusion": "Both encoder and decoder approaches contribute to preventing digital manipulation, with encoders offering better accuracy and decoders providing superior generalization potential across different use cases."}}
{"id": "2509.14938", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.14938", "abs": "https://arxiv.org/abs/2509.14938", "authors": ["Zeyu Chen", "Wen Chen", "Jun Li", "Qingqing Wu", "Ming Ding", "Xuefeng Han", "Xiumei Deng", "Liwei Wang"], "title": "Hierarchical Federated Learning for Social Network with Mobility", "comment": null, "summary": "Federated Learning (FL) offers a decentralized solution that allows\ncollaborative local model training and global aggregation, thereby protecting\ndata privacy. In conventional FL frameworks, data privacy is typically\npreserved under the assumption that local data remains absolutely private,\nwhereas the mobility of clients is frequently neglected in explicit modeling.\nIn this paper, we propose a hierarchical federated learning framework based on\nthe social network with mobility namely HFL-SNM that considers both data\nsharing among clients and their mobility patterns. Under the constraints of\nlimited resources, we formulate a joint optimization problem of resource\nallocation and client scheduling, which objective is to minimize the energy\nconsumption of clients during the FL process. In social network, we introduce\nthe concepts of Effective Data Coverage Rate and Redundant Data Coverage Rate.\nWe analyze the impact of effective data and redundant data on the model\nperformance through preliminary experiments. We decouple the optimization\nproblem into multiple sub-problems, analyze them based on preliminary\nexperimental results, and propose Dynamic Optimization in Social Network with\nMobility (DO-SNM) algorithm. Experimental results demonstrate that our\nalgorithm achieves superior model performance while significantly reducing\nenergy consumption, compared to traditional baseline algorithms.", "AI": {"tldr": "Proposes HFL-SNM, a hierarchical federated learning framework that incorporates social networks and client mobility to optimize resource allocation and energy consumption while maintaining data privacy.", "motivation": "Traditional FL frameworks assume absolute data privacy but neglect client mobility patterns, leading to inefficient resource usage and higher energy consumption during collaborative training.", "method": "Developed HFL-SNM framework with concepts of Effective Data Coverage Rate and Redundant Data Coverage Rate. Formulated joint optimization problem for resource allocation and client scheduling, then decoupled into sub-problems solved by DO-SNM algorithm.", "result": "Experimental results show superior model performance with significantly reduced energy consumption compared to traditional baseline algorithms.", "conclusion": "The proposed HFL-SNM framework successfully addresses mobility-aware federated learning challenges, achieving better model performance while optimizing energy efficiency through social network-based resource allocation."}}
{"id": "2509.15207", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.15207", "abs": "https://arxiv.org/abs/2509.15207", "authors": ["Xuekai Zhu", "Daixuan Cheng", "Dinghuai Zhang", "Hengli Li", "Kaiyan Zhang", "Che Jiang", "Youbang Sun", "Ermo Hua", "Yuxin Zuo", "Xingtai Lv", "Qizheng Zhang", "Lin Chen", "Fanghao Shao", "Bo Xue", "Yunchong Song", "Zhenjie Yang", "Ganqu Cui", "Ning Ding", "Jianfeng Gao", "Xiaodong Liu", "Bowen Zhou", "Hongyuan Mei", "Zhouhan Lin"], "title": "FlowRL: Matching Reward Distributions for LLM Reasoning", "comment": null, "summary": "We propose FlowRL: matching the full reward distribution via flow balancing\ninstead of maximizing rewards in large language model (LLM) reinforcement\nlearning (RL). Recent advanced reasoning models adopt reward-maximizing methods\n(\\eg, PPO and GRPO), which tend to over-optimize dominant reward signals while\nneglecting less frequent but valid reasoning paths, thus reducing diversity. In\ncontrast, we transform scalar rewards into a normalized target distribution\nusing a learnable partition function, and then minimize the reverse KL\ndivergence between the policy and the target distribution. We implement this\nidea as a flow-balanced optimization method that promotes diverse exploration\nand generalizable reasoning trajectories. We conduct experiments on math and\ncode reasoning tasks: FlowRL achieves a significant average improvement of\n$10.0\\%$ over GRPO and $5.1\\%$ over PPO on math benchmarks, and performs\nconsistently better on code reasoning tasks. These results highlight reward\ndistribution-matching as a key step toward efficient exploration and diverse\nreasoning in LLM reinforcement learning.", "AI": {"tldr": "FlowRL proposes distribution matching instead of reward maximization in LLM RL to improve diversity and avoid over-optimizing dominant rewards.", "motivation": "Traditional reward-maximizing methods like PPO and GRPO tend to over-optimize dominant reward signals while neglecting less frequent but valid reasoning paths, reducing diversity in reasoning trajectories.", "method": "Transform scalar rewards into normalized target distribution using learnable partition function, then minimize reverse KL divergence between policy and target distribution through flow-balanced optimization.", "result": "Achieves 10.0% average improvement over GRPO and 5.1% over PPO on math benchmarks, with consistent better performance on code reasoning tasks.", "conclusion": "Reward distribution-matching is a key step toward efficient exploration and diverse reasoning in LLM reinforcement learning."}}
{"id": "2509.14945", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.14945", "abs": "https://arxiv.org/abs/2509.14945", "authors": ["Amsalu Tessema", "Tizazu Bayih", "Kassahun Azezew", "Ayenew Kassie"], "title": "Data-Driven Prediction of Maternal Nutritional Status in Ethiopia Using Ensemble Machine Learning Models", "comment": "9 pages, 5 figures, 2 Tables", "summary": "Malnutrition among pregnant women is a major public health challenge in\nEthiopia, increasing the risk of adverse maternal and neonatal outcomes.\nTraditional statistical approaches often fail to capture the complex and\nmultidimensional determinants of nutritional status. This study develops a\npredictive model using ensemble machine learning techniques, leveraging data\nfrom the Ethiopian Demographic and Health Survey (2005-2020), comprising 18,108\nrecords with 30 socio-demographic and health attributes. Data preprocessing\nincluded handling missing values, normalization, and balancing with SMOTE,\nfollowed by feature selection to identify key predictors. Several supervised\nensemble algorithms including XGBoost, Random Forest, CatBoost, and AdaBoost\nwere applied to classify nutritional status. Among them, the Random Forest\nmodel achieved the best performance, classifying women into four categories\n(normal, moderate malnutrition, severe malnutrition, and overnutrition) with\n97.87% accuracy, 97.88% precision, 97.87% recall, 97.87% F1-score, and 99.86%\nROC AUC. These findings demonstrate the effectiveness of ensemble learning in\ncapturing hidden patterns from complex datasets and provide timely insights for\nearly detection of nutritional risks. The results offer practical implications\nfor healthcare providers, policymakers, and researchers, supporting data-driven\nstrategies to improve maternal nutrition and health outcomes in Ethiopia.", "AI": {"tldr": "Ensemble machine learning models effectively predict malnutrition among pregnant Ethiopian women with 97.87% accuracy using demographic and health survey data.", "motivation": "Malnutrition among pregnant women in Ethiopia poses serious public health risks, and traditional statistical methods fail to capture the complex multidimensional determinants of nutritional status.", "method": "Used ensemble ML techniques (XGBoost, Random Forest, CatBoost, AdaBoost) on Ethiopian DHS data (2005-2020, 18,108 records, 30 attributes) with preprocessing including SMOTE balancing and feature selection.", "result": "Random Forest achieved best performance: 97.87% accuracy, 97.88% precision, 97.87% recall, 97.87% F1-score, and 99.86% ROC AUC for classifying four nutritional categories.", "conclusion": "Ensemble learning effectively captures hidden patterns in complex datasets, providing timely insights for early nutritional risk detection and supporting data-driven strategies for improving maternal health outcomes."}}
{"id": "2509.14952", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.14952", "abs": "https://arxiv.org/abs/2509.14952", "authors": ["Zhuanghua Liu", "Luo Luo"], "title": "Stochastic Bilevel Optimization with Heavy-Tailed Noise", "comment": null, "summary": "This paper considers the smooth bilevel optimization in which the lower-level\nproblem is strongly convex and the upper-level problem is possibly nonconvex.\nWe focus on the stochastic setting that the algorithm can access the unbiased\nstochastic gradient evaluation with heavy-tailed noise, which is prevalent in\nmany machine learning applications such as training large language models and\nreinforcement learning. We propose a nested-loop normalized stochastic bilevel\napproximation (N$^2$SBA) for finding an $\\epsilon$-stationary point with the\nstochastic first-order oracle (SFO) complexity of\n$\\tilde{\\mathcal{O}}\\big(\\kappa^{\\frac{7p-3}{p-1}} \\sigma^{\\frac{p}{p-1}}\n\\epsilon^{-\\frac{4 p - 2}{p-1}}\\big)$, where $\\kappa$ is the condition number,\n$p\\in(1,2]$ is the order of central moment for the noise, and $\\sigma$ is the\nnoise level. Furthermore, we specialize our idea to solve the\nnonconvex-strongly-concave minimax optimization problem, achieving an\n$\\epsilon$-stationary point with the SFO complexity of $\\tilde{\\mathcal\nO}\\big(\\kappa^{\\frac{2p-1}{p-1}} \\sigma^{\\frac{p}{p-1}}\n\\epsilon^{-\\frac{3p-2}{p-1}}\\big)$. All above upper bounds match the best-known\nresults under the special case of the bounded variance setting, i.e., $p=2$.", "AI": {"tldr": "Proposes N\u00b2SBA algorithm for stochastic bilevel optimization with heavy-tailed noise, achieving improved complexity bounds for finding \u03f5-stationary points in nonconvex-strongly convex bilevel problems and nonconvex-strongly concave minimax problems.", "motivation": "Addresses the challenge of stochastic bilevel optimization with heavy-tailed noise, which is prevalent in machine learning applications like large language model training and reinforcement learning, where traditional methods assuming bounded variance may fail.", "method": "Develops a nested-loop normalized stochastic bilevel approximation (N\u00b2SBA) algorithm that handles heavy-tailed noise by leveraging central moment assumptions rather than bounded variance, using stochastic first-order oracles with normalized gradient updates.", "result": "Achieves SFO complexity of \u039e(\u03ba\u2077\u1d48\u207b\u00b3/\u1d48\u207b\u00b9 \u03c3\u1d48/\u1d48\u207b\u00b9 \u03f5\u207b\u2074\u1d48\u207b\u00b2/\u1d48\u207b\u00b9) for bilevel optimization and \u039e(\u03ba\u00b2\u1d48\u207b\u00b9/\u1d48\u207b\u00b9 \u03c3\u1d48/\u1d48\u207b\u00b9 \u03f5\u207b\u00b3\u1d48\u207b\u00b2/\u1d48\u207b\u00b9) for minimax optimization, matching best-known results when p=2 (bounded variance case).", "conclusion": "The proposed N\u00b2SBA algorithm provides an effective solution for stochastic bilevel and minimax optimization under heavy-tailed noise conditions, generalizing existing bounded variance results and demonstrating practical relevance for modern machine learning applications."}}
{"id": "2509.14968", "categories": ["cs.LG", "cs.NI"], "pdf": "https://arxiv.org/pdf/2509.14968", "abs": "https://arxiv.org/abs/2509.14968", "authors": ["Carlos Barroso-Fern\u00e1ndez", "Alejandro Calvillo-Fernandez", "Antonio de la Oliva", "Carlos J. Bernardos"], "title": "FAWN: A MultiEncoder Fusion-Attention Wave Network for Integrated Sensing and Communication Indoor Scene Inference", "comment": "7 pages, 6 figures and tables, less than 5500 words. Under revision\n  at IEEE Communication Magazine", "summary": "The upcoming generations of wireless technologies promise an era where\neverything is interconnected and intelligent. As the need for intelligence\ngrows, networks must learn to better understand the physical world. However,\ndeploying dedicated hardware to perceive the environment is not always\nfeasible, mainly due to costs and/or complexity. Integrated Sensing and\nCommunication (ISAC) has made a step forward in addressing this challenge.\nWithin ISAC, passive sensing emerges as a cost-effective solution that reuses\nwireless communications to sense the environment, without interfering with\nexisting communications. Nevertheless, the majority of current solutions are\nlimited to one technology (mostly Wi-Fi or 5G), constraining the maximum\naccuracy reachable. As different technologies work with different spectrums, we\nsee a necessity in integrating more than one technology to augment the coverage\narea. Hence, we take the advantage of ISAC passive sensing, to present FAWN, a\nMultiEncoder Fusion-Attention Wave Network for ISAC indoor scene inference.\nFAWN is based on the original transformers architecture, to fuse information\nfrom Wi-Fi and 5G, making the network capable of understanding the physical\nworld without interfering with the current communication. To test our solution,\nwe have built a prototype and integrated it in a real scenario. Results show\nerrors below 0.6 m around 84% of times.", "AI": {"tldr": "FAWN is a MultiEncoder Fusion-Attention Wave Network that fuses Wi-Fi and 5G signals for indoor scene inference using ISAC passive sensing, achieving sub-0.6m accuracy 84% of the time without interfering with communications.", "motivation": "Current ISAC passive sensing solutions are limited to single technologies (Wi-Fi or 5G), constraining accuracy. Different technologies work with different spectrums, creating a need to integrate multiple technologies to augment coverage area and improve environmental perception.", "method": "Proposed FAWN network based on transformers architecture with MultiEncoder Fusion-Attention to fuse information from both Wi-Fi and 5G signals for passive sensing. Built a prototype and tested in real scenarios.", "result": "Achieved errors below 0.6 meters around 84% of the time in indoor scene inference tasks.", "conclusion": "FAWN demonstrates that fusing multiple wireless technologies (Wi-Fi and 5G) through transformer-based architecture enables accurate indoor scene inference without interfering with existing communications, advancing ISAC passive sensing capabilities."}}
{"id": "2509.14969", "categories": ["cs.LG", "math.OC", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.14969", "abs": "https://arxiv.org/abs/2509.14969", "authors": ["Jean-Fran\u00e7ois Aujol", "J\u00e9r\u00e9mie Bigot", "Camille Castera"], "title": "Stochastic Adaptive Gradient Descent Without Descent", "comment": null, "summary": "We introduce a new adaptive step-size strategy for convex optimization with\nstochastic gradient that exploits the local geometry of the objective function\nonly by means of a first-order stochastic oracle and without any\nhyper-parameter tuning. The method comes from a theoretically-grounded\nadaptation of the Adaptive Gradient Descent Without Descent method to the\nstochastic setting. We prove the convergence of stochastic gradient descent\nwith our step-size under various assumptions, and we show that it empirically\ncompetes against tuned baselines.", "AI": {"tldr": "New adaptive step-size strategy for stochastic convex optimization that requires no hyperparameter tuning and uses only first-order stochastic oracle information.", "motivation": "To develop a theoretically-grounded adaptive step-size method for stochastic gradient descent that automatically adapts to local geometry without requiring manual hyperparameter tuning.", "method": "Adaptation of the Adaptive Gradient Descent Without Descent method to stochastic setting, using first-order stochastic oracle to exploit local geometry of objective function.", "result": "Proven convergence under various assumptions and empirical demonstration of competitiveness against tuned baseline methods.", "conclusion": "The proposed adaptive step-size strategy provides an effective, theoretically-sound approach for stochastic convex optimization that eliminates the need for hyperparameter tuning while maintaining competitive performance."}}
{"id": "2509.15033", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.15033", "abs": "https://arxiv.org/abs/2509.15033", "authors": ["Padmaksha Roy", "Almuatazbellah Boker", "Lamine Mili"], "title": "Beyond Marginals: Learning Joint Spatio-Temporal Patterns for Multivariate Anomaly Detection", "comment": null, "summary": "In this paper, we aim to improve multivariate anomaly detection (AD) by\nmodeling the \\textit{time-varying non-linear spatio-temporal correlations}\nfound in multivariate time series data . In multivariate time series data, an\nanomaly may be indicated by the simultaneous deviation of interrelated time\nseries from their expected collective behavior, even when no individual time\nseries exhibits a clearly abnormal pattern on its own. In many existing\napproaches, time series variables are assumed to be (conditionally)\nindependent, which oversimplifies real-world interactions. Our approach\naddresses this by modeling joint dependencies in the latent space and\ndecoupling the modeling of \\textit{marginal distributions, temporal dynamics,\nand inter-variable dependencies}. We use a transformer encoder to capture\ntemporal patterns, and to model spatial (inter-variable) dependencies, we fit a\nmulti-variate likelihood and a copula. The temporal and the spatial components\nare trained jointly in a latent space using a self-supervised contrastive\nlearning objective to learn meaningful feature representations to separate\nnormal and anomaly samples.", "AI": {"tldr": "A novel approach for multivariate anomaly detection that models time-varying non-linear spatio-temporal correlations using transformer encoders, multivariate likelihood, and copula models trained with contrastive learning.", "motivation": "Existing approaches oversimplify real-world interactions by assuming time series variables are independent, missing the collective behavior patterns that indicate anomalies when no individual series shows clear abnormalities.", "method": "Models joint dependencies in latent space by decoupling marginal distributions, temporal dynamics, and inter-variable dependencies. Uses transformer encoder for temporal patterns, multivariate likelihood and copula for spatial dependencies, trained jointly with self-supervised contrastive learning.", "result": "The approach learns meaningful feature representations that effectively separate normal and anomaly samples by capturing complex spatio-temporal correlations.", "conclusion": "The proposed method successfully addresses the limitations of independence assumptions in multivariate anomaly detection by modeling comprehensive spatio-temporal dependencies through a joint training framework."}}
{"id": "2509.15060", "categories": ["cs.LG", "cs.IT", "math.IT", "math.ST", "stat.CO", "stat.ML", "stat.TH", "94A20, 94A13, 94A12, 94A08, 94-08, 94-04, 68T07, 68P30", "G.3; E.4; I.2; I.2.6; I.5.5"], "pdf": "https://arxiv.org/pdf/2509.15060", "abs": "https://arxiv.org/abs/2509.15060", "authors": ["Lukas Silvester Barth", "Paulo von Petersenn"], "title": "Probabilistic and nonlinear compressive sensing", "comment": null, "summary": "We present a smooth probabilistic reformulation of $\\ell_0$ regularized\nregression that does not require Monte Carlo sampling and allows for the\ncomputation of exact gradients, facilitating rapid convergence to local optima\nof the best subset selection problem. The method drastically improves\nconvergence speed compared to similar Monte Carlo based approaches.\nFurthermore, we empirically demonstrate that it outperforms compressive sensing\nalgorithms such as IHT and (Relaxed-) Lasso across a wide range of settings and\nsignal-to-noise ratios. The implementation runs efficiently on both CPUs and\nGPUs and is freely available at\nhttps://github.com/L0-and-behold/probabilistic-nonlinear-cs.\n  We also contribute to research on nonlinear generalizations of compressive\nsensing by investigating when parameter recovery of a nonlinear teacher network\nis possible through compression of a student network. Building upon theorems of\nFefferman and Markel, we show theoretically that the global optimum in the\ninfinite-data limit enforces recovery up to certain symmetries. For empirical\nvalidation, we implement a normal-form algorithm that selects a canonical\nrepresentative within each symmetry class. However, while compression can help\nto improve test loss, we find that exact parameter recovery is not even\npossible up to symmetries. In particular, we observe a surprising rebound\neffect where teacher and student configurations initially converge but\nsubsequently diverge despite continuous decrease in test loss. These findings\nindicate fundamental differences between linear and nonlinear compressive\nsensing.", "AI": {"tldr": "A smooth probabilistic reformulation of \u21130 regularized regression that enables exact gradient computation and rapid convergence, outperforming Monte Carlo approaches and compressive sensing algorithms like IHT and Lasso. Also explores nonlinear compressive sensing with theoretical and empirical analysis showing fundamental differences from linear cases.", "motivation": "To address the computational challenges of \u21130 regularized regression by developing a method that avoids Monte Carlo sampling and enables exact gradient computation for faster convergence. Additionally, to investigate the feasibility of parameter recovery in nonlinear compressive sensing scenarios.", "method": "Developed a smooth probabilistic reformulation of \u21130 regularization that computes exact gradients without Monte Carlo sampling. For nonlinear compressive sensing, built upon Fefferman and Markel theorems and implemented a normal-form algorithm to select canonical representatives within symmetry classes for empirical validation.", "result": "The method significantly improves convergence speed compared to Monte Carlo approaches and outperforms compressive sensing algorithms (IHT, Relaxed-Lasso) across various settings and SNR ratios. For nonlinear cases, compression improves test loss but exact parameter recovery is impossible even up to symmetries, with observed rebound effect where configurations initially converge then diverge.", "conclusion": "The proposed probabilistic reformulation provides efficient \u21130 regularization with exact gradients. Nonlinear compressive sensing fundamentally differs from linear cases - while compression can improve performance, exact parameter recovery is not achievable even considering symmetries, indicating inherent limitations in nonlinear settings."}}
{"id": "2509.15072", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.15072", "abs": "https://arxiv.org/abs/2509.15072", "authors": ["Martha Cash", "Alexander Wyglinski"], "title": "Improving Internet Traffic Matrix Prediction via Time Series Clustering", "comment": "Accepted to ICMLA 2025", "summary": "We present a novel framework that leverages time series clustering to improve\ninternet traffic matrix (TM) prediction using deep learning (DL) models.\nTraffic flows within a TM often exhibit diverse temporal behaviors, which can\nhinder prediction accuracy when training a single model across all flows. To\naddress this, we propose two clustering strategies, source clustering and\nhistogram clustering, that group flows with similar temporal patterns prior to\nmodel training. Clustering creates more homogeneous data subsets, enabling\nmodels to capture underlying patterns more effectively and generalize better\nthan global prediction approaches that fit a single model to the entire TM.\nCompared to existing TM prediction methods, our method reduces RMSE by up to\n92\\% for Abilene and 75\\% for G\\'EANT. In routing scenarios, our clustered\npredictions also reduce maximum link utilization (MLU) bias by 18\\% and 21\\%,\nrespectively, demonstrating the practical benefits of clustering when TMs are\nused for network optimization.", "AI": {"tldr": "A framework using time series clustering to improve internet traffic matrix prediction by grouping flows with similar temporal patterns before training deep learning models, achieving significant error reduction and better network optimization.", "motivation": "Traffic flows in internet traffic matrices exhibit diverse temporal behaviors that hinder prediction accuracy when using a single model for all flows, necessitating a clustering approach to create homogeneous data subsets.", "method": "Proposes two clustering strategies (source clustering and histogram clustering) to group flows with similar temporal patterns before training deep learning models, enabling better pattern capture and generalization.", "result": "Reduces RMSE by up to 92% for Abilene and 75% for G\u00c9ANT compared to existing methods, and reduces maximum link utilization bias by 18% and 21% respectively in routing scenarios.", "conclusion": "Clustering-based approach significantly improves traffic matrix prediction accuracy and demonstrates practical benefits for network optimization applications."}}
{"id": "2509.15073", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.15073", "abs": "https://arxiv.org/abs/2509.15073", "authors": ["Shaoang Li", "Jian Li"], "title": "Constrained Feedback Learning for Non-Stationary Multi-Armed Bandits", "comment": null, "summary": "Non-stationary multi-armed bandits enable agents to adapt to changing\nenvironments by incorporating mechanisms to detect and respond to shifts in\nreward distributions, making them well-suited for dynamic settings. However,\nexisting approaches typically assume that reward feedback is available at every\nround - an assumption that overlooks many real-world scenarios where feedback\nis limited. In this paper, we take a significant step forward by introducing a\nnew model of constrained feedback in non-stationary multi-armed bandits, where\nthe availability of reward feedback is restricted. We propose the first\nprior-free algorithm - that is, one that does not require prior knowledge of\nthe degree of non-stationarity - that achieves near-optimal dynamic regret in\nthis setting. Specifically, our algorithm attains a dynamic regret of\n$\\tilde{\\mathcal{O}}({K^{1/3} V_T^{1/3} T }/{ B^{1/3}})$, where $T$ is the\nnumber of rounds, $K$ is the number of arms, $B$ is the query budget, and $V_T$\nis the variation budget capturing the degree of non-stationarity.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2509.15076", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2509.15076", "abs": "https://arxiv.org/abs/2509.15076", "authors": ["Mohammad Saleh Vahdatpour", "Maryam Eyvazi", "Yanqing Zhang"], "title": "Forecasting and Visualizing Air Quality from Sky Images with Vision-Language Models", "comment": "Published at ICCVW 2025", "summary": "Air pollution remains a critical threat to public health and environmental\nsustainability, yet conventional monitoring systems are often constrained by\nlimited spatial coverage and accessibility. This paper proposes an AI-driven\nagent that predicts ambient air pollution levels from sky images and\nsynthesizes realistic visualizations of pollution scenarios using generative\nmodeling. Our approach combines statistical texture analysis with supervised\nlearning for pollution classification, and leverages vision-language model\n(VLM)-guided image generation to produce interpretable representations of air\nquality conditions. The generated visuals simulate varying degrees of\npollution, offering a foundation for user-facing interfaces that improve\ntransparency and support informed environmental decision-making. These outputs\ncan be seamlessly integrated into intelligent applications aimed at enhancing\nsituational awareness and encouraging behavioral responses based on real-time\nforecasts. We validate our method using a dataset of urban sky images and\ndemonstrate its effectiveness in both pollution level estimation and\nsemantically consistent visual synthesis. The system design further\nincorporates human-centered user experience principles to ensure accessibility,\nclarity, and public engagement in air quality forecasting. To support scalable\nand energy-efficient deployment, future iterations will incorporate a green CNN\narchitecture enhanced with FPGA-based incremental learning, enabling real-time\ninference on edge platforms.", "AI": {"tldr": "AI-driven system predicts air pollution from sky images and generates realistic pollution visualizations using generative modeling and vision-language models for improved public awareness and decision-making.", "motivation": "Conventional air pollution monitoring systems have limited spatial coverage and accessibility, creating a need for more accessible and transparent methods to help the public understand and respond to air quality conditions.", "method": "Combines statistical texture analysis with supervised learning for pollution classification, and uses vision-language model (VLM)-guided image generation to create interpretable pollution visualizations that simulate varying air quality conditions.", "result": "Validated on urban sky image dataset, demonstrating effectiveness in both pollution level estimation and semantically consistent visual synthesis. The system provides realistic pollution scenario visualizations for user interfaces.", "conclusion": "The approach offers a foundation for intelligent applications that enhance situational awareness and support environmental decision-making. Future work will incorporate green CNN architecture with FPGA-based incremental learning for scalable, energy-efficient real-time deployment on edge platforms."}}
{"id": "2509.15087", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.15087", "abs": "https://arxiv.org/abs/2509.15087", "authors": ["Lei Wang", "Jieming Bian", "Letian Zhang", "Jie Xu"], "title": "Adaptive LoRA Experts Allocation and Selection for Federated Fine-Tuning", "comment": "Accepted to NeurIPS 2025", "summary": "Large Language Models (LLMs) have demonstrated impressive capabilities across\nvarious tasks, but fine-tuning them for domain-specific applications often\nrequires substantial domain-specific data that may be distributed across\nmultiple organizations. Federated Learning (FL) offers a privacy-preserving\nsolution, but faces challenges with computational constraints when applied to\nLLMs. Low-Rank Adaptation (LoRA) has emerged as a parameter-efficient\nfine-tuning approach, though a single LoRA module often struggles with\nheterogeneous data across diverse domains. This paper addresses two critical\nchallenges in federated LoRA fine-tuning: 1. determining the optimal number and\nallocation of LoRA experts across heterogeneous clients, and 2. enabling\nclients to selectively utilize these experts based on their specific data\ncharacteristics. We propose FedLEASE (Federated adaptive LoRA Expert Allocation\nand SElection), a novel framework that adaptively clusters clients based on\nrepresentation similarity to allocate and train domain-specific LoRA experts.\nIt also introduces an adaptive top-$M$ Mixture-of-Experts mechanism that allows\neach client to select the optimal number of utilized experts. Our extensive\nexperiments on diverse benchmark datasets demonstrate that FedLEASE\nsignificantly outperforms existing federated fine-tuning approaches in\nheterogeneous client settings while maintaining communication efficiency.", "AI": {"tldr": "FedLEASE is a federated learning framework that adaptively allocates and selects LoRA experts for efficient LLM fine-tuning across heterogeneous clients while preserving privacy.", "motivation": "Fine-tuning LLMs for domain-specific applications requires substantial domain data often distributed across organizations, but federated learning faces computational constraints and single LoRA modules struggle with heterogeneous data.", "method": "Proposes FedLEASE framework that clusters clients based on representation similarity to allocate domain-specific LoRA experts, and uses adaptive top-M Mixture-of-Experts mechanism for optimal expert selection per client.", "result": "Extensive experiments on diverse benchmark datasets show FedLEASE significantly outperforms existing federated fine-tuning approaches in heterogeneous client settings while maintaining communication efficiency.", "conclusion": "FedLEASE effectively addresses the challenges of determining optimal LoRA expert allocation and enabling selective expert utilization in federated learning environments with heterogeneous data."}}
{"id": "2509.15090", "categories": ["cs.LG", "cs.GT", "econ.TH"], "pdf": "https://arxiv.org/pdf/2509.15090", "abs": "https://arxiv.org/abs/2509.15090", "authors": ["Natalie Collina", "Surbhi Goel", "Aaron Roth", "Emily Ryu", "Mirah Shi"], "title": "Emergent Alignment via Competition", "comment": null, "summary": "Aligning AI systems with human values remains a fundamental challenge, but\ndoes our inability to create perfectly aligned models preclude obtaining the\nbenefits of alignment? We study a strategic setting where a human user\ninteracts with multiple differently misaligned AI agents, none of which are\nindividually well-aligned. Our key insight is that when the users utility lies\napproximately within the convex hull of the agents utilities, a condition that\nbecomes easier to satisfy as model diversity increases, strategic competition\ncan yield outcomes comparable to interacting with a perfectly aligned model. We\nmodel this as a multi-leader Stackelberg game, extending Bayesian persuasion to\nmulti-round conversations between differently informed parties, and prove three\nresults: (1) when perfect alignment would allow the user to learn her\nBayes-optimal action, she can also do so in all equilibria under the convex\nhull condition (2) under weaker assumptions requiring only approximate utility\nlearning, a non-strategic user employing quantal response achieves near-optimal\nutility in all equilibria and (3) when the user selects the best single AI\nafter an evaluation period, equilibrium guarantees remain near-optimal without\nfurther distributional assumptions. We complement the theory with two sets of\nexperiments.", "AI": {"tldr": "Strategic competition among multiple misaligned AI agents can yield outcomes comparable to perfect alignment when user utility lies within the convex hull of agents' utilities, especially with increased model diversity.", "motivation": "Addressing the fundamental challenge of aligning AI systems with human values, exploring whether imperfect alignment can still provide benefits through strategic interaction with multiple misaligned agents.", "method": "Modeled as a multi-leader Stackelberg game, extending Bayesian persuasion to multi-round conversations between differently informed parties, with theoretical analysis and two sets of experiments.", "result": "Three key theoretical results: (1) user can learn Bayes-optimal action under convex hull condition, (2) near-optimal utility achieved with quantal response, (3) equilibrium guarantees remain near-optimal when selecting best single AI after evaluation.", "conclusion": "Strategic competition among diverse misaligned AI agents can effectively substitute for perfect alignment, providing comparable benefits without requiring individually well-aligned models."}}
{"id": "2509.15097", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.15097", "abs": "https://arxiv.org/abs/2509.15097", "authors": ["Mohammad Saleh Vahdatpour", "Huaiyuan Chu", "Yanqing Zhang"], "title": "The Energy-Efficient Hierarchical Neural Network with Fast FPGA-Based Incremental Learning", "comment": "Published at IJCNN 2025", "summary": "The rising computational and energy demands of deep learning, particularly in\nlarge-scale architectures such as foundation models and large language models\n(LLMs), pose significant challenges to sustainability. Traditional\ngradient-based training methods are inefficient, requiring numerous iterative\nupdates and high power consumption. To address these limitations, we propose a\nhybrid framework that combines hierarchical decomposition with FPGA-based\ndirect equation solving and incremental learning. Our method divides the neural\nnetwork into two functional tiers: lower layers are optimized via single-step\nequation solving on FPGAs for efficient and parallelizable feature extraction,\nwhile higher layers employ adaptive incremental learning to support continual\nupdates without full retraining. Building upon this foundation, we introduce\nthe Compound LLM framework, which explicitly deploys LLM modules across both\nhierarchy levels. The lower-level LLM handles reusable representation learning\nwith minimal energy overhead, while the upper-level LLM performs adaptive\ndecision-making through energy-aware updates. This integrated design enhances\nscalability, reduces redundant computation, and aligns with the principles of\nsustainable AI. Theoretical analysis and architectural insights demonstrate\nthat our method reduces computational costs significantly while preserving high\nmodel performance, making it well-suited for edge deployment and real-time\nadaptation in energy-constrained environments.", "AI": {"tldr": "A hybrid framework combining hierarchical decomposition with FPGA-based equation solving and incremental learning to reduce computational and energy demands of large language models while maintaining performance.", "motivation": "Address the unsustainable computational and energy requirements of deep learning, particularly in large-scale architectures like foundation models and LLMs, where traditional gradient-based training is inefficient and power-hungry.", "method": "Divides neural network into two tiers: lower layers use FPGA-based single-step equation solving for efficient feature extraction, while higher layers employ adaptive incremental learning. Introduces Compound LLM framework with lower-level LLM for representation learning and upper-level LLM for adaptive decision-making.", "result": "Significantly reduces computational costs while preserving high model performance, making it suitable for edge deployment and real-time adaptation in energy-constrained environments.", "conclusion": "The proposed hybrid framework enhances scalability, reduces redundant computation, and aligns with sustainable AI principles by combining efficient hardware acceleration with adaptive learning strategies."}}
{"id": "2509.15105", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.15105", "abs": "https://arxiv.org/abs/2509.15105", "authors": ["Liran Nochumsohn", "Raz Marshanski", "Hedi Zisling", "Omri Azencot"], "title": "Super-Linear: A Lightweight Pretrained Mixture of Linear Experts for Time Series Forecasting", "comment": null, "summary": "Time series forecasting (TSF) is critical in domains like energy, finance,\nhealthcare, and logistics, requiring models that generalize across diverse\ndatasets. Large pre-trained models such as Chronos and Time-MoE show strong\nzero-shot (ZS) performance but suffer from high computational costs. In this\nwork, We introduce Super-Linear, a lightweight and scalable mixture-of-experts\n(MoE) model for general forecasting. It replaces deep architectures with simple\nfrequency-specialized linear experts, trained on resampled data across multiple\nfrequency regimes. A lightweight spectral gating mechanism dynamically selects\nrelevant experts, enabling efficient, accurate forecasting. Despite its\nsimplicity, Super-Linear matches state-of-the-art performance while offering\nsuperior efficiency, robustness to various sampling rates, and enhanced\ninterpretability. The implementation of Super-Linear is available at\n\\href{https://github.com/azencot-group/SuperLinear}{https://github.com/azencot-group/SuperLinear}", "AI": {"tldr": "Super-Linear is a lightweight mixture-of-experts model that uses frequency-specialized linear experts and spectral gating for efficient time series forecasting, matching SOTA performance with superior efficiency.", "motivation": "Existing large pre-trained models for time series forecasting suffer from high computational costs despite strong zero-shot performance, creating a need for more efficient solutions.", "method": "Replaces deep architectures with simple linear experts specialized for different frequencies, trained on resampled data, and uses a lightweight spectral gating mechanism to dynamically select relevant experts.", "result": "Matches state-of-the-art performance while offering superior efficiency, robustness to various sampling rates, and enhanced interpretability.", "conclusion": "Super-Linear provides an effective lightweight alternative to computationally expensive models for general time series forecasting across diverse domains."}}
{"id": "2509.15107", "categories": ["cs.LG", "cs.DL"], "pdf": "https://arxiv.org/pdf/2509.15107", "abs": "https://arxiv.org/abs/2509.15107", "authors": ["Amy Rafferty", "Rishi Ramaesh", "Ajitha Rajan"], "title": "Limitations of Public Chest Radiography Datasets for Artificial Intelligence: Label Quality, Domain Shift, Bias and Evaluation Challenges", "comment": null, "summary": "Artificial intelligence has shown significant promise in chest radiography,\nwhere deep learning models can approach radiologist-level diagnostic\nperformance. Progress has been accelerated by large public datasets such as\nMIMIC-CXR, ChestX-ray14, PadChest, and CheXpert, which provide hundreds of\nthousands of labelled images with pathology annotations. However, these\ndatasets also present important limitations. Automated label extraction from\nradiology reports introduces errors, particularly in handling uncertainty and\nnegation, and radiologist review frequently disagrees with assigned labels. In\naddition, domain shift and population bias restrict model generalisability,\nwhile evaluation practices often overlook clinically meaningful measures. We\nconduct a systematic analysis of these challenges, focusing on label quality,\ndataset bias, and domain shift. Our cross-dataset domain shift evaluation\nacross multiple model architectures revealed substantial external performance\ndegradation, with pronounced reductions in AUPRC and F1 scores relative to\ninternal testing. To assess dataset bias, we trained a source-classification\nmodel that distinguished datasets with near-perfect accuracy, and performed\nsubgroup analyses showing reduced performance for minority age and sex groups.\nFinally, expert review by two board-certified radiologists identified\nsignificant disagreement with public dataset labels. Our findings highlight\nimportant clinical weaknesses of current benchmarks and emphasise the need for\nclinician-validated datasets and fairer evaluation frameworks.", "AI": {"tldr": "Current chest X-ray AI datasets have significant limitations including label errors from automated extraction, domain shift issues, and population biases that reduce clinical applicability and generalizability.", "motivation": "To systematically analyze the limitations of current public chest X-ray datasets (MIMIC-CXR, ChestX-ray14, PadChest, CheXpert) that are widely used for AI development but suffer from label quality issues, domain shift, and population biases.", "method": "Conducted cross-dataset domain shift evaluation across multiple model architectures, trained source-classification models to detect dataset bias, performed subgroup analyses for demographic groups, and conducted expert review by two board-certified radiologists to validate label accuracy.", "result": "Found substantial external performance degradation with reduced AUPRC and F1 scores, near-perfect dataset distinguishability indicating bias, reduced performance for minority age/sex groups, and significant disagreement between radiologist review and automated labels.", "conclusion": "Current chest X-ray benchmarks have important clinical weaknesses, highlighting the need for clinician-validated datasets and fairer evaluation frameworks to ensure reliable AI performance in real clinical settings."}}
{"id": "2509.15110", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.15110", "abs": "https://arxiv.org/abs/2509.15110", "authors": ["Dan Zhang", "Min Cai", "Jonathan Li", "Ziniu Hu", "Yisong Yue", "Yuxiao Dong", "Jie Tang"], "title": "TDRM: Smooth Reward Models with Temporal Difference for LLM RL and Inference", "comment": "9 figures, 7 tables", "summary": "Reward models are central to both reinforcement learning (RL) with language\nmodels and inference-time verification. However, existing reward models often\nlack temporal consistency, leading to ineffective policy updates and unstable\nRL training. We introduce TDRM, a method for learning smoother and more\nreliable reward models by minimizing temporal differences during training. This\ntemporal-difference (TD) regularization produces smooth rewards and improves\nalignment with long-term objectives. Incorporating TDRM into the actor-critic\nstyle online RL loop yields consistent empirical gains. It is worth noting that\nTDRM is a supplement to verifiable reward methods, and both can be used in\nseries. Experiments show that TD-trained process reward models (PRMs) improve\nperformance across Best-of-N (up to 6.6%) and tree-search (up to 23.7%)\nsettings. When combined with Reinforcement Learning with Verifiable Rewards\n(RLVR), TD-trained PRMs lead to more data-efficient RL -- achieving comparable\nperformance with just 2.5k data to what baseline methods require 50.1k data to\nattain -- and yield higher-quality language model policies on 8 model variants\n(5 series), e.g., Qwen2.5-(0.5B, 1,5B), GLM4-9B-0414, GLM-Z1-9B-0414,\nQwen2.5-Math-(1.5B, 7B), and DeepSeek-R1-Distill-Qwen-(1.5B, 7B). We release\nall code at https://github.com/THUDM/TDRM.", "AI": {"tldr": "TDRM introduces temporal-difference regularization to create smoother, more reliable reward models that improve RL training stability and alignment with long-term objectives.", "motivation": "Existing reward models lack temporal consistency, leading to ineffective policy updates and unstable reinforcement learning training.", "method": "TDRM minimizes temporal differences during training through TD regularization to produce smooth rewards and improve alignment with long-term objectives.", "result": "TD-trained process reward models improve performance across Best-of-N (up to 6.6%) and tree-search (up to 23.7%) settings. Combined with RLVR, they achieve comparable performance with just 2.5k data vs 50.1k for baselines, yielding higher-quality policies on 8 model variants.", "conclusion": "TDRM is an effective supplement to verifiable reward methods that produces more data-efficient RL training and higher-quality language model policies across multiple model series."}}
{"id": "2509.15113", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.15113", "abs": "https://arxiv.org/abs/2509.15113", "authors": ["Andrei Chertkov", "Artem Basharin", "Mikhail Saygin", "Evgeny Frolov", "Stanislav Straupe", "Ivan Oseledets"], "title": "Low-rank surrogate modeling and stochastic zero-order optimization for training of neural networks with black-box layers", "comment": null, "summary": "The growing demand for energy-efficient, high-performance AI systems has led\nto increased attention on alternative computing platforms (e.g., photonic,\nneuromorphic) due to their potential to accelerate learning and inference.\nHowever, integrating such physical components into deep learning pipelines\nremains challenging, as physical devices often offer limited expressiveness,\nand their non-differentiable nature renders on-device backpropagation difficult\nor infeasible. This motivates the development of hybrid architectures that\ncombine digital neural networks with reconfigurable physical layers, which\neffectively behave as black boxes. In this work, we present a framework for the\nend-to-end training of such hybrid networks. This framework integrates\nstochastic zeroth-order optimization for updating the physical layer's internal\nparameters with a dynamic low-rank surrogate model that enables gradient\npropagation through the physical layer. A key component of our approach is the\nimplicit projector-splitting integrator algorithm, which updates the\nlightweight surrogate model after each forward pass with minimal hardware\nqueries, thereby avoiding costly full matrix reconstruction. We demonstrate our\nmethod across diverse deep learning tasks, including: computer vision, audio\nclassification, and language modeling. Notably, across all modalities, the\nproposed approach achieves near-digital baseline accuracy and consistently\nenables effective end-to-end training of hybrid models incorporating various\nnon-differentiable physical components (spatial light modulators, microring\nresonators, and Mach-Zehnder interferometers). This work bridges hardware-aware\ndeep learning and gradient-free optimization, thereby offering a practical\npathway for integrating non-differentiable physical components into scalable,\nend-to-end trainable AI systems.", "AI": {"tldr": "A framework for end-to-end training of hybrid digital-physical neural networks using stochastic zeroth-order optimization and dynamic low-rank surrogate models to handle non-differentiable physical components.", "motivation": "The need to integrate energy-efficient physical computing components (photonic, neuromorphic) into deep learning pipelines despite their non-differentiable nature and limited expressiveness.", "method": "Combines stochastic zeroth-order optimization for physical layer updates with a dynamic low-rank surrogate model for gradient propagation, using an implicit projector-splitting integrator algorithm to minimize hardware queries.", "result": "Achieves near-digital baseline accuracy across computer vision, audio classification, and language modeling tasks with various non-differentiable physical components.", "conclusion": "Bridges hardware-aware deep learning and gradient-free optimization, providing a practical pathway for integrating non-differentiable physical components into scalable AI systems."}}
{"id": "2509.15120", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.15120", "abs": "https://arxiv.org/abs/2509.15120", "authors": ["Yahav Cohen", "Jacob Goldberger", "Tom Tirer"], "title": "Efficient Conformal Prediction for Regression Models under Label Noise", "comment": null, "summary": "In high-stakes scenarios, such as medical imaging applications, it is\ncritical to equip the predictions of a regression model with reliable\nconfidence intervals. Recently, Conformal Prediction (CP) has emerged as a\npowerful statistical framework that, based on a labeled calibration set,\ngenerates intervals that include the true labels with a pre-specified\nprobability. In this paper, we address the problem of applying CP for\nregression models when the calibration set contains noisy labels. We begin by\nestablishing a mathematically grounded procedure for estimating the noise-free\nCP threshold. Then, we turn it into a practical algorithm that overcomes the\nchallenges arising from the continuous nature of the regression problem. We\nevaluate the proposed method on two medical imaging regression datasets with\nGaussian label noise. Our method significantly outperforms the existing\nalternative, achieving performance close to the clean-label setting.", "AI": {"tldr": "Conformal Prediction for regression with noisy labels in medical imaging, achieving near-clean-label performance", "motivation": "In high-stakes medical imaging applications, reliable confidence intervals are critical, but existing Conformal Prediction methods assume clean labels while real-world calibration sets often contain noisy labels", "method": "Developed a mathematically grounded procedure to estimate noise-free CP threshold, then created a practical algorithm to handle continuous regression challenges with Gaussian label noise", "result": "Significantly outperforms existing alternatives on two medical imaging regression datasets, achieving performance close to clean-label setting", "conclusion": "The proposed method successfully addresses the challenge of applying Conformal Prediction to regression models with noisy calibration labels, providing reliable confidence intervals in practical medical imaging scenarios"}}
{"id": "2509.15145", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.15145", "abs": "https://arxiv.org/abs/2509.15145", "authors": ["Lorne Applebaum", "Travis Dick", "Claudio Gentile", "Haim Kaplan", "Tomer Koren"], "title": "Optimal Learning from Label Proportions with General Loss Functions", "comment": null, "summary": "Motivated by problems in online advertising, we address the task of Learning\nfrom Label Proportions (LLP). In this partially-supervised setting, training\ndata consists of groups of examples, termed bags, for which we only observe the\naverage label value. The main goal, however, remains the design of a predictor\nfor the labels of individual examples. We introduce a novel and versatile\nlow-variance de-biasing methodology to learn from aggregate label information,\nsignificantly advancing the state of the art in LLP. Our approach exhibits\nremarkable flexibility, seamlessly accommodating a broad spectrum of\npractically relevant loss functions across both binary and multi-class\nclassification settings. By carefully combining our estimators with standard\ntechniques, we substantially improve sample complexity guarantees for a large\nclass of losses of practical relevance. We also empirically validate the\nefficacy of our proposed approach across a diverse array of benchmark datasets,\ndemonstrating compelling empirical advantages over standard baselines.", "AI": {"tldr": "Novel low-variance de-biasing method for Learning from Label Proportions (LLP) that works with various loss functions in binary and multi-class classification, improving sample complexity and outperforming baselines.", "motivation": "Addressing problems in online advertising where training data consists of groups (bags) with only average label values available, requiring individual example prediction.", "method": "Introduces a versatile low-variance de-biasing methodology to learn from aggregate label information, combining estimators with standard techniques.", "result": "Significantly advances state of the art in LLP, improves sample complexity guarantees, and demonstrates compelling empirical advantages across diverse benchmark datasets.", "conclusion": "The proposed approach effectively handles LLP problems with flexibility across various loss functions and classification settings, showing both theoretical and practical improvements."}}
{"id": "2509.15147", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2509.15147", "abs": "https://arxiv.org/abs/2509.15147", "authors": ["Viktor Kovalchuk", "Nikita Kotelevskii", "Maxim Panov", "Samuel Horv\u00e1th", "Martin Tak\u00e1\u010d"], "title": "Who to Trust? Aggregating Client Knowledge in Logit-Based Federated Learning", "comment": null, "summary": "Federated learning (FL) usually shares model weights or gradients, which is\ncostly for large models. Logit-based FL reduces this cost by sharing only\nlogits computed on a public proxy dataset. However, aggregating information\nfrom heterogeneous clients is still challenging. This paper studies this\nproblem, introduces and compares three logit aggregation methods: simple\naveraging, uncertainty-weighted averaging, and a learned meta-aggregator.\nEvaluated on MNIST and CIFAR-10, these methods reduce communication overhead,\nimprove robustness under non-IID data, and achieve accuracy competitive with\ncentralized training.", "AI": {"tldr": "Logit-based federated learning reduces communication costs by sharing only logits instead of full model weights/gradients, with three aggregation methods showing competitive accuracy to centralized training.", "motivation": "Traditional federated learning shares model weights or gradients which is costly for large models. Logit-based FL reduces communication overhead by sharing only logits computed on a public proxy dataset, but aggregating information from heterogeneous clients remains challenging.", "method": "Three logit aggregation methods: simple averaging, uncertainty-weighted averaging, and a learned meta-aggregator. Evaluated on MNIST and CIFAR-10 datasets.", "result": "The methods reduce communication overhead, improve robustness under non-IID data distributions, and achieve accuracy competitive with centralized training.", "conclusion": "Logit-based federated learning with effective aggregation strategies provides a communication-efficient alternative to traditional FL while maintaining competitive performance, especially valuable for large models and heterogeneous client data."}}
{"id": "2509.15155", "categories": ["cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2509.15155", "abs": "https://arxiv.org/abs/2509.15155", "authors": ["Seyed Kamyar Seyed Ghasemipour", "Ayzaan Wahid", "Jonathan Tompson", "Pannag Sanketi", "Igor Mordatch"], "title": "Self-Improving Embodied Foundation Models", "comment": "Appearing in the Conference on Neural Information Processing Systems\n  (NeurIPS 2025)", "summary": "Foundation models trained on web-scale data have revolutionized robotics, but\ntheir application to low-level control remains largely limited to behavioral\ncloning. Drawing inspiration from the success of the reinforcement learning\nstage in fine-tuning large language models, we propose a two-stage\npost-training approach for robotics. The first stage, Supervised Fine-Tuning\n(SFT), fine-tunes pretrained foundation models using both: a) behavioral\ncloning, and b) steps-to-go prediction objectives. In the second stage,\nSelf-Improvement, steps-to-go prediction enables the extraction of a\nwell-shaped reward function and a robust success detector, enabling a fleet of\nrobots to autonomously practice downstream tasks with minimal human\nsupervision. Through extensive experiments on real-world and simulated robot\nembodiments, our novel post-training recipe unveils significant results on\nEmbodied Foundation Models. First, we demonstrate that the combination of SFT\nand Self-Improvement is significantly more sample-efficient than scaling\nimitation data collection for supervised learning, and that it leads to\npolicies with significantly higher success rates. Further ablations highlight\nthat the combination of web-scale pretraining and Self-Improvement is the key\nto this sample-efficiency. Next, we demonstrate that our proposed combination\nuniquely unlocks a capability that current methods cannot achieve: autonomously\npracticing and acquiring novel skills that generalize far beyond the behaviors\nobserved in the imitation learning datasets used during training. These\nfindings highlight the transformative potential of combining pretrained\nfoundation models with online Self-Improvement to enable autonomous skill\nacquisition in robotics. Our project website can be found at\nhttps://self-improving-efms.github.io .", "AI": {"tldr": "A two-stage post-training approach combining supervised fine-tuning and self-improvement enables autonomous skill acquisition in robotics, outperforming behavioral cloning in sample efficiency and success rates.", "motivation": "Foundation models have transformed robotics but remain limited to behavioral cloning for low-level control. The success of reinforcement learning in fine-tuning large language models inspired this approach to enable autonomous skill acquisition.", "method": "Two-stage approach: 1) Supervised Fine-Tuning (SFT) with behavioral cloning and steps-to-go prediction objectives, 2) Self-Improvement stage where steps-to-go prediction enables reward function extraction and success detection for autonomous practice by robot fleets.", "result": "Significantly more sample-efficient than scaling imitation data collection, leads to higher success rates, and uniquely enables autonomous acquisition of novel skills beyond behaviors in training datasets.", "conclusion": "Combining pretrained foundation models with online self-improvement has transformative potential for autonomous skill acquisition in robotics."}}
{"id": "2509.15157", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.15157", "abs": "https://arxiv.org/abs/2509.15157", "authors": ["Shiwan Zhao", "Xuyang Zhao", "Jiaming Zhou", "Aobo Kong", "Qicheng Li", "Yong Qin"], "title": "Mind the Gap: Data Rewriting for Stable Off-Policy Supervised Fine-Tuning", "comment": null, "summary": "Supervised fine-tuning (SFT) of large language models can be viewed as an\noff-policy learning problem, where expert demonstrations come from a fixed\nbehavior policy while training aims to optimize a target policy. Importance\nsampling is the standard tool for correcting this distribution mismatch, but\nlarge policy gaps lead to high variance and training instability. Existing\napproaches mitigate this issue using KL penalties or clipping, which passively\nconstrain updates rather than actively reducing the gap. We propose a simple\nyet effective data rewriting framework that proactively shrinks the policy gap\nby keeping correct solutions as on-policy data and rewriting incorrect ones\nwith guided re-solving, falling back to expert demonstrations only when needed.\nThis aligns the training distribution with the target policy before\noptimization, reducing importance sampling variance and stabilizing off-policy\nfine-tuning. Experiments on five mathematical reasoning benchmarks demonstrate\nconsistent and significant gains over both vanilla SFT and the state-of-the-art\nDynamic Fine-Tuning (DFT) approach. The data and code will be released at\nhttps://github.com/NKU-HLT/Off-Policy-SFT.", "AI": {"tldr": "A data rewriting framework for off-policy supervised fine-tuning that proactively reduces policy gaps by keeping correct solutions and rewriting incorrect ones with guided re-solving, improving training stability and performance.", "motivation": "Supervised fine-tuning suffers from distribution mismatch between expert demonstrations and target policy, leading to high variance and instability in off-policy learning through importance sampling.", "method": "Proactive data rewriting framework that aligns training distribution with target policy by keeping correct solutions as on-policy data, rewriting incorrect ones with guided re-solving, and only using expert demonstrations when necessary.", "result": "Experiments on five mathematical reasoning benchmarks show consistent and significant gains over vanilla SFT and state-of-the-art Dynamic Fine-Tuning approach.", "conclusion": "The proposed data rewriting framework effectively reduces importance sampling variance and stabilizes off-policy fine-tuning by proactively shrinking policy gaps before optimization."}}
{"id": "2509.15187", "categories": ["cs.LG", "cs.AR"], "pdf": "https://arxiv.org/pdf/2509.15187", "abs": "https://arxiv.org/abs/2509.15187", "authors": ["Giorgos Armeniakos", "Alexis Maras", "Sotirios Xydis", "Dimitrios Soudris"], "title": "MaRVIn: A Cross-Layer Mixed-Precision RISC-V Framework for DNN Inference, from ISA Extension to Hardware Acceleration", "comment": "Accepted for publication by IEEE Transactions on Computer-Aided\n  Design of Integrated Circuits and Systems, March 2025", "summary": "The evolution of quantization and mixed-precision techniques has unlocked new\npossibilities for enhancing the speed and energy efficiency of NNs. Several\nrecent studies indicate that adapting precision levels across different\nparameters can maintain accuracy comparable to full-precision models while\nsignificantly reducing computational demands. However, existing embedded\nmicroprocessors lack sufficient architectural support for efficiently executing\nmixed-precision NNs, both in terms of ISA extensions and hardware design,\nresulting in inefficiencies such as excessive data packing/unpacking and\nunderutilized arithmetic units. In this work, we propose novel ISA extensions\nand a micro-architecture implementation specifically designed to optimize\nmixed-precision execution, enabling energy-efficient deep learning inference on\nRISC-V architectures. We introduce MaRVIn, a cross-layer hardware-software\nco-design framework that enhances power efficiency and performance through a\ncombination of hardware improvements, mixed-precision quantization, ISA-level\noptimizations, and cycle-accurate emulation. At the hardware level, we enhance\nthe ALU with configurable mixed-precision arithmetic (2, 4, 8 bits) for\nweights/activations and employ multi-pumping to reduce execution latency while\nimplementing soft SIMD for efficient 2-bit ops. At the software level, we\nintegrate a pruning-aware fine-tuning method to optimize model compression and\na greedy-based DSE approach to efficiently search for Pareto-optimal\nmixed-quantized models. Additionally, we incorporate voltage scaling to boost\nthe power efficiency of our system. Our experimental evaluation over widely\nused DNNs and datasets, such as CIFAR10 and ImageNet, demonstrates that our\nframework can achieve, on average, 17.6x speedup for less than 1% accuracy loss\nand outperforms the ISA-agnostic state-of-the-art RISC-V cores, delivering up\nto 1.8 TOPs/W.", "AI": {"tldr": "Proposed MaRVIn framework with novel ISA extensions and micro-architecture for efficient mixed-precision NN execution on RISC-V, achieving 17.6x speedup with <1% accuracy loss and up to 1.8 TOPs/W.", "motivation": "Existing embedded microprocessors lack architectural support for mixed-precision NNs, causing inefficiencies like excessive data packing/unpacking and underutilized arithmetic units.", "method": "Cross-layer hardware-software co-design with enhanced ALU for configurable mixed-precision arithmetic (2,4,8 bits), multi-pumping, soft SIMD, pruning-aware fine-tuning, greedy DSE for Pareto-optimal models, and voltage scaling.", "result": "Achieved average 17.6x speedup with <1% accuracy loss on CIFAR10 and ImageNet datasets, outperforming state-of-the-art RISC-V cores with up to 1.8 TOPs/W power efficiency.", "conclusion": "MaRVIn framework successfully addresses mixed-precision execution inefficiencies through hardware-software co-design, delivering significant performance and energy efficiency improvements for deep learning inference on RISC-V architectures."}}
{"id": "2509.15194", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2509.15194", "abs": "https://arxiv.org/abs/2509.15194", "authors": ["Yujun Zhou", "Zhenwen Liang", "Haolin Liu", "Wenhao Yu", "Kishan Panaganti", "Linfeng Song", "Dian Yu", "Xiangliang Zhang", "Haitao Mi", "Dong Yu"], "title": "Evolving Language Models without Labels: Majority Drives Selection, Novelty Promotes Variation", "comment": null, "summary": "Large language models (LLMs) are increasingly trained with reinforcement\nlearning from verifiable rewards (RLVR), yet real-world deployment demands\nmodels that can self-improve without labels or external judges. Existing\nlabel-free methods, confidence minimization, self-consistency, or majority-vote\nobjectives, stabilize learning but steadily shrink exploration, causing an\nentropy collapse: generations become shorter, less diverse, and brittle. Unlike\nprior approaches such as Test-Time Reinforcement Learning (TTRL), which\nprimarily adapt models to the immediate unlabeled dataset at hand, our goal is\nbroader: to enable general improvements without sacrificing the model's\ninherent exploration capacity and generalization ability, i.e., evolving. We\nformalize this issue and propose EVolution-Oriented and Label-free\nReinforcement Learning (EVOL-RL), a simple rule that couples stability with\nvariation under a label-free setting. EVOL-RL keeps the majority-voted answer\nas a stable anchor (selection) while adding a novelty-aware reward that favors\nresponses whose reasoning differs from what has already been produced\n(variation), measured in semantic space. Implemented with GRPO, EVOL-RL also\nuses asymmetric clipping to preserve strong signals and an entropy regularizer\nto sustain search. This majority-for-selection + novelty-for-variation design\nprevents collapse, maintains longer and more informative chains of thought, and\nimproves both pass@1 and pass@n. EVOL-RL consistently outperforms the\nmajority-only TTRL baseline; e.g., training on label-free AIME24 lifts\nQwen3-4B-Base AIME25 pass@1 from TTRL's 4.6% to 16.4%, and pass@16 from 18.5%\nto 37.9%. EVOL-RL not only prevents diversity collapse but also unlocks\nstronger generalization across domains (e.g., GPQA). Furthermore, we\ndemonstrate that EVOL-RL also boosts performance in the RLVR setting,\nhighlighting its broad applicability.", "AI": {"tldr": "EVOL-RL is a label-free reinforcement learning method that prevents entropy collapse by combining majority-vote stability with novelty-based variation, enabling LLMs to self-improve without sacrificing exploration diversity.", "motivation": "Existing label-free RL methods for LLMs cause entropy collapse where generations become shorter and less diverse. Current approaches like TTRL focus on immediate dataset adaptation rather than general improvement while maintaining exploration capacity.", "method": "EVOL-RL uses majority-voted answers as stable anchors plus novelty-aware rewards that favor semantically different responses. Implemented with GRPO, it includes asymmetric clipping and entropy regularization to maintain search diversity.", "result": "EVOL-RL significantly outperforms TTRL baseline, improving Qwen3-4B-Base AIME25 pass@1 from 4.6% to 16.4% and pass@16 from 18.5% to 37.9%. It prevents diversity collapse and enhances generalization across domains like GPQA.", "conclusion": "EVOL-RL effectively enables LLMs to self-improve without labels while maintaining exploration diversity and generalization ability, working well in both label-free and RLVR settings with broad applicability."}}
{"id": "2509.15198", "categories": ["cs.LG", "stat.AP", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.15198", "abs": "https://arxiv.org/abs/2509.15198", "authors": ["Ahc\u00e8ne Boubekki", "Konstantinos Patlatzoglou", "Joseph Barker", "Fu Siong Ng", "Ant\u00f4nio H. Ribeiro"], "title": "Explaining deep learning for ECG using time-localized clusters", "comment": null, "summary": "Deep learning has significantly advanced electrocardiogram (ECG) analysis,\nenabling automatic annotation, disease screening, and prognosis beyond\ntraditional clinical capabilities. However, understanding these models remains\na challenge, limiting interpretation and gaining knowledge from these\ndevelopments. In this work, we propose a novel interpretability method for\nconvolutional neural networks applied to ECG analysis. Our approach extracts\ntime-localized clusters from the model's internal representations, segmenting\nthe ECG according to the learned characteristics while quantifying the\nuncertainty of these representations. This allows us to visualize how different\nwaveform regions contribute to the model's predictions and assess the certainty\nof its decisions. By providing a structured and interpretable view of deep\nlearning models for ECG, our method enhances trust in AI-driven diagnostics and\nfacilitates the discovery of clinically relevant electrophysiological patterns.", "AI": {"tldr": "Novel interpretability method for CNN-based ECG analysis that extracts time-localized clusters from model representations to visualize waveform contributions and quantify prediction uncertainty.", "motivation": "Deep learning models for ECG analysis lack interpretability, limiting clinical trust and knowledge extraction from these advanced diagnostic tools.", "method": "Extracts time-localized clusters from CNN's internal representations to segment ECG based on learned characteristics while quantifying representation uncertainty.", "result": "Enables visualization of how different ECG waveform regions contribute to predictions and assessment of decision certainty.", "conclusion": "Provides structured interpretability for ECG deep learning models, enhancing trust in AI diagnostics and facilitating discovery of clinically relevant electrophysiological patterns."}}
{"id": "2509.15199", "categories": ["cs.LG", "cs.DB"], "pdf": "https://arxiv.org/pdf/2509.15199", "abs": "https://arxiv.org/abs/2509.15199", "authors": ["Ying Zheng", "Yangfan Jiang", "Kian-Lee Tan"], "title": "CausalPre: Scalable and Effective Data Pre-processing for Causal Fairness", "comment": null, "summary": "Causal fairness in databases is crucial to preventing biased and inaccurate\noutcomes in downstream tasks. While most prior work assumes a known causal\nmodel, recent efforts relax this assumption by enforcing additional\nconstraints. However, these approaches often fail to capture broader attribute\nrelationships that are critical to maintaining utility. This raises a\nfundamental question: Can we harness the benefits of causal reasoning to design\nefficient and effective fairness solutions without relying on strong\nassumptions about the underlying causal model? In this paper, we seek to answer\nthis question by introducing CausalPre, a scalable and effective\ncausality-guided data pre-processing framework that guarantees justifiable\nfairness, a strong causal notion of fairness. CausalPre extracts causally fair\nrelationships by reformulating the originally complex and computationally\ninfeasible extraction task into a tailored distribution estimation problem. To\nensure scalability, CausalPre adopts a carefully crafted variant of\nlow-dimensional marginal factorization to approximate the joint distribution,\ncomplemented by a heuristic algorithm that efficiently tackles the associated\ncomputational challenge. Extensive experiments on benchmark datasets\ndemonstrate that CausalPre is both effective and scalable, challenging the\nconventional belief that achieving causal fairness requires trading off\nrelationship coverage for relaxed model assumptions.", "AI": {"tldr": "CausalPre is a scalable causality-guided data pre-processing framework that achieves causal fairness without requiring strong causal model assumptions, outperforming previous approaches in both effectiveness and efficiency.", "motivation": "Existing causal fairness methods either assume known causal models or enforce constraints that fail to capture broader attribute relationships, limiting their utility. The paper addresses whether causal fairness can be achieved efficiently without strong causal model assumptions.", "method": "CausalPre reformulates the causal fairness extraction task into a distribution estimation problem, using low-dimensional marginal factorization to approximate joint distributions and a heuristic algorithm for computational efficiency.", "result": "Extensive experiments show CausalPre is both effective and scalable, achieving strong causal fairness while maintaining utility, challenging the belief that causal fairness requires trading off relationship coverage.", "conclusion": "CausalPre demonstrates that causal fairness can be achieved efficiently without strong causal model assumptions, providing a practical solution that maintains both fairness and utility in database applications."}}
{"id": "2506.11445", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.11445", "abs": "https://arxiv.org/abs/2506.11445", "authors": ["Xuan Duy Ta", "Bang Giang Le", "Thanh Ha Le", "Viet Cuong Ta"], "title": "Resolve Highway Conflict in Multi-Autonomous Vehicle Controls with Local State Attention", "comment": null, "summary": "In mixed-traffic environments, autonomous vehicles must adapt to\nhuman-controlled vehicles and other unusual driving situations. This setting\ncan be framed as a multi-agent reinforcement learning (MARL) environment with\nfull cooperative reward among the autonomous vehicles. While methods such as\nMulti-agent Proximal Policy Optimization can be effective in training MARL\ntasks, they often fail to resolve local conflict between agents and are unable\nto generalize to stochastic events. In this paper, we propose a Local State\nAttention module to assist the input state representation. By relying on the\nself-attention operator, the module is expected to compress the essential\ninformation of nearby agents to resolve the conflict in traffic situations.\nUtilizing a simulated highway merging scenario with the priority vehicle as the\nunexpected event, our approach is able to prioritize other vehicles'\ninformation to manage the merging process. The results demonstrate significant\nimprovements in merging efficiency compared to popular baselines, especially in\nhigh-density traffic settings.", "AI": {"tldr": "Proposes Local State Attention module using self-attention to improve autonomous vehicle coordination in mixed-traffic environments, showing significant merging efficiency improvements.", "motivation": "Autonomous vehicles need to adapt to human-controlled vehicles and unusual driving situations in mixed-traffic environments. Current MARL methods often fail to resolve local conflicts between agents and cannot generalize to stochastic events.", "method": "Introduces a Local State Attention module that uses self-attention operators to compress essential information from nearby agents, helping resolve traffic conflicts. Tested in simulated highway merging scenarios with priority vehicles as unexpected events.", "result": "The approach demonstrates significant improvements in merging efficiency compared to popular baselines, particularly in high-density traffic settings. It effectively prioritizes other vehicles' information to manage merging processes.", "conclusion": "The Local State Attention module successfully enhances autonomous vehicle coordination in mixed-traffic environments by better handling local conflicts and stochastic events through improved state representation."}}
